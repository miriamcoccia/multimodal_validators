--------------------------------------------------
src/__init__.py
--------------------------------------------------

--------------------------------------------------
src/combine_results.py
--------------------------------------------------
#!/usr/bin/env python3
import argparse
import sys
from pathlib import Path

import pandas as pd


def main():
    parser = argparse.ArgumentParser(description="Combine multiple clean_results CSVs.")
    parser.add_argument(
        "-i",
        "--input-dirs",
        nargs="+",  # Accept one or more directories
        required=True,
        help="List of input directories (e.g., data/results/gemma327)",
    )
    parser.add_argument(
        "-o",
        "--output-file",
        required=True,
        help="Path for the final combined CSV file.",
    )
    parser.add_argument(
        "-f",
        "--filename",
        default="clean_evaluation_results.csv",
        help="The name of the CSV file to find in each directory.",
    )
    args = parser.parse_args()

    all_dfs = []
    input_dirs = [Path(d).expanduser().resolve() for d in args.input_dirs]
    output_file = Path(args.output_file).expanduser().resolve()

    print(f"Combining CSVs into {output_file}...")

    for directory in input_dirs:
        csv_path = directory / args.filename
        if csv_path.exists():
            print(f"  -> Loading {csv_path.relative_to(Path.cwd())}")
            df = pd.read_csv(csv_path)
            all_dfs.append(df)
        else:
            print(f"âš ï¸ Warning: File not found, skipping: {csv_path}")

    if not all_dfs:
        print("âŒ Error: No CSV files were found to combine.", file=sys.stderr)
        sys.exit(1)

    combined_df = pd.concat(all_dfs, ignore_index=True)
    
    # Ensure the output directory exists
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    combined_df.to_csv(output_file, index=False)
    print(f"\nâœ… Success! Combined {len(all_dfs)} files ({len(combined_df)} rows) into:")
    print(f"   {output_file}")


if __name__ == "__main__":
    main()

--------------------------------------------------
src/combined_traits_builder.py
--------------------------------------------------
from typing import Optional, Dict, Any, List
from PIL import Image
import logging
import time
from pathlib import Path
from textwrap import dedent

from src.llm_service.utils import image_to_data_url
from src.config import settings
from src.img_traits_def import ImgTraitDefinition
from src.prompt_template import combi_system_prompt_template
from src.science_qa import ScienceQA, build_question, build_characteristics
from src.llm_service.config import ConfigManager
from src.llm_service.schemas import ValidationListSchema, NebiusValidationListSchema



logger = logging.getLogger(__name__)


class CombinedTraitsBuilder:
    """
    This class creates the prompt and the request payload to evaluate questions according to multiple traits.
    """

    def __init__(
        self,
        trait_list: List[str],
    ):
        # Stricter input validation
        self.config = ConfigManager()
        for trait_name in trait_list:
            if not trait_name or not trait_name.strip():
                raise ValueError("Trait name cannot be empty or contain only whitespace.")

        #self.trait_name = trait_name.strip()
        self.trait_definitions = ImgTraitDefinition()
        self.trait_list = trait_list
        self.trait_name = "combined_traits"
        self.system_prompt = self._build_combined_system_prompt(combi_system_prompt_template)
        
        logger.info(f"CombinedTraitsBuilder initialized.")

    def _build_combined_system_prompt(self, combi_template: str) -> str:
        """Helper function that uses the trait list to build a system prompt that uses all traits and their information instead of a single trait"""
        traits_info_block = ""
        all_traits = ", ".join(self.trait_list)
        for trait_name in self.trait_list:
            definition = self.trait_definitions.retrieve_definition(trait_name)
            note = self.trait_definitions.retrieve_note(trait_name)
            evaluation_questions = self.trait_definitions.retrieve_evaluation_questions(trait_name)
            information = dedent("""
            TRAIT: {trait_name}
            DEFINITION: {definition}
            FOCUS: {note}

            -------------------------------
            EVALUATION QUESTIONS:
            {evaluation_questions}

            ------------------------------- 
            """)
            
            info = information.format(trait_name=trait_name, definition=definition, note=note, evaluation_questions=evaluation_questions)

            traits_info_block += info

        return combi_template.format(
            traits_list=all_traits,
            traits_info=traits_info_block,
        )

    def _create_user_prompt(self, question_data: ScienceQA) -> str:
        """
        Combines question, context, options, and characteristics into one user prompt.
        """
        question_str = build_question(question_data, format_str="QCMLE-A")
        characteristics_str = build_characteristics(
            question_data, format_str="GSTCSkTa"
        )
        return f"{characteristics_str}\n\n{question_str}"

    def inputs_for(
        self,
        question_data: ScienceQA,
        provider: str,
        pil_images: Optional[List[Image.Image]] = None,
        image_file_ids: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Builds input messages.
        For OpenAI: returns list for Responses API "input" field
        For Nebius: returns list that will be added to messages array
        """
        user_text = self._create_user_prompt(question_data)

        user_content: List[Dict[str, Any]] = [{"type": "input_text", "text": user_text}]

        if provider == "openai":
            if image_file_ids:
                for fid in image_file_ids:
                    user_content.append({"type": "input_image", "file_id": fid})

            return [{"role": "user", "content": user_content}]

        elif provider == "nebius":
            # For Nebius, use Chat Completions format
            content: List[Dict[str, Any]] = [{"type": "text", "text": user_text}]

            if pil_images:
                for img in pil_images:
                    data_url = image_to_data_url(img)
                    content.append(
                        {"type": "image_url", "image_url": {"url": data_url}}
                    )

            return [{"role": "user", "content": content}]

        return []

    def prepare_single_request(
        self,
        question_data: ScienceQA,
        provider: str,
        model_id: str, 
        pil_images: Optional[List[Image.Image]] = None,
        image_file_ids: Optional[List[str]] = None,
    ):
        try:
            qid = question_data.id
            if provider == "openai":
                model = settings["models"]["openai"][model_id]
                request_url = "/v1/responses" 
            elif provider == "nebius":
                model = settings["models"]["nebius"][model_id]
                request_url = "/v1/chat/completions"
            else:
                raise ValueError(f"Unknown provider: {provider}")

            request_id = f"request-{model_id}-{self.trait_name}-{qid}-{int(time.time() * 1000)}"

            openai_params = self.config.params.get("openai", {})
            max_out = int(openai_params.get("max_tokens", 3000))

            inputs = self.inputs_for(
                question_data=question_data,
                provider=provider,
                pil_images=pil_images,
                image_file_ids=image_file_ids,
            )

            

            # Different payload based on provider
            if provider == "openai":
                schema = ValidationListSchema.model_json_schema()
                payload = {
                    "model": model,
                    "instructions": self.system_prompt,
                    "max_output_tokens": max_out,
                    "input": inputs,
                    "text": {
                        "format": {
                            "type": "json_schema",
                            "name": "validated_trait_list",
                            "schema": schema,
                            "strict": True,
                        },
                    },
                }
                # Reasoning effort only available for GPT5 models
                if model_id.startswith("GPT5"):
                    payload["reasoning"] = {"effort": "low"}
                    payload["text"]["verbosity"] = "low"
                
                # GPT5 does not take temperature or top_p parameters in the payload
                if not model_id.startswith("GPT5"):
                    payload["temperature"] = 0
                    payload["top_p"] = 1

            else:  # nebius
                schema = NebiusValidationListSchema.model_json_schema()                
                messages = [{"role": "system", "content": self.system_prompt}]

                for msg in inputs:
                    if msg.get("role") == "user":
                        messages.append(msg)

                payload = {
                    "model": model,
                    "messages": messages,
                    "response_format": {
                        "type": "json_schema",
                        "json_schema": {
                            "name": "validated_trait_list",
                            "schema": schema,
                            "strict": True,
                        },
                    },
                    "max_tokens": max_out,
                }

                if not model_id.startswith("GPT5"):
                    payload["temperature"] = 0
                    payload["top_p"] = 1

            request_text = {
                "custom_id": request_id,
                "method": "POST",
                "url": request_url,
                "body": payload,
            }


            return request_text

        except Exception as e:
            logger.error(
                f"Failed to prepare request for trait '{self.trait_name}': {e}"
            )
            return None


if __name__ == "__main__":
    try:
        trait_def_path = Path(settings["paths"]["trait_definitions_json"])
        if not trait_def_path.exists():
            logger.error(f"Trait definitions JSON not found: {trait_def_path.resolve()}")
            raise FileNotFoundError(f"Trait definitions JSON not found: {trait_def_path.resolve()}")
        trait_def = ImgTraitDefinition(trait_def_path)
        if not trait_def.traits:
             logger.error("Trait definitions loaded but resulted in an empty traits dictionary.")
             raise ValueError("Trait definitions loaded but resulted in an empty traits dictionary.")
        trait_names = list(trait_def.traits.keys())
        logger.info(f"ðŸ§¬ Loaded {len(trait_names)} traits: {', '.join(trait_names)}")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize trait definitions: {e}")
        
    builder = CombinedTraitsBuilder(trait_list=trait_names)
    
   

--------------------------------------------------
src/config/__init__.py
--------------------------------------------------
import sys
from pathlib import Path
import os
from dotenv import load_dotenv

if sys.version_info >= (3, 11):
    import tomllib
else:
    try:
        import tomli as tomllib
    except ImportError:
        print(
            "ERROR: 'tomli' package not found. Please install it with 'pip install tomli'"
        )
        sys.exit(1)

# --- PATH DEFINITION ---
PROJECT_ROOT = Path(__file__).parent.parent.parent

CONFIG_FILE_PATH = (
    Path(__file__).parent / "config.toml"
)  # path relative to the current file __file__ refers to this file we are on rn
ENV_PATH = PROJECT_ROOT / "secrets.env"

print(f"Attempting to load environment variables from: {ENV_PATH}")
load_dotenv(dotenv_path=ENV_PATH)  # Load the .env file into the environment
print("âœ… Environment variables loaded.")


def load_config() -> dict:
    """
    Loads and validates the configuration from the config.toml file.
    Resolves API keys from environment variables into an 'api' section.
    """
    print(f"Attempting to load configuration from: {CONFIG_FILE_PATH}")
    if not CONFIG_FILE_PATH.is_file():
        print(f"FATAL ERROR: Configuration file not found at '{CONFIG_FILE_PATH}'.")
        sys.exit(1)

    try:
        with open(CONFIG_FILE_PATH, "rb") as f:
            config_data = tomllib.load(f)

        api_keys_config = config_data.get("api_keys", {})
        resolved_secrets = {}

        for key_name_in_toml, env_var_name in api_keys_config.items():
            if key_name_in_toml.endswith("_env"):
                secret_value = os.getenv(env_var_name)
                new_key = key_name_in_toml.removesuffix("_env")

                if secret_value:
                    resolved_secrets[new_key] = secret_value
                    print(f"âœ… Secret '{new_key}' loaded successfully.")
                else:
                    resolved_secrets[new_key] = None
                    print(
                        f"âš ï¸ WARNING: Environment variable '{env_var_name}' for '{new_key}' not found."
                    )

        # CRITICAL FIX: Store resolved secrets in a new 'api' section
        if resolved_secrets:
            if "api" not in config_data:
                config_data["api"] = {}
            config_data["api"].update(resolved_secrets)
            print(
                f"âœ… Stored {len(resolved_secrets)} resolved API keys in config['api']"
            )

        # Build all_models index
        all_models = {}
        if "models" in config_data:
            for model_group in config_data.get("models", {}).values():
                if isinstance(model_group, dict):
                    all_models.update(model_group)
        config_data["all_models"] = all_models

        print("âœ… Configuration loaded and processed successfully.")
        return config_data

    except tomllib.TOMLDecodeError as e:
        print(f"FATAL ERROR: Could not parse '{CONFIG_FILE_PATH}': {e}")
        sys.exit(1)


# --- GLOBAL SETTINGS VARIABLE ---
# This line executes when the module is imported, loading the settings once.
# Any other module can now access settings by using `from src.config import settings`.
settings = load_config()

--------------------------------------------------
src/ensemble.py
--------------------------------------------------
from enum import Enum
import pandas as pd
from collections import Counter
from typing import List, Dict, Any, Tuple, Union
from src.config import settings
import logging

logger = logging.getLogger(__name__)


class MultiJudgesType(Enum):
    """Enumeration of available ensemble strategies."""

    MajorityVoting = 0
    AllTrue = 1
    AtLeastOne = 2


class EnsembleEvaluation:
    """
    Performs ensemble evaluation on trait validation results from multiple models.
    """

    def __init__(self, records_df: pd.DataFrame):
        self.records_df = records_df.copy()

    def _apply_voting_logic(
        self, trait_outputs: List[bool], judge: MultiJudgesType
    ) -> bool:
        """Applies the selected voting logic to a list of boolean votes."""
        if judge == MultiJudgesType.MajorityVoting:
            vote_count = Counter(trait_outputs)
            return vote_count.get(True, 0) > vote_count.get(False, 0)
        elif judge == MultiJudgesType.AllTrue:
            return all(trait_outputs)
        elif judge == MultiJudgesType.AtLeastOne:
            return any(trait_outputs)
        return False

    def _get_model_combinations(self) -> List[Tuple[str, ...]]:
        """
        Get model combinations from config or generate defaults.
        Returns list of tuples containing model IDs.
        """
        # Try to get from config
        ensemble_config = settings.get("ensemble_models", {})
        combinations = ensemble_config.get("ensemble_combinations", [])

        # Validate and normalize combinations
        valid_combinations = []
        for combo in combinations:
            if isinstance(combo, (list, tuple)):
                # Filter out empty strings and None values
                valid_models = [str(m).strip() for m in combo if m and str(m).strip()]
                if len(valid_models) >= 2:  # Need at least 2 models for ensemble
                    valid_combinations.append(tuple(valid_models))
            elif isinstance(combo, str) and combo.strip():
                # Single model string - skip as we need pairs
                logger.warning(f"Skipping single model combination: {combo}")

        # If no valid combinations from config, create defaults from available models
        if not valid_combinations:
            unique_models = self.records_df["model_id"].unique().tolist()
            logger.info(
                f"No valid ensemble combinations in config. Available models: {unique_models}"
            )

            # Create default pairs if we have at least 2 models
            if len(unique_models) >= 2:
                # Just use first two models as a default combination
                valid_combinations = [(unique_models[0], unique_models[1])]
                logger.info(f"Using default combination: {valid_combinations[0]}")

        return valid_combinations

    def run_ensemble_strategy(self, judge: MultiJudgesType) -> pd.DataFrame:
        """
        Performs the chosen ensemble strategy for each (question_id, trait) pair.
        Returns a new DataFrame with the ensemble result.
        """
        if self.records_df.empty:
            logger.warning("Empty records DataFrame provided to ensemble evaluation")
            return pd.DataFrame()

        # Get model combinations
        model_combinations = self._get_model_combinations()

        if not model_combinations:
            logger.warning("No valid model combinations for ensemble evaluation")
            return pd.DataFrame()

        ensemble_results = []
        grouped = self.records_df.groupby(["question_id", "trait"])

        for combo in model_combinations:
            # Build ensemble name from judge type and model IDs
            ensemble_name = f"{judge.name}_{'_'.join(combo)}"
            logger.debug(f"Processing ensemble: {ensemble_name}")

            for (qid, trait), group in grouped:
                # Get votes from models in this combination
                votes = []
                for model_id in combo:
                    model_votes = group[group["model_id"] == model_id]["validity"]
                    if not model_votes.empty:
                        # Convert to boolean and get the value (handle Series properly)
                        vote_value = (
                            model_votes.iloc[0]
                            if len(model_votes) == 1
                            else model_votes.mode().iloc[0]
                        )
                        votes.append(bool(vote_value))

                if not votes:
                    logger.debug(
                        f"No votes found for {qid}/{trait} with models {combo}"
                    )
                    continue

                final_validity = self._apply_voting_logic(votes, judge)

                ensemble_results.append(
                    {
                        "question_id": qid,
                        "trait": trait,
                        "model_id": ensemble_name,
                        "validity": final_validity,
                        "contributing_models": len(votes),
                    }
                )

        result_df = pd.DataFrame(ensemble_results)
        logger.info(
            f"Ensemble evaluation complete. Generated {len(result_df)} results."
        )
        return result_df

--------------------------------------------------
src/export_changes.py
--------------------------------------------------
import os
from pathlib import Path
import time


def combine_python_files_fast(
    source_dir, output_file="project.txt", exclude_dirs=None, show_progress=True
):
    """
    Fast version that combines Python files with minimal overhead.
    Args:
        source_dir: Directory containing Python files
        output_file: Name of the output text file
        exclude_dirs: Set of directory names to exclude
        show_progress: Show progress while processing
    """
    start_time = time.time()
    source_path = Path(source_dir)

    if not source_path.exists():
        print(f"Error: Directory '{source_dir}' does not exist")
        return

    # Default exclusions (using set for O(1) lookup)
    if exclude_dirs is None:
        exclude_dirs = {
            "__pycache__",
            ".git",
            "venv",
            "env",
            ".venv",
            "node_modules",
            ".tox",
            "dist",
            "build",
            ".eggs",
        }
    else:
        exclude_dirs = set(exclude_dirs)

    # Collect files first (faster than filtering during iteration)
    python_files = []

    # Use os.walk for better performance than Path.rglob
    for root, dirs, files in os.walk(source_dir):
        # Modify dirs in-place to skip excluded directories
        dirs[:] = [d for d in dirs if d not in exclude_dirs]

        # Add Python files from current directory
        for file in files:
            if file.endswith(".py"):
                python_files.append(Path(root) / file)

    if not python_files:
        print(f"No Python files found in '{source_dir}'")
        return

    # Sort files
    python_files.sort()

    if show_progress:
        print(f"Found {len(python_files)} Python files. Combining...")

    # Use larger buffer for better I/O performance
    with open(output_file, "w", encoding="utf-8", buffering=8192) as outfile:
        for i, py_file in enumerate(python_files):
            if show_progress and (i + 1) % 10 == 0:
                print(f"Processing: {i + 1}/{len(python_files)} files...")

            # Get relative path
            try:
                relative_path = py_file.relative_to(source_path)
            except ValueError:
                relative_path = py_file

            # Write header
            outfile.write("-" * 50 + "\n")
            outfile.write(f"{relative_path}\n")
            outfile.write("-" * 50 + "\n")

            # Read and write file contents
            try:
                with open(py_file, "r", encoding="utf-8", errors="replace") as infile:
                    # Read entire file at once (faster than line by line)
                    contents = infile.read()
                    outfile.write(contents)

                    if contents and not contents.endswith("\n"):
                        outfile.write("\n")

                    if i < len(python_files) - 1:
                        outfile.write("\n")

            except Exception as e:
                outfile.write(f"# Error reading file: {e}\n\n")

    elapsed = time.time() - start_time
    print(f"\nâœ“ Successfully combined {len(python_files)} files into '{output_file}'")
    print(f"  Time taken: {elapsed:.2f} seconds")

    # Show file size
    output_size = Path(output_file).stat().st_size
    if output_size > 1024 * 1024:
        print(f"  Output size: {output_size / (1024 * 1024):.2f} MB")
    else:
        print(f"  Output size: {output_size / 1024:.2f} KB")


def combine_specific_files(file_list, output_file="project.txt"):
    """
    Combine only specific Python files (useful for large projects).
    Args:
        file_list: List of specific file paths to include
        output_file: Name of the output text file
    """
    with open(output_file, "w", encoding="utf-8", buffering=8192) as outfile:
        for i, py_file in enumerate(file_list):
            py_file = Path(py_file)

            if not py_file.exists():
                print(f"Warning: {py_file} does not exist, skipping...")
                continue

            outfile.write("-" * 50 + "\n")
            outfile.write(f"{py_file.name}\n")
            outfile.write("-" * 50 + "\n")

            try:
                with open(py_file, "r", encoding="utf-8", errors="replace") as infile:
                    contents = infile.read()
                    outfile.write(contents)

                    if contents and not contents.endswith("\n"):
                        outfile.write("\n")

                if i < len(file_list) - 1:
                    outfile.write("\n")

            except Exception as e:
                outfile.write(f"# Error reading file: {e}\n\n")

    print(f"Combined {len(file_list)} files into '{output_file}'")


def combine_by_pattern(source_dir, patterns=None, output_file="project.txt"):
    """
    Combine only files matching specific patterns.
    Args:
        source_dir: Directory to search
        patterns: List of patterns like ['**/models/*.py', '**/views/*.py']
        output_file: Output file name
    """
    if patterns is None:
        patterns = ["**/*.py"]

    source_path = Path(source_dir)
    python_files = []

    for pattern in patterns:
        python_files.extend(source_path.glob(pattern))

    # Remove duplicates and sort
    python_files = sorted(set(python_files))

    if not python_files:
        print(f"No files found matching patterns: {patterns}")
        return

    with open(output_file, "w", encoding="utf-8", buffering=8192) as outfile:
        for i, py_file in enumerate(python_files):
            relative_path = py_file.relative_to(source_path)

            outfile.write("-" * 50 + "\n")
            outfile.write(f"{relative_path}\n")
            outfile.write("-" * 50 + "\n")

            try:
                with open(py_file, "r", encoding="utf-8", errors="replace") as infile:
                    contents = infile.read()
                    outfile.write(contents)

                    if contents and not contents.endswith("\n"):
                        outfile.write("\n")

                    if i < len(python_files) - 1:
                        outfile.write("\n")

            except Exception as e:
                outfile.write(f"# Error reading file: {e}\n\n")

    print(f"Combined {len(python_files)} files into '{output_file}'")


# Example usage
if __name__ == "__main__":
    # FAST: Basic usage
    combine_python_files_fast(".", "project.txt")

    # For very large projects, combine only specific directories
    # combine_by_pattern('.', patterns=['src/**/*.py', 'lib/**/*.py'], output_file='project.txt')

    # Or list specific files
    # files_to_combine = [
    #     'main.py',
    #     'models/user.py',
    #     'utils/helpers.py'
    # ]
    # combine_specific_files(files_to_combine, 'project.txt')

--------------------------------------------------
src/get_results.py
--------------------------------------------------
"""
Script to normalize batch JSONL results to raw and clean CSV files.
Uses ResultsHandler to parse one or more JSONL formats.
"""
from pathlib import Path
import argparse
import sys
import pandas as pd
from src.results_handler import ResultsHandler
from src.config import settings


def main():
    """Main execution function."""
    ap = argparse.ArgumentParser(description="Normalize batch JSONL to CSVs")
    ap.add_argument(
        "--jsonl",
        nargs="?",
        default=None,
        help="Path to batch results .jsonl (if omitted, use config + .results.jsonl)",
    )
    ap.add_argument(
        "-o",
        "--outdir",
        default=None,
        help="Output directory for CSVs (default: settings['paths']['results_dir'])",
    )
    args = ap.parse_args()

    # Resolve JSONL input path
    jsonl_path: Path
    if args.jsonl:
        jsonl_path = Path(args.jsonl).expanduser().resolve()
    else:
        # Derive from config batch_request_file
        base = Path(settings["paths"]["batch_request_file"]).expanduser().resolve()
        jsonl_path = base.with_suffix(".results.jsonl")

    # Resolve output dir
    outdir: Path = (
        Path(args.outdir).expanduser().resolve()
        if args.outdir
        else Path(settings["paths"]["results_dir"]).expanduser().resolve()
    )

    print(f"ðŸ“¥ Input JSONL : {jsonl_path}")
    print(f"ðŸ“¤ Output dir  : {outdir}")

    if not jsonl_path.exists():
        print(f"âŒ Results JSONL not found: {jsonl_path}", file=sys.stderr)
        sys.exit(1)

    rh = ResultsHandler(outdir)
    raw_df = rh.load_batch_results_jsonl(jsonl_path)
    print(f"ðŸ§¾ Parsed rows : {len(raw_df)}")

    if raw_df.empty:
        print("âš ï¸ No rows were parsed. Exiting.")
        sys.exit(0)

    rh.save_raw_results(raw_df)

    try:
        clean_df = rh.clean_results(raw_df)
    except Exception as e:
        print(f"âš ï¸ Cleaning failed: {e}", file=sys.stderr)
        # Show a small sample of columns to debug
        with pd.option_context("display.max_columns", None, "display.width", 200):
            print("ðŸ”Ž Raw sample:\n", raw_df.head(5))
        sys.exit(2)

    if clean_df.empty:
        print("âš ï¸ Cleaning resulted in an empty DataFrame. Check raw results.")
        sys.exit(0)
        
    rh.save_clean_results(clean_df)

    print(f"âœ… Clean CSV: {rh.CLEAN_RESULTS_FILENAME.resolve()}")
    with pd.option_context("display.max_columns", None, "display.width", 200):
        print("\n--- Clean Results Sample ---")
        print(clean_df.head(10).to_string(index=False))
        print("----------------------------")


if __name__ == "__main__":
    main()

--------------------------------------------------
src/img_traits_def.py
--------------------------------------------------
import json
from typing import Optional
from pathlib import Path
from src.config import PROJECT_ROOT


class ImgTraitDefinition:
    """
    Loads and provides trait definitions and example prompts for multimodal evaluation.
    """

    def __init__(self, path: Optional[Path] = None):
        prompt_file = path or PROJECT_ROOT / "data" / "prompt_fillers.json"
        try:
            with open(prompt_file, "r", encoding="utf-8") as file:
                self.traits = json.load(file)
        except FileNotFoundError:
            print(f"âŒ Could not find prompt file at {prompt_file}")
            self.traits = {}

    def retrieve_definition(self, trait_name: str) -> str:
        trait = self.traits.get(trait_name.title())
        if trait:
            return trait.get("definition", "Definition not found.")
        return "Trait not found."

    def retrieve_note(self, trait_name: str) -> str:
        note = self.traits.get(trait_name.title(), {}).get("note", "")
        if note:
            return note
        return "Note not found."

    def retrieve_evaluation_questions(self, trait_name: str) -> str:
        eval_questions_list = self.traits.get(trait_name.title(), {}).get(
            "evaluation_questions", []
        )
        if eval_questions_list:
            return "\n".join([f"- {q}" for q in eval_questions_list])
        return "Evaluation Questions not found."

    def get_examples_for_prompt(self, trait_name: str) -> str:
        examples = self.traits.get(trait_name.title(), {}).get("examples", [])
        if not examples:
            return "No examples available."

        formatted = []
        for i, ex in enumerate(examples, 1):
            input_data = ex["input"]
            output_data = ex["output"]

            output = {
                "trait": output_data["trait"],
                "validity": output_data["validity"],
            }
            if "reasoning" in output_data:
                output["reasoning"] = output_data["reasoning"]

            formatted.append(
                f"Example {i}:\n"
                f"Input:\n"
                f"  Trait: {input_data['trait']}\n"
                f'  Question: "{input_data["question"]}"\n'
                f"  Image: {input_data['image']}\n"
                f"Correct Output:\n{json.dumps(output, indent=2)}\n"
            )
        return "\n".join(formatted)

--------------------------------------------------
src/llm_service/config.py
--------------------------------------------------
import os
from typing import Dict, Any, Optional
from pathlib import Path

from src.config import settings


class ConfigManager:
    """Centralized configuration management for the LLM service."""

    def __init__(self):
        self._settings = settings

    @property
    def models_openai(self) -> Dict[str, str]:
        return self._settings.get("models", {}).get("openai", {}) or {}

    @property
    def models_nebius(self) -> Dict[str, str]:
        return self._settings.get("models", {}).get("nebius", {}) or {}

    @property
    def model_index(self) -> Dict[str, str]:
        """Combined index of all models across providers."""
        index = {}
        index.update(self.models_openai)
        index.update(self.models_nebius)
        return index

    @property
    def defaults(self) -> Dict[str, str]:
        return self._settings.get("defaults", {})

    @property
    def params(self) -> Dict[str, Any]:
        return self._settings.get("parameters", {})

    @property
    def timeout(self) -> int:
        return self._settings.get("settings", {}).get("default_timeout", 600)

    @property
    def openai_api_key(self) -> Optional[str]:
        env_var = self._settings.get("api", {}).get(
            "openai_api_key_env", "OPENAI_API_KEY"
        )
        return os.environ.get(env_var)

    @property
    def nebius_api_key(self) -> Optional[str]:
        env_var = self._settings.get("api", {}).get(
            "nebius_api_key_env", "NEBIUS_API_KEY"
        )
        return os.environ.get(env_var)

    @property
    def openai_base_url(self) -> Optional[str]:
        return self._settings.get("services", {}).get("openai", {}).get("base_url")

    @property
    def nebius_base_url(self) -> Optional[str]:
        return self._settings.get("services", {}).get("nebius", {}).get("base_url")


# Add this to the end of src/llm_service/config.py for testing
if __name__ == "__main__":
    import os

    # Set a fake key for the test to run
    os.environ["NEBIUS_API_KEY"] = "fake-nebius-key-for-testing"

    manager = ConfigManager()

    print("Checking Nebius models...")
    assert "L_Gemma327B" in manager.models_nebius, "Nebius models not loaded!"
    print("âœ… Nebius models loaded.")

    print("Checking Nebius API key...")
    assert (
        manager.nebius_api_key == "fake-nebius-key-for-testing"
    ), "Nebius API key not found!"
    print("âœ… Nebius API key loaded.")

    print("Checking Nebius base URL...")
    assert (
        manager.nebius_base_url and "nebius.com" in manager.nebius_base_url
    ), "Nebius base URL not found!"
    print("âœ… Nebius base URL loaded.")

    print("Checking combined model index...")
    assert "L_Qwen25VL72B" in manager.model_index, "Nebius model missing from index!"
    assert "GPT4oMini" in manager.model_index, "OpenAI model missing from index!"
    print("âœ… Model index is correct.")

    print("\nðŸŽ‰ All ConfigManager checks passed!")

--------------------------------------------------
src/llm_service/providers/__init__.py
--------------------------------------------------
from .base import BaseProvider
from .openai import OpenAIProvider
from .nebius import NebiusProvider

__all__ = ["BaseProvider", "OpenAIProvider", "NebiusProvider"]

--------------------------------------------------
src/llm_service/providers/base.py
--------------------------------------------------
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import logging

from PIL import Image
import aiohttp

from ..config import ConfigManager
from ..schemas import ValidationSchema
from ..utils import _extract_core_fields

logger = logging.getLogger(__name__)


class BaseProvider(ABC):
    """Abstract base class for all LLM providers."""

    def __init__(self, config: ConfigManager, session: aiohttp.ClientSession):
        self.config = config
        self.session = session
        self.error_models: List[str] = []

    @abstractmethod
    async def execute(
        self,
        model_id: str,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        pil_images: Optional[List[Image.Image]] = None,
    ) -> Optional[Dict[str, Any]]:
        """Execute a prompt and return validated response."""
        pass

    @abstractmethod
    def resolve_model_id(self, model_id: str) -> Optional[str]:
        """Resolve a friendly model ID to the actual model identifier."""
        pass

    def _safe_validate_response(
        self, raw_data: Optional[Dict[str, Any]], model_id: str
    ) -> Optional[Dict]:
        """Safely validate response data against schema."""
        if not raw_data:
            logger.error(f"Empty response data for {model_id}")
            return None

        try:
            validated = ValidationSchema.model_validate(raw_data)
            return validated.model_dump()
        except Exception as strict_error:
            logger.warning(f"Strict validation failed for {model_id}: {strict_error}")

            try:
                core_data = _extract_core_fields(raw_data)
                if core_data:
                    validated = ValidationSchema.model_validate(core_data)
                    logger.info(f"Successfully validated core fields for {model_id}")
                    return validated.model_dump()
            except Exception as fallback_error:
                logger.error(
                    f"Fallback validation also failed for {model_id}: {fallback_error}"
                )
                logger.error(f"Raw data was: {raw_data}")
        return None

--------------------------------------------------
src/llm_service/providers/basic_batch.py
--------------------------------------------------
"""
Base class from which we can inherit to create batch services that point to specific API providers (OpenAI, or Nebius)
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import json


class BaseBatchService(ABC):
    """
    Abstract base class for a batch processing service.
    Defines the common interface for submitting, monitoring, and retrieving batch jobs.
    """

    @abstractmethod
    def submit_batch(self, filepath: str, name: Optional[str] = None) -> str:
        """
        Submits a batch job for processing.

        Args:
            filepath: The path to the file containing batch requests.
            name: An optional name or description for the batch job.

        Returns:
            A unique identifier for the submbitted batch job.
        """
        pass

    @abstractmethod
    def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """Retrieves the status of a specific batch job."""
        pass

    @abstractmethod
    def download_batch_results(self, batch_id: str, output_path: str) -> bool:
        """Downloads the resutls of a completed batch job."""
        pass

    @abstractmethod
    def cancel_batch(self, batch_id: str) -> bool:
        """Cancels an in-progress batch job."""
        pass

    @abstractmethod
    def delete_batch(self, batch_id: str) -> bool:
        """Deletes a batch job and its associated files."""
        pass

    def write_jsonl_file(self, requests: List[Dict[str, Any]], filepath: str) -> None:
        """
        Writes batch requests to a JSONL file that will be used by the different batch services.

        Args:
            requests: List of request dictionaries
            filepath: Path where the JSONL file should be written
        """

        with open(filepath, "w+", encoding="utf-8") as f:
            for request in requests:
                json.dump(request, f)
                f.write("\n")

--------------------------------------------------
src/llm_service/providers/nebius.py
--------------------------------------------------
"""
Nebius API provider integration
"""

import json
import logging
from typing import List, Dict, Any, Optional

from PIL import Image
import aiohttp

from .base import BaseProvider
from ..utils import image_to_data_url, retry_with_backoff
from ..schemas import ValidationSchema

logger = logging.getLogger(__name__)


class NebiusProvider(BaseProvider):
    """Nebius API provider using function calling for structured output."""

    def resolve_model_id(self, model_id: str) -> Optional[str]:
        """Resolve Nebius model ID with fallback to default."""
        if model_id in self.config.model_index:
            return self.config.model_index[model_id]

        fallback = self.config.defaults.get("nebius_multimodal_default", "")
        return self.config.model_index.get(fallback)

    @retry_with_backoff()
    async def _make_request(
        self, payload: Dict[str, Any], headers: Dict[str, str]
    ) -> Optional[Dict[str, Any]]:
        """Make HTTP request to OpenAI API."""
        if self.config.openai_base_url:
            async with self.session.post(
                self.config.openai_base_url, headers=headers, json=payload
            ) as r:
                if r.status != 200:
                    text = await r.text()
                    logger.warning(f"OpenAI response status {r.status}: {text[:200]}")
                r.raise_for_status()
                return await r.json()
        return None

    # TODO: turn this method into a method to create a request line that can be processed through the batch api, do not write it yet, because we need other metadata. The batch jsonl file should be created only ONCE, by the orchestrator
    async def execute(
        self,
        model_id: str,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        pil_images: Optional[List[Image.Image]] = None,
    ) -> Optional[Dict[str, Any]]:
        """Execute prompt using Nebius function calling."""
        if not self.config.nebius_api_key:
            logger.error("Cannot execute prompt: API key is missing")
            return None

        model_name = self.resolve_model_id(model_id)
        if not model_name:
            logger.error(f"No Nebius model found for: {model_id}")
            return None

        if not self.config.nebius_base_url:
            logger.error("Nebius API URL is not set in config")
            return None

        nebius_params = self.config.params.get("nebius", {})
        max_tokens_cfg = int(nebius_params.get("max_tokens", 1024))

        messages: List[Dict[str, Any]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        user_content: List[Dict[str, Any]] = [{"type": "text", "text": user_prompt}]
        if pil_images:
            for img in pil_images:
                user_content.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": image_to_data_url(img)},
                    }
                )
        messages.append({"role": "user", "content": user_content})

        payload = {
            "model": model_name,
            "messages": messages,
            "max_tokens": max_tokens_cfg,
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "validated_trait",
                        "description": "Return JSON with trait, validity, and reasoning.",
                        "parameters": ValidationSchema.model_json_schema(),
                    },
                }
            ],
            "tool_choice": {
                "type": "function",
                "function": {"name": "validated_trait"},
            },
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.nebius_api_key}",
        }

        try:
            resp_data = await self._make_request(payload, headers)

            if resp_data is None:
                logger.error(f"Nebius request failed after retries for {model_id}")
                self.error_models.append(model_id)
                return None

            message = resp_data.get("choices", [{}])[0].get("message", {})
            tool_calls = message.get("tool_calls", [])

            if not tool_calls:
                logger.error(f"No tool calls in Nebius {model_id} response")
                raise ValueError("No tool calls found in the response.")

            args_str = tool_calls[0].get("function", {}).get("arguments", "{}")
            logger.debug(f"Nebius {model_id} function args: {args_str}")
            args_dict = json.loads(args_str)

            result = self._safe_validate_response(args_dict, model_id)
            if result:
                return result
            else:
                raise ValueError(f"Validation failed for Nebius response: {args_dict}")

        except (
            aiohttp.ClientError,
            json.JSONDecodeError,
            KeyError,
            IndexError,
            ValueError,
        ) as exc:
            logger.error(f"Nebius execution error for {model_id}: {exc}")
            self.error_models.append(model_id)
            return None

--------------------------------------------------
src/llm_service/providers/openai.py
--------------------------------------------------
import json
import logging
from typing import List, Dict, Any, Optional

from PIL import Image
import aiohttp

from .base import BaseProvider
from ..utils import image_to_data_url, retry_with_backoff
from ..schemas import ValidationSchema

logger = logging.getLogger(__name__)


class OpenAIProvider(BaseProvider):
    """OpenAI API provider using function calling for structured output."""

    def resolve_model_id(self, model_id: str) -> Optional[str]:
        """Resolve OpenAI model ID with fallback to default."""
        if model_id in self.config.model_index:
            return self.config.model_index[model_id]

        fallback = self.config.defaults.get("openai_multimodal_default", "")
        return self.config.model_index.get(fallback)

    # TODO: remove this method because we are using batches instead
    @retry_with_backoff()
    async def _make_request(
        self, payload: Dict[str, Any], headers: Dict[str, str]
    ) -> Optional[Dict[str, Any]]:
        """Make HTTP request to OpenAI API."""
        if self.config.openai_base_url:
            async with self.session.post(
                self.config.openai_base_url, headers=headers, json=payload
            ) as r:
                if r.status != 200:
                    text = await r.text()
                    logger.warning(f"OpenAI response status {r.status}: {text[:200]}")
                r.raise_for_status()
                return await r.json()

    # TODO: turn this method into a method to create a request line that can be processed through the batch api, do not write it yet, because we need other metadata. The batch jsonl file should be created only ONCE, by the orchestrator
    async def execute(
        self,
        model_id: str,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        pil_images: Optional[List[Image.Image]] = None,
    ) -> Optional[Dict[str, Any]]:
        """Execute prompt using OpenAI function calling."""
        if not self.config.openai_api_key:
            logger.error("Cannot execute GPT prompt: API key is missing")
            return None

        model_name = self.resolve_model_id(model_id)
        if not model_name:
            logger.error(f"No OpenAI model found for: {model_id}")
            return None

        if not self.config.openai_base_url:
            logger.error("OpenAI API URL is not set in config")
            return None

        openai_params = self.config.params.get("openai", {})
        max_tokens_cfg = int(openai_params.get("max_tokens", 1024))

        messages: List[Dict[str, Any]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        user_content: List[Dict[str, Any]] = [{"type": "text", "text": user_prompt}]
        if pil_images:
            for img in pil_images:
                user_content.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": image_to_data_url(img)},
                    }
                )
        messages.append({"role": "user", "content": user_content})

        payload = {
            "model": model_name,
            "messages": messages,
            "max_tokens": max_tokens_cfg,
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "validated_trait",
                        "description": "Return JSON with trait, validity, and reasoning.",
                        "parameters": ValidationSchema.model_json_schema(),
                    },
                }
            ],
            "tool_choice": {
                "type": "function",
                "function": {"name": "validated_trait"},
            },
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.openai_api_key}",
        }

        try:
            resp_data = await self._make_request(payload, headers)

            if resp_data is None:
                logger.error(f"OpenAI request failed after retries for {model_id}")
                self.error_models.append(model_id)
                return None

            message = resp_data.get("choices", [{}])[0].get("message", {})
            tool_calls = message.get("tool_calls", [])

            if not tool_calls:
                logger.error(f"No tool calls in OpenAI {model_id} response")
                raise ValueError("No tool calls found in the response.")

            args_str = tool_calls[0].get("function", {}).get("arguments", "{}")
            logger.debug(f"OpenAI {model_id} function args: {args_str}")
            args_dict = json.loads(args_str)

            result = self._safe_validate_response(args_dict, model_id)
            if result:
                return result
            else:
                raise ValueError(f"Validation failed for OpenAI response: {args_dict}")

        except (
            aiohttp.ClientError,
            json.JSONDecodeError,
            KeyError,
            IndexError,
            ValueError,
        ) as exc:
            logger.error(f"OpenAI execution error for {model_id}: {exc}")
            self.error_models.append(model_id)
            return None

--------------------------------------------------
src/llm_service/schemas.py
--------------------------------------------------
from typing import List, Literal
from pydantic import BaseModel, Field, model_validator

class ValidTrait(BaseModel):
    """Schema that all valid traits should follow"""
    model_config = {"extra": "forbid"}

    trait: str = Field(..., description="The name of the trait being evaluated.")
    validity: Literal[True]
    reasoning: Literal[""]


class InvalidTrait(BaseModel):
    """Schema that all invalid traits should follow"""
    model_config = {"extra": "forbid"}

    trait: str = Field(..., description="The name of the trait being evaluated.")
    validity: Literal[False]
    reasoning: str = Field(..., description="One sentence explaining why the trait is invalid.", min_length=1, max_length=200)

TraitEvaluation = ValidTrait | InvalidTrait

class ValidationSchema(BaseModel):
    """Canonical output schema for a single trait evaluation."""

    model_config = {"extra": "forbid"}

    trait: str = Field(..., description="The name of the trait being evaluated.")
    validity: bool = Field(..., description="True if the trait is valid.")
    reasoning: str = Field(
        ..., 
        description="Explanation for invalid traits. Empty string when validity is True."
    )

    @classmethod
    def model_json_schema(cls):
        """
        Generates a JSON schema where ALL fields are required,
        and 'reasoning' is a non-nullable string.
        """
        schema = super().model_json_schema()

        if "properties" in schema:
            schema["required"] = list(schema["properties"].keys())

            reasoning_field = schema["properties"].get("reasoning", {})
            reasoning_field.pop("nullable", None)
            reasoning_field["type"] = "string"

        return schema

    @model_validator(mode="after")
    def _enforce_reasoning_rules(self):
        """
        Internal correctness:
        - validity=True  -> reasoning=""
        - validity=False -> reasoning must be a non-empty string
        """
        if self.validity is True:
            self.reasoning = ""
        else:
            if not self.reasoning or not self.reasoning.strip():
                raise ValueError(
                    "When validity is False, 'reasoning' must be a non-empty string."
                )
        return self


class ValidationListSchema(BaseModel):
    """Output schema for combined evaluation of all traits."""

    model_config = {"extra": "forbid"}

    traits_output: List[TraitEvaluation] = Field(
        ..., description="A list of validation results for all traits."
    )

    @classmethod
    def model_json_schema(cls):
        schema = super().model_json_schema()

        if "properties" in schema:
            # All fields required
            schema["required"] = list(schema["properties"].keys())

            # Ensure nested ValidationSchema.reasoning is non-nullable
            items_schema = schema["properties"]["traits_output"].get("items", {})
            reasoning = items_schema.get("properties", {}).get("reasoning", {})
            reasoning.pop("nullable", None)
            reasoning["type"] = "string"

        return schema


class NebiusTraitEvaluation(BaseModel):
    """Simplified schema that fits all results into one shape."""
    model_config = {"extra": "forbid"}

    trait: str = Field(..., description="The name of the trait.")
    validity: bool = Field(..., description="True if valid, False if invalid.")
    reasoning: str = Field(..., description="Explanation if invalid, empty string if valid.")

class NebiusValidationListSchema(BaseModel):
    """Compatible Schema for Nebius (Flat properties)"""
    model_config = {"extra": "forbid"}
    traits_output: List[NebiusTraitEvaluation] = Field(..., description="List of validations.")

--------------------------------------------------
src/llm_service/service.py
--------------------------------------------------
import logging
from typing import List, Dict, Any, Optional
import json
import asyncio

from PIL import Image
import aiohttp

from .config import ConfigManager
from .providers import OpenAIProvider, BaseProvider, NebiusProvider
from .providers.basic_batch import BaseBatchService

from openai import OpenAI


logger = logging.getLogger(__name__)


class OpenAIBatchService(BaseBatchService):
    """
    An implementation of the abstract BaseBatchService that can handle batches for OpenAI
    """

    def __init__(self):
        """
        Initializes the service with the necessary API key.
        """

        logger.info("ðŸ”§ Initializing OpenAIBatchService...")
        self.config = ConfigManager()
        if not self.config.openai_api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=self.config.openai_api_key)
        logger.info("âœ… OpenAI client initialized.")

    def submit_batch(self, filepath: str, name: Optional[str] = None) -> str:
        """
        Uploads a file and creates a new batch job on OpenAI
        """
        logger.info(f"Submitting batch file: {filepath}...")
        try:
            with open(filepath, "rb") as f:
                batch_input_file = self.client.files.create(file=f, purpose="batch")
            logger.debug(f"Uploaded file ID {batch_input_file.id}")

            # creating batch job based on the uploaded file
            description = name or f"Batch job for {filepath}"
            batch = self.client.batches.create(
                input_file_id=batch_input_file.id,
                endpoint="/v1/responses",
                completion_window="24h",
                metadata={"description": description},
            )

            logger.info(f"ðŸš€ Submitted batch job with ID: {batch.id}")
            return batch.id
        except Exception as e:
            logger.error(f"âŒ Batch submission failed: {e}")
            raise

    def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """
        Retrieves the status and metadata of a specific batch job from OpenAI
        """
        logger.info(f"Checking status for batch ID: {batch_id}...")
        try:
            batch = self.client.batches.retrieve(batch_id)

            status_info = {
                "id": batch.id,
                "status": batch.status,
                "created_at": batch.created_at,
                "completed_at": batch.completed_at,
                "request_counts": batch.request_counts,
                "output_file_id": batch.output_file_id,
                "error_file_id": batch.error_file_id,
            }
            logger.info(f"Status for batch {batch_id}: {batch.status.upper()}")

            return status_info
        except Exception as e:
            logger.error(f"âŒ Failed to retrieve status for batch {batch_id}: {e}")
            raise

    def download_batch_results(self, batch_id: str, output_path: str) -> bool:
        """Download complete batch results."""
        try:
            batch = self.client.batches.retrieve(batch_id)

            if batch.status != "completed":
                logger.warning(f"Batch {batch_id} status: {batch.status}")
                return False

            if not batch.output_file_id:
                logger.error(f"No output file for batch {batch_id}")
                return False

            result_content = self.client.files.content(batch.output_file_id)

            with open(output_path, "wb") as f:
                f.write(result_content.content)

            logger.info(f"ðŸ“¥ Downloaded results to {output_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ Download failed: {e}")
            return False

    def cancel_batch(self, batch_id: str) -> bool:
        try:
            self.client.batches.cancel(batch_id)
            return True

        except Exception as e:
            logger.error(f"âŒ Cancel batch failed: {e}")
            return False

    def delete_batch(self, batch_id: str) -> bool:
        """Deletes batch files if the batch is completed/failed/canceled"""
        try:
            batch = self.client.batches.retrieve(batch_id)

            if batch.status == "in_progress":
                logger.warning(
                    f"âš ï¸ Cannot delete a batch (batch id {batch.id}) that is being currently processed. Cancel it first."
                )
                return False

            if batch.input_file_id:
                self.client.files.delete(batch.input_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted input file: {batch.input_file_id}")

            if batch.output_file_id:
                self.client.files.delete(batch.output_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted output file: {batch.output_file_id}")

            if batch.error_file_id:
                self.client.files.delete(batch.error_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted error file: {batch.error_file_id}")

            logger.info(f"âœ… All files related to batch: {batch_id} have been deleted.")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to delete batch {batch_id}: {e}")
            return False


class NebiusBatchService(BaseBatchService):
    """
    An implementation of the abstract BaseBatchService that can handle batches for OpenAI
    """

    def __init__(self):
        """
        Initializes the service with the necessary API key.
        """

        logger.info("ðŸ”§ Initializing OpenAIBatchService...")
        self.config = ConfigManager()
        if not self.config.nebius_api_key:
            raise ValueError("Nebius API key is required.")
        self.client = OpenAI(
            base_url=self.config.nebius_base_url, api_key=self.config.nebius_api_key
        )
        logger.info("âœ… Nebius client initialized.")

    def submit_batch(self, filepath: str, name: Optional[str] = None) -> str:
        """
        Uploads a file and creates a new batch job on Nebius
        """
        logger.info(f"Submitting batch file: {filepath}...")
        try:
            with open(filepath, "rb") as f:
                batch_input_file = self.client.files.create(file=f, purpose="batch")
            logger.debug(f"Uploaded file ID {batch_input_file.id}")

            # creating batch job based on the uploaded file
            description = name or f"Batch job for {filepath}"
            batch = self.client.batches.create(
                input_file_id=batch_input_file.id,
                endpoint="/v1/chat/completions",
                completion_window="24h",
                metadata={"description": description},
            )

            logger.info(f"ðŸš€ Submitted batch job with ID: {batch.id}")
            return batch.id
        except Exception as e:
            logger.error(f"âŒ Batch submission failed: {e}")
            raise

    def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """
        Retrieves the status and metadata of a specific batch job from Nebius
        """
        logger.info(f"Checking status for batch ID: {batch_id}...")
        try:
            batch = self.client.batches.retrieve(batch_id)

            status_info = {
                "id": batch.id,
                "status": batch.status,
                "created_at": batch.created_at,
                "completed_at": batch.completed_at,
                "request_counts": batch.request_counts,
                "output_file_id": batch.output_file_id,
                "error_file_id": batch.error_file_id,
            }
            logger.info(f"Status for batch {batch_id}: {batch.status.upper()}")

            return status_info
        except Exception as e:
            logger.error(f"âŒ Failed to retrieve status for batch {batch_id}: {e}")
            raise

    def download_batch_results(self, batch_id: str, output_path: str) -> bool:
        """Download complete batch results."""
        try:
            batch = self.client.batches.retrieve(batch_id)

            if batch.status != "completed":
                logger.warning(f"Batch {batch_id} status: {batch.status}")
                return False

            if not batch.output_file_id:
                logger.error(f"No output file for batch {batch_id}")
                return False

            result_content = self.client.files.content(batch.output_file_id)

            with open(output_path, "wb") as f:
                f.write(result_content.content)

            logger.info(f"ðŸ“¥ Downloaded results to {output_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ Download failed: {e}")
            return False

    def cancel_batch(self, batch_id: str) -> bool:
        try:
            self.client.batches.cancel(batch_id)
            return True

        except Exception as e:
            logger.error(f"âŒ Cancel batch failed: {e}")
            return False

    def delete_batch(self, batch_id: str) -> bool:
        """Deletes batch files if the batch is completed/failed/canceled"""
        try:
            batch = self.client.batches.retrieve(batch_id)

            if batch.status == "in_progress":
                logger.warning(
                    f"âš ï¸ Cannot delete a batch (batch id {batch.id}) that is being currently processed. Cancel it first."
                )
                return False

            if batch.input_file_id:
                self.client.files.delete(batch.input_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted input file: {batch.input_file_id}")

            if batch.output_file_id:
                self.client.files.delete(batch.output_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted output file: {batch.output_file_id}")

            if batch.error_file_id:
                self.client.files.delete(batch.error_file_id)
                logger.info(f"ðŸ—‘ï¸ Deleted error file: {batch.error_file_id}")

            logger.info(f"âœ… All files related to batch: {batch_id} have been deleted.")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to delete batch {batch_id}: {e}")
            return False

--------------------------------------------------
src/llm_service/utils.py
--------------------------------------------------
import io
import json
import base64
import asyncio
import logging
from typing import Dict, Any, Optional, Callable, TypeVar, List, Awaitable
from functools import wraps

from PIL import Image
import aiohttp

logger = logging.getLogger(__name__)

T = TypeVar("T")


def _extract_core_fields(data: Dict[str, Any]) -> Dict[str, Any]:
    """Extract only the core fields needed for ValidationSchema, ignoring extras."""
    if not isinstance(data, dict):
        return {}

    core_keys = {"trait", "validity", "reasoning"}
    return {k: v for k, v in data.items() if k in core_keys}


def image_to_data_url(pil_image: Image.Image, format: str = "PNG") -> str:
    """Converts a PIL image into a base64 data URL."""
    buf = io.BytesIO()
    if pil_image.mode in ("RGBA", "P"):
        pil_image = pil_image.convert("RGB")

    pil_image.save(buf, format=format)
    b64_str = base64.b64encode(buf.getvalue()).decode("utf-8")

    return f"data:image/{format.lower()};base64,{b64_str}"


def write_jsonl_file(requests: List[Dict[str, Any]], filepath: str) -> None:
    with open(filepath, "a+", encoding="utf-8") as f:
        for request in requests:
            json.dump(request, f)
            f.write("\n")
    logger.info(f"ðŸ“ Wrote {len(requests)} requests to {filepath}")


def retry_with_backoff(
    max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 30.0
) -> Callable:
    """Decorator for automatic retry with exponential backoff."""

    def decorator(func: Callable[..., T]) -> Callable[..., Awaitable[Optional[T]]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> Optional[T]:
            last_exception = None

            for attempt in range(max_retries):
                try:
                    start_time = asyncio.get_event_loop().time()
                    result = func(*args, **kwargs)
                    if asyncio.iscoroutine(result):
                        result = await result
                    elapsed = asyncio.get_event_loop().time() - start_time
                    logger.debug(
                        f"Request completed in {elapsed:.2f}s on attempt {attempt + 1}"
                    )
                    return result

                except aiohttp.ClientResponseError as e:
                    if e.status == 429:
                        retry_after = (
                            dict(e.headers).get("Retry-After", "60")
                            if e.headers
                            else "60"
                        )
                        delay = float(retry_after)
                        logger.warning(
                            f"Rate limited. Waiting {delay}s as requested by server"
                        )
                        await asyncio.sleep(delay)
                        continue
                    last_exception = e

                except (asyncio.TimeoutError, aiohttp.ClientError, OSError) as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        logger.error(f"Max retries ({max_retries}) exceeded: {e}")
                        break

                    delay = min(base_delay * (2**attempt), max_delay)
                    logger.warning(
                        f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s..."
                    )
                    await asyncio.sleep(delay)

                except Exception as e:
                    logger.error(f"Non-retryable error on attempt {attempt + 1}: {e}")
                    last_exception = e
                    break

            if last_exception:
                logger.error(f"All retry attempts failed. Last error: {last_exception}")
            return None

        return wrapper

    return decorator

--------------------------------------------------
src/main.py
--------------------------------------------------
import asyncio
import pandas as pd
from pathlib import Path
import argparse
import logging
import time
import json
from typing import Optional, List, Dict, Any

from src.science_qa import ScienceQA
from src.orchestrator import Orchestrator
from src.llm_service.providers.basic_batch import BaseBatchService
from src.llm_service.service import (
    OpenAIBatchService,
    NebiusBatchService,
)
from src.config import settings
from src.img_traits_def import ImgTraitDefinition
from src_orig.orig_traits_def import OriginalTraitDefinition

# ---------- module-level logger to avoid NameError in helpers ----------
logger = logging.getLogger(__name__)


def setup_logging(level: str = "INFO"):
    """Configure logging for the application"""
 
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    logger.setLevel(getattr(logging, level.upper()))


def get_provider_from_model_id(model_id: str) -> str:
    """Determines the provider based on the model ID prefix"""
    if model_id.upper().startswith("GPT"):
        return "openai"
    elif model_id.upper().startswith("L"):
        return "nebius"
    raise ValueError(f"Could not determine provider for model {model_id}")


async def test_models_batch(
    model_ids: List[str],
    data: pd.DataFrame,
    orchestrator: Orchestrator,
    mode: str, # <-- ADDED
) -> List[Dict[str, Any]]:
    """
    Collect all batch requests across models and questions.
    """
    all_requests = []
    total_questions = len(data)

    for model_id in model_ids:
        provider = get_provider_from_model_id(model_id=model_id)
        # UPDATED print statement
        print(f"\nðŸ“‹ Collecting requests for {model_id} (Provider: {provider}, Mode: {mode})...")

        for i, (_, row) in enumerate(data.iterrows()):
            question = ScienceQA.from_df_row(row)

            if mode in ("single", "both"):
                single_reqs, _ = await orchestrator.prepare_batch_requests(
                    question, provider, model_id
                )
                all_requests.extend(single_reqs)

            if mode in ("combined", "both"):
                combined_reqs, _ = await orchestrator.prepare_combined_batch_requests(
                    question, provider, model_id
                )
                all_requests.extend(combined_reqs)
            if (i + 1) % 5 == 0:
                print(f"  ðŸ“ Processed {i + 1}/{total_questions} questions")

    return all_requests


async def monitor_batch_progress(
    batch_id: str, batch_service: BaseBatchService, check_interval: int = 300
):
    """Monitor batch progress until completion"""
    _log = logging.getLogger(__name__)
    _log.info(f"ðŸ‘€ Monitoring batch {batch_id}...")

    while True:
        try:
            status = batch_service.get_batch_status(batch_id)
            batch_status = status.get("status", "unknown")

            if batch_status == "completed":
                _log.info(f"ðŸŽ‰ Batch {batch_id} completed!")

                # Check for errr file
                error_file_id = status.get("error_file_id")
                if error_file_id:
                    _log.warning("âš ï¸ Batch has errors, checking error file...")
                    try:
                        error_content = batch_service.client.files.content(
                            error_file_id
                        )
                        text_preview = getattr(error_content, "text", None)
                        if text_preview:
                            _log.error(f"First error: {text_preview[:1000]}...")
                    except Exception as e:
                        _log.error(f"Could not retrieve error file: {e}")

                return True

            elif batch_status == "failed":
                _log.error(f"âŒ Batch {batch_id} failed!")
                return False

            elif batch_status == "in_progress":
                counts = status.get("request_counts", {})
                _log.info(f"â³ Progress: {counts}")

            await asyncio.sleep(check_interval)

        except Exception as e:
            _log.error(f"Error checking batch: {e}")
            await asyncio.sleep(check_interval)



async def main(args):
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)

    trait_def = ImgTraitDefinition(Path(settings["paths"]["trait_definitions_json"]))
    trait_list = list(trait_def.traits.keys())
    orchestrator = Orchestrator(
        trait_names=trait_list,
    )
    # --- Ad-hoc Batch Commands ---
    # These commands operate on a single provider, specified by the --provider flag.
    # --- Check for batch error retrieval ---
    if args.batch_error:
        if args.provider != "openai":
            print("âŒ Only OpenAI batches support error file retrieval currently.")
            return

        batch_service: OpenAIBatchService = orchestrator.openai_batch_service
        status = batch_service.get_batch_status(args.batch_error)
        error_file_id = status.get("error_file_id")

        if error_file_id:
            try:
                import openai
                content = openai.File.download(error_file_id)
                text = content.decode("utf-8")
                print(f"\nâš ï¸ Error file for batch {args.batch_error}:\n")
                print(text[:2000])  # show first 2k chars
            except Exception as e:
                print(f"âŒ Failed to download error file: {e}")
        else:
            print(f"âœ… Batch {args.batch_error} has no error file.")
        return  # exit after handling
    if (
        args.list_batches
        or args.cancel_batch
        or args.delete_batch
        or args.check_batch
        or args.download_batch
    ):
        batch_service: Optional[BaseBatchService] = None
        if args.provider == "openai":
            batch_service = OpenAIBatchService()
        elif args.provider == "nebius":
            # You might need to add a check here if Nebius doesn't support these commands
            batch_service = NebiusBatchService()

        if not batch_service:
            logger.error(
                f"Provider '{args.provider}' is not supported for ad-hoc commands."
            )
            return

        print(f"--- Running command for provider: {args.provider} ---")

        if args.list_batches:
            logger.info("Listing batches...")
            try:
                batches = batch_service.client.batches.list(limit=20)

                if not batches.data:
                    print("\nðŸ“‹ No batches found.")
                else:
                    print(f"\nðŸ“‹ Found {len(batches.data)} batches:\n")

                    for batch in batches.data:
                        print(f"ID: {batch.id}")
                        print(f"  Status: {batch.status}")
                        print(f"  Created: {batch.created_at}")

                        # Show progress if available
                        counts = batch.request_counts
                        print(
                            f"  Requests: {counts.total} total, {counts.completed} completed, {counts.failed} failed"
                        )
                        print("---")

            except Exception as e:
                logger.error(f"Failed to list batches: {e}")
                print(f"\nâŒ Error: {e}")

            return

        elif args.cancel_batch:
            success = batch_service.cancel_batch(args.cancel_batch)
            print(f"Cancellation status: {'Success' if success else 'Failed'}")

        elif args.delete_batch:
            success = batch_service.delete_batch(args.delete_batch)
            print(f"Deletion status: {'Success' if success else 'Failed'}")

        elif args.check_batch:
            status_info = batch_service.get_batch_status(args.check_batch)

            print(f"\nðŸ“Š Batch Status:")
            print(f"  ID: {status_info.get('id')}")
            print(f"  Status: {status_info.get('status')}")
            print(f"  Created: {status_info.get('created_at')}")
            print(f"  Completed: {status_info.get('completed_at')}")
            print(f"  Failed: {status_info.get('failed_at')}")

            counts = status_info.get("request_counts")
            if counts:
                print(f"\n  Requests:")
                print(f"    Total: {counts.total}")
                print(f"    Completed: {counts.completed}")
                print(f"    Failed: {counts.failed}")

            print(f"\n  Output File: {status_info.get('output_file_id')}")
            print(f"  Error File: {status_info.get('error_file_id')}")

            if status_info.get("error_details"):
                print(f"\n  âš ï¸ Error Details:")
                print(f"    {status_info['error_details'][:500]}...")

        elif args.download_batch:
            output_path = Path(f"./{args.download_batch}_results.jsonl")
            success = batch_service.download_batch_results(
                args.download_batch, str(output_path)
            )
            if success:
                (f"ðŸ“¥ Results downloaded to: {output_path}")

        return  # Exit after running the ad-hoc command

    # --- Main Evaluation Logic ---

    test_df = pd.read_csv(Path(settings["paths"]["input_data_csv"])).head(
        args.num_questions
    )
    model_ids_to_test = args.models.split(",") if args.models else ["GPT5Nano"]

    # Create the Orchestrator once.
    orchestrator = Orchestrator(
        trait_names=trait_list,
    )

    if args.batch:
        print(f"\nðŸš€ BATCH MODE: Collecting requests (Mode: {args.mode})...")

        # If a pre-built JSONL file was provided, skip generating requests
        if args.batch_file:
            provided = Path(args.batch_file)
            if not provided.exists():
                print(f"âŒ Provided batch file does not exist: {provided}")
                return

            # Determine provider from flag
            if args.provider == "openai":
                # Submit existing OpenAI jsonl file
                openai_file = provided
                print(f"ðŸ“¤ Submitting existing OpenAI batch file: {openai_file}")
                if args.submit_batch:
                    batch_id = orchestrator.openai_batch_service.submit_batch(str(openai_file), args.batch_name)
                    print(f"âž¡ï¸ Submitted OpenAI batch id: {batch_id}")
                    if batch_id and args.monitor:
                        await monitor_batch_progress(batch_id, orchestrator.openai_batch_service, args.check_interval)
            elif args.provider == "nebius":
                nebius_file = provided
                print(f"ðŸ“¤ Submitting existing Nebius batch file: {nebius_file}")
                if args.submit_batch:
                    batch_id = orchestrator.nebius_batch_service.submit_batch(str(nebius_file), args.batch_name)
                    print(f"âž¡ï¸ Submitted Nebius batch id: {batch_id}")
                    if batch_id and args.monitor:
                        await monitor_batch_progress(batch_id, orchestrator.nebius_batch_service, args.check_interval)
            else:
                print(f"âŒ Unsupported provider for direct file submission: {args.provider}")
            return  # done

        # --- Otherwise: generate requests as before ---

        all_requests = await test_models_batch(
            model_ids_to_test, test_df, orchestrator,
            mode=args.trait_mode
        )

        # Group by provider, strategy, and model so each model gets its own JSONL.
        groups: dict[tuple[str, str, str], list[dict[str, Any]]] = {}
        for r in all_requests:
            provider = r.get("_provider")
            strategy = r.get("_strategy", "single")
            model_id = r.get("_model_id", "unknown")
            if provider is None:
                continue
            key = (provider, strategy, model_id)
            groups.setdefault(key, []).append(r)

        # Helper to drop internal metadata keys (those starting with "_")
        def _clean(reqs: list[dict[str, Any]]) -> list[dict[str, Any]]:
            return [{k: v for k, v in r.items() if not k.startswith("_")} for r in reqs]

        base_path = Path(settings["paths"]["batch_request_file"])

        for (provider, strategy, model_id), reqs in groups.items():
            cleaned = _clean(reqs)
            safe_model = "".join(c if c.isalnum() else "_" for c in model_id)
            suffix_provider = "openai" if provider == "openai" else "nebius"
            output_file = base_path.with_name(
                f"{base_path.stem}_{suffix_provider}_{safe_model}_{strategy}{base_path.suffix}"
            )

            # Choose correct batch service
            if provider == "openai":
                orchestrator.openai_batch_service.write_jsonl_file(cleaned, str(output_file))
            elif provider == "nebius":
                orchestrator.nebius_batch_service.write_jsonl_file(cleaned, str(output_file))
            else:
                continue

            print(f"ðŸ’¾ Saved {len(cleaned)} {provider.upper()} {strategy} requests for {model_id} to: {output_file}")

            # Optionally submit this file depending on strategy / trait_mode
            should_submit = args.submit_batch and (
                args.trait_mode == strategy or args.trait_mode == "both"
            )
            if not should_submit:
                continue

            if provider == "openai":
                batch_id = orchestrator.openai_batch_service.submit_batch(
                    str(output_file), args.batch_name
                )
                if batch_id and args.monitor:
                    await monitor_batch_progress(
                        batch_id, orchestrator.openai_batch_service, args.check_interval
                    )
            elif provider == "nebius":
                batch_id = orchestrator.nebius_batch_service.submit_batch(
                    str(output_file), args.batch_name
                )
                if batch_id and args.monitor:
                    await monitor_batch_progress(
                        batch_id, orchestrator.nebius_batch_service, args.check_interval
                    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Agentic Framework Evaluation.")

    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Set the logging level (default: INFO).",
    )

    parser.add_argument(
        "--batch-file",
        type=str,
        default=None,
        help="Path to an existing JSONL batch file to submit (skip writing new JSONL).",
    )

    parser.add_argument(
        "-tm",
        "--trait-mode",
        choices=["single", "combined", "both"],
        default="both",
        help="How to build batch requests: per-trait agents, combined traits, or both. Default: both."
    )

    parser.add_argument(
        "-p",
        "--provider",
        type=str,
        default="nebius",
        choices=["nebius", "openai"],
        help="Choose the provider: either 'nebius' or 'openai' (default: 'nebius'). This will influence the models available.",
    )

    parser.add_argument(
        "-m",
        "--models",
        type=str,
        help='Comma-separated list of model IDs to test (e.g., "GPT4o,L_Gemma34B").',
    )

    parser.add_argument(
        "-n",
        "--num-questions",
        type=int,
        default=2,
        help="Number of questions to process from the test set.",
    )

    parser.add_argument(
        "-o",
        "--output-dir",
        type=str,
        help="Path to the output directory for results (overrides config.toml).",
    )

    parser.add_argument(
        "--batch",
        action="store_true",
        help="Use OpenAI batch processing (collect requests, submit batch, exit)",
    )

    parser.add_argument(
        "--batch-name", type=str, help="Optional custom name for batch identification"
    )

    parser.add_argument(
        "--submit-batch",
        action="store_true",
        help="Actually submit the batch to OpenAI (default: just generate JSONL)",
    )

    parser.add_argument("--check-batch", type=str, help="Check status of batch by ID")

    parser.add_argument(
        "--quiet", action="store_true", help="Minimal output for scripting"
    )

    parser.add_argument(
        "--download-batch", type=str, help="Download results for completed batch by ID"
    )

    parser.add_argument(
        "--list-batches", action="store_true", help="List all batches and their status"
    )

    parser.add_argument("--cancel-batch", type=str, help="Cancel a batch by ID")

    parser.add_argument(
        "--delete-batch",
        type=str,
        help="Delete a batch by ID (only works for completed/failed batches)",
    )

    parser.add_argument(
        "--monitor",
        action="store_true",
        help="Automatically monitor batch after submission",
    )

    parser.add_argument(
        "--check-interval",
        type=int,
        default=300,
        help="Batch monitoring interval in seconds (default: 300)",
    )
    
    parser.add_argument(
        "--batch-error",
        type=str,
        help="Retrieve and display error file for failed batch by ID",
    )
    # --- SINGLE VS MULTIPLE TRAIT COMPARISON ---  #TODO: update this
    parser.add_argument(
        "--mode",
        type=str,
        default="singular",
        choices=["singular", "combined"],
        help="Evaluation mode: 'singular' (one request per trait) or 'combined' (one request for all traits)."
    )
    
    args = parser.parse_args()
    asyncio.run(main(args))

--------------------------------------------------
src/normalize_nebius.py
--------------------------------------------------
#!/usr/bin/env python3
import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, Optional


def transform_line(line: str) -> Optional[Dict[str, Any]]:
    """
    Transforms a Nebius JSONL line into the specific
    format expected by the user's ResultsHandler.
    (i.e., response.body.output[...].text)
    """
    try:
        data = json.loads(line)
    except json.JSONDecodeError:
        print(f"Skipping bad JSON line: {line[:50]}...", file=sys.stderr)
        return None

    # Check if it's already in the target format (has body.output)
    if (
        data.get("response")
        and isinstance(data["response"], dict)
        and data["response"].get("body")
        and isinstance(data["response"]["body"], dict)
        and "output" in data["response"]["body"]
    ):
        return data  # It's already converted or is an OpenAI file

    original_response = data.get("response")
    original_error = data.get("error")
    new_response_wrapper: Dict[str, Any] = {}

    # If there's no response, just format the error
    if not original_response or not isinstance(original_response, dict):
        new_response_wrapper = {
            "status_code": 500,
            "request_id": None,
            "body": {"error": original_error or "Missing response object"},
        }
    else:
        # This is the Nebius "response" object:
        # {"id": "chatcmpl-...", "choices": [...], "created": ..., "model": ...}

        # Extract the content string
        text_content = None
        try:
            text_content = original_response["choices"][0]["message"]["content"]
        except (KeyError, IndexError, TypeError):
            print(
                f"Warning: Could not find text content for custom_id {data.get('custom_id')}",
                file=sys.stderr,
            )

        # --- THIS IS THE CRITICAL CHANGE ---
        # Build the nested 'body' structure that ResultsHandler expects
        new_body = {
            "model": original_response.get("model"),
            "created_at": original_response.get("created"),
            "status": "completed" if text_content else "failed",
            "error": None,
            "output": [  # The 'output' list
                {
                    "content": [  # The 'content' list
                        {
                            "type": "output_text",
                            "text": text_content,  # Put the JSON string here
                        }
                    ]
                }
            ],
        }
        # --- END OF CHANGE ---

        new_response_wrapper = {
            "status_code": 200,
            "request_id": original_response.get("id"),
            "body": new_body,  # Use our newly constructed body
        }

    # Reconstruct the top-level object
    new_data = {
        "id": data.get("id"),
        "custom_id": data.get("custom_id"),
        "response": new_response_wrapper,
        "error": original_error,  # This is the top-level Nebius error, if any
    }

    return new_data


def main():
    parser = argparse.ArgumentParser(
        description="Convert Nebius batch JSONL to the format expected by ResultsHandler."
    )
    parser.add_argument(
        "input_file",
        type=Path,
        help="Path to the original Nebius results JSONL file.",
    )
    parser.add_argument(
        "output_file",
        type=Path,
        help="Path to write the new custom-formatted JSONL file.",
    )
    args = parser.parse_args()

    args.input_file = args.input_file.expanduser().resolve()
    args.output_file = args.output_file.expanduser().resolve()

    if not args.input_file.exists():
        print(f"âŒ Input file not found: {args.input_file}", file=sys.stderr)
        sys.exit(1)

    print(f"Converting {args.input_file.name} -> {args.output_file.name}")

    lines_processed = 0
    lines_written = 0

    with open(args.input_file, "r") as infile, open(args.output_file, "w") as outfile:
        for line in infile:
            lines_processed += 1
            transformed_data = transform_line(line)

            if transformed_data:
                outfile.write(json.dumps(transformed_data) + "\n")
                lines_written += 1

    print(f"âœ… Done. Processed {lines_processed} lines, wrote {lines_written} lines.")


if __name__ == "__main__":
    main()

--------------------------------------------------
src/orchestrator.py
--------------------------------------------------
from typing import List, Dict, Any, Optional
import logging

from src.trait_agent import TraitAgent
from src.llm_service.service import OpenAIBatchService, NebiusBatchService
from src.science_qa import ScienceQA
from src.combined_traits_builder import CombinedTraitsBuilder

logger = logging.getLogger(__name__)


class Orchestrator:
    """
    Manages a team of TraitAgents to evaluate a ScienceQA problem.
    This version runs agents sequentially to ensure stability on systems
    with computation bottlenecks (e.g., CPU-only or driver issues).
    The evaluations can be either performed in real-time or via batches.
    """

    def __init__(
        self,
        trait_names: List[str]
    ):
        logger.info("ðŸŽ¶ Initializing Sequential Orchestrator...")

        self.openai_batch_service = OpenAIBatchService()
        self.nebius_batch_service = NebiusBatchService()

        # self.all_agents = [TraitAgent(name) for name in trait_names]  ALL TRAITS
        self.all_agents = [TraitAgent(name) for name in trait_names]  

        # combined builder to check all traits at once for each question
        self.combined_builder = CombinedTraitsBuilder(trait_list=trait_names)

        logger.info(f"âœ… Orchestrator created with {len(self.all_agents)} agents.")

    async def prepare_batch_requests(
        self, question_data: ScienceQA, provider: str, model_id: str
    ) -> tuple[list[Dict[str, Any]], list[str]]:
        """
        Prepares batch requests for a single question by collecting requests from all agents.
        It handles image formats based on the provider.
        Returns list of request dictionaries ready for JSONL batch file.
        """

        pil_images = []
        image_file_ids = []

        if provider == "openai":
            image_file_ids = await question_data.upload_images_to_openai(
                self.openai_batch_service.client
            )

        else:
            pil_images = await question_data.load_images()

        requests = []
        qid = question_data.id
        total_agents = len(self.all_agents)
        logger.info(
            f"ðŸ”§ Orchestrator preparing batch requests for QID {qid} with {total_agents} agents..."
        )
        for _, agent in enumerate(self.all_agents):
            agent_name = agent.trait_name
            logger.debug(f"Preparing request for agent '{agent_name}' (QID {qid})")

            try:
                request = agent.prepare_single_request(
                    question_data=question_data,
                    provider=provider,
                    model_id=model_id,
                    pil_images=pil_images,
                    image_file_ids=image_file_ids,
                )

                if request is not None:
                    request["_provider"] = provider
                    request["_strategy"] = "single"
                    request["_model_id"] = model_id
                    requests.append(request)
                else:
                    logger.warning(
                        f"Agent '{agent_name}' returned None for batch request preparation"
                    )
            except Exception as e:
                logger.error(
                    f"Error preparing batch request for agent '{agent_name}': {e}"
                )
                continue

        logger.info(
            f"ðŸ“¦ Orchestrator prepared {len(requests)} batch requests for QID {qid}"
        )
        return requests, image_file_ids

    async def prepare_combined_batch_requests(
        self, question_data: ScienceQA, provider: str, model_id: str) -> tuple[list[Dict[str, Any]], list[str]]:
        pil_images = []
        image_file_ids: list[str] = []

        if provider == "openai":
            image_file_ids = await question_data.upload_images_to_openai(
                self.openai_batch_service.client
            )
        else:
            pil_images = await question_data.load_images()

        req = self.combined_builder.prepare_single_request(
            question_data=question_data,
            provider=provider,
            model_id=model_id,
            pil_images=pil_images,
            image_file_ids=image_file_ids,
        )

        requests: list[Dict[str, Any]] = []
        if req is not None:
            req["_provider"] = provider
            req["_strategy"] = "combined"
            req["_model_id"] = model_id
            requests.append(req)

        return requests, image_file_ids

--------------------------------------------------
src/process_all_results.py
--------------------------------------------------
"""
Batch processor that finds all .jsonl results in data/batch_results,
processes them using ResultsHandler, and saves them to data/processed_results.
"""
import logging
import sys
from pathlib import Path
import pandas as pd

# Ensure src is in path if running as script
sys.path.append(str(Path(__file__).parents[1]))

from src.results_handler import ResultsHandler
from src.config import PROJECT_ROOT

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("process_all")


def process_all_batches():
    # 1. Define Directories
    input_root = PROJECT_ROOT / "data" / "batch_results"
    output_root = PROJECT_ROOT / "data" / "processed_results"

    if not input_root.exists():
        logger.error(f"âŒ Input directory not found: {input_root}")
        return

    # 2. Find all JSONL files recursively
    jsonl_files = list(input_root.rglob("*.jsonl"))
    
    if not jsonl_files:
        logger.warning(f"âš ï¸ No .jsonl files found in {input_root}")
        return

    logger.info(f"ðŸš€ Found {len(jsonl_files)} batch result files to process.")

    success_count = 0
    error_count = 0

    # 3. Iterate and Process
    for i, jsonl_path in enumerate(jsonl_files, 1):
        try:
            # Calculate relative path structure (e.g., nebius/L_Gemma327B/combined)
            relative_path = jsonl_path.relative_to(input_root).parent
            filename_stem = jsonl_path.stem  # Filename without .jsonl

            # Create output path: processed_results/{provider}/{model}/{mode}/{batch_filename}/
            # We add the filename_stem to avoid overwriting if multiple batches exist for one mode
            target_dir = output_root / relative_path / filename_stem
            
            logger.info(f"[{i}/{len(jsonl_files)}] Processing: {jsonl_path.name}")
            logger.debug(f"   ðŸ“ Target: {target_dir}")

            # Initialize Handler
            rh = ResultsHandler(target_dir)

            # Load
            raw_df = rh.load_batch_results_jsonl(jsonl_path)
            if raw_df.empty:
                logger.warning(f"   âš ï¸ Empty dataframe for {jsonl_path.name}. Skipping.")
                error_count += 1
                continue

            # Save Raw
            rh.save_raw_results(raw_df)

            # Clean & Save
            clean_df = rh.clean_results(raw_df)
            if clean_df.empty:
                logger.warning(f"   âš ï¸ Cleaning resulted in empty data for {jsonl_path.name}.")
            else:
                rh.save_clean_results(clean_df)
                logger.info(f"   âœ… Saved clean CSV with {len(clean_df)} rows.")
                success_count += 1

        except Exception as e:
            logger.error(f"   âŒ Failed to process {jsonl_path.name}: {e}")
            error_count += 1

    # 4. Summary
    logger.info("=" * 40)
    logger.info(f"ðŸŽ‰ Processing Complete.")
    logger.info(f"âœ… Successful: {success_count}")
    logger.info(f"âŒ Failed/Empty: {error_count}")
    logger.info(f"ðŸ“‚ Results stored in: {output_root}")


if __name__ == "__main__":
    process_all_batches()

--------------------------------------------------
src/prompt_template.py
--------------------------------------------------
system_prompt_template = """
âš ï¸ CRITICAL: You are evaluating the IMAGE'S visual design quality, NOT looking for question text within the image. The question is provided separately. Do NOT expect to see the question or answer options written inside the image.

You are evaluating whether an image demonstrates good visual design for the trait: "{trait_name}".

-------------------------------
TRAIT: {trait_name}
Definition: {definition}
Focus: {note}

-------------------------------
EVALUATION QUESTIONS:
{evaluation_questions}

-------------------------------
COMMON MISTAKES TO AVOID:
âŒ "The image doesn't contain the question text" - CORRECT: Question text is provided separately
âŒ "I can't see answer options in the image" - CORRECT: Options aren't supposed to be in the image
âŒ "The image doesn't show the question" - EVALUATE: The image's visual design quality only
âœ… FOCUS ON: Visual design principles of the image itself

-------------------------------

REQUIRED OUTPUT FORMAT (STRICT SCHEMA):
You must choose strictly between two output formats:

1. [VALID] If the image MEETS the design standard:
   {{"trait": "{trait_name}", "validity": true, "reasoning": ""}}

2. [INVALID] If the image FAILS the design standard:
   {{"trait": "{trait_name}", "validity": false, "reasoning": "Brief explanation of design flaw (ONE SENTENCE)"}}

âš ï¸ CRITICAL RULES:
- When validity is true, reasoning MUST be an empty string (""). It cannot be null or omitted.
- When validity is false, reasoning MUST be a non-empty string.
""".strip()
# -------------------------------
# EXAMPLES:
# {examples}

combi_system_prompt_template = """
âš ï¸ CRITICAL: You are evaluating the IMAGE'S visual design quality, NOT looking for question text within the image. The question is provided separately. Do NOT expect to see the question or answer options written inside the image.

You are evaluating whether an image demonstrates good visual design for the following traits: "{traits_list}".
-------------------------------
TRAITS TO EVALUATE:
{traits_info}

-------------------------------
COMMON MISTAKES TO AVOID:
âŒ "The image doesn't contain the question text" - CORRECT: Question text is provided separately
âŒ "I can't see answer options in the image" - CORRECT: Options aren't supposed to be in the image
âŒ "The image doesn't show the question" - EVALUATE: The image's visual design quality only
âœ… FOCUS ON: Visual design principles of the image itself

-------------------------------

REQUIRED OUTPUT FORMAT (STRICT SCHEMA):
You MUST return a JSON object with a single key "traits_output".
"traits_output" MUST be a LIST of JSON objects.

For EACH trait in the list, you must strictly adhere to one of these two formats:

1. [VALID]
   {{"trait": "TraitName", "validity": true,  "reasoning": ""}}

2. [INVALID]
   {{"trait": "TraitName", "validity": false, "reasoning": "Brief explanation of design flaw (ONE SENTENCE)"}}

âš ï¸ CRITICAL RULES:
- If validity is true, reasoning MUST be an empty string ("").
- If validity is false, reasoning MUST be a non-empty string.

The final output structure MUST look like:
{{
  "traits_output": [
    {{"trait": "TraitName1", "validity": true,  "reasoning": ""}},
    {{"trait": "TraitName2", "validity": false, "reasoning": "Brief explanation of design flaw (ONE SENTENCE)"}},
    {{"trait": "TraitName3", "validity": true,  "reasoning": ""}}
  ]
}}
""".strip()


--------------------------------------------------
src/results_handler.py
--------------------------------------------------
import json
import re
import csv
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import pandas as pd


class ResultsHandler:
    """Handles saving, cleaning, and managing evaluation result files."""

    # Pattern based on your custom_id convention:
    # request-<ModelAlias>-<trait_name>-<question_id>-<timestamp>
    CID_RX = re.compile(
        r"^request-(?P<model>[^-]+)-(?P<trait>[^-]+)-(?P<qid>\d+)-(?P<ts>\d+)$"
    )

    def __init__(self, results_dir: Path):
        self.RESULTS_DIR = results_dir
        self.RESULTS_DIR.mkdir(parents=True, exist_ok=True)

        self.RAW_RESULTS_FILENAME = self.RESULTS_DIR / "raw_evaluation_results.csv"
        self.CLEAN_RESULTS_FILENAME = self.RESULTS_DIR / "clean_evaluation_results.csv"

    def load_batch_results_jsonl(self, jsonl_path: Path) -> pd.DataFrame:
        """
        Load Batch API JSONL output and normalize into a DataFrame.
        Handles flattening of 'combined' trait results (1 request -> N rows).
        """
        rows: List[Dict[str, Any]] = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for ln, line in enumerate(f, 1):
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except json.JSONDecodeError as e:
                    rows.append(
                        {
                            "_line": ln,
                            "_parse_error": str(e),
                            "custom_id": None,
                            "model_id": "PARSE_ERROR",
                            "trait": None,
                            "question_id": None,
                            "validity": None,
                            "reasoning": None,
                        }
                    )
                    continue

                # Parse the object. This returns a LIST of rows.
                # We use .extend() to flatten them into the main list.
                parsed_items = self._parse_batch_object(obj, ln)
                rows.extend(parsed_items)

        df = pd.DataFrame(rows)

        # Define column order (standard fields first, extras last)
        preferred = [
            "custom_id",
            "question_id",
            "model_id",
            "model_reported",
            "trait",
            "validity",
            "reasoning",
            "http_status",
            "request_status",
            "created_at",
            "error",
            "_raw_text",
            "_line",
            "_parse_error",
            "timestamp_token",
        ]
        cols = [c for c in preferred if c in df.columns] + [
            c for c in df.columns if c not in preferred
        ]
        return df[cols]

    @staticmethod
    def _extract_output_text(response_obj: Dict[str, Any]) -> Optional[str]:
        """
        Extract the inner JSON string emitted by the model.
        Handles both OpenAI 'Responses' API (body -> output)
        and Standard/Nebius 'Chat' API (choices -> message -> content).
        """
        # 1. Normalize: Get the inner 'body' or use response_obj itself if no body wrapper
        body = response_obj.get("body")
        if not body:
            # Nebius/Standard format often puts 'choices' directly in 'response'
            body = response_obj

        # 2. Try OpenAI "Responses" API format (output_text)
        out = body.get("output", [])
        if isinstance(out, list):
            for msg in out:
                content = msg.get("content", [])
                if isinstance(content, list):
                    for part in content:
                        if part.get("type") == "output_text":
                            return part.get("text", "")

        # 3. Try Standard Chat Completions format (Nebius/OpenAI Chat)
        choices = body.get("choices", [])
        if isinstance(choices, list) and len(choices) > 0:
            message = choices[0].get("message", {})
            content = message.get("content")
            if content:
                return str(content)

        return None

    @staticmethod
    def _clean_json_text(text: str) -> str:
        """Strips markdown code fences (```json ... ```) to ensure json.loads works."""
        if not isinstance(text, str):
            return ""
        t = text.strip()
        if t.startswith("```"):
            # Remove first line (e.g. ```json)
            parts = t.split("\n", 1)
            if len(parts) > 1:
                t = parts[1]
            # Remove last line (```)
            if t.endswith("```"):
                t = t.rsplit("\n", 1)[0]
            else:
                t = t.strip("`")
        return t.strip()

    @staticmethod
    def _coerce_bool(val: Any) -> Optional[bool]:
        """Convert diverse truthy/falsey representations to real booleans."""
        if isinstance(val, bool):
            return val
        if isinstance(val, (int, float)):
            if val in (0, 1):
                return bool(val)
            return None
        if isinstance(val, str):
            v = val.strip().lower()
            if v in {"true", "t", "yes", "y", "1"}:
                return True
            if v in {"false", "f", "no", "n", "0"}:
                return False
        return None

    def _parse_batch_object(self, obj: Dict[str, Any], ln: int) -> List[Dict[str, Any]]:
        """
        Parse a single JSONL object from the batch output.
        Returns a LIST of normalized rows to handle combined traits (One-to-Many).
        """
        custom_id = obj.get("custom_id", "")
        response = obj.get("response", {}) or {}
        
        status_code = response.get("status_code")
        # Nebius might not have a status code in the response dict, usually 200 if present
        if status_code is None and "choices" in response:
            status_code = 200

        # Metadata extraction
        # Check both 'body' and 'response' levels for metadata
        body = response.get("body") or response
        
        model_reported = body.get("model")
        created_at = body.get("created_at")
        req_status = body.get("status") 
        error = obj.get("error") # Check top level error too
        request_id = response.get("request_id") or body.get("id")

        # Derive fields from custom_id
        trait_hint = qid = ts = None
        model_alias = None
        m = self.CID_RX.search(custom_id) if isinstance(custom_id, str) else None
        if m:
            model_alias = m.group("model")
            trait_hint = m.group("trait") 
            qid = m.group("qid")
            ts = m.group("ts")

        # --- Base row template (common to all traits in this request) ---
        base_row = {
            "_line": ln,
            "custom_id": custom_id,
            "request_id": request_id,
            "http_status": status_code,
            "request_status": req_status,
            "created_at": created_at,
            "error": error,
            "model_reported": model_reported,
            "model_id": model_alias or model_reported,
            "question_id": qid,
            "timestamp_token": ts,
            "trait": trait_hint, # Fallback trait
            "validity": None,
            "reasoning": None,
            "_raw_text": None,
        }

        # Extract Text Payload
        raw_text = self._extract_output_text(response)
        if not raw_text:
            # If no text (e.g. error), return one row with error details
            return [base_row]

        # Clean and Parse JSON
        cleaned_text = self._clean_json_text(raw_text)
        try:
            inner_json = json.loads(cleaned_text)
        except json.JSONDecodeError:
            base_row["_raw_text"] = raw_text # preserve raw for debugging
            return [base_row]

        # --- LOGIC: Determine if output is Singular or Combined ---
        
        items_to_process = []

        # Case A: It's a list directly (e.g., [ {trait...}, {trait...} ])
        if isinstance(inner_json, list):
            items_to_process = inner_json
        
        # Case B: It's a dict wrapper (e.g. { "traits_output": [...] } )
        elif isinstance(inner_json, dict):
            if "traits_output" in inner_json and isinstance(inner_json["traits_output"], list):
                items_to_process = inner_json["traits_output"]
            elif "traits" in inner_json and isinstance(inner_json["traits"], list):
                items_to_process = inner_json["traits"]
            else:
                # Case C: It's a single trait object (e.g. { "trait": ..., "validity": ... })
                items_to_process = [inner_json]
        
        if not items_to_process:
            # Empty list or dict? Return base row
            return [base_row]

        # --- Generate a row for each trait found ---
        generated_rows = []
        for item in items_to_process:
            if not isinstance(item, dict): 
                continue

            new_row = base_row.copy()
            
            # 1. Get specific trait name from JSON, fall back to custom_id hint
            json_trait = item.get("trait")
            new_row["trait"] = json_trait if json_trait else trait_hint
            
            # 2. Get validity/reasoning
            new_row["validity"] = self._coerce_bool(item.get("validity"))
            new_row["reasoning"] = item.get("reasoning")
            
            generated_rows.append(new_row)

        return generated_rows

    def save_raw_results(self, df: pd.DataFrame):
        """Saves the raw DataFrame of results to a CSV file."""
        self.RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        df.to_csv(self.RAW_RESULTS_FILENAME, index=False, quoting=csv.QUOTE_ALL)
        ok = self.RAW_RESULTS_FILENAME.exists()
        size = self.RAW_RESULTS_FILENAME.stat().st_size if ok else 0
        print(
            f"ðŸ’¾ Raw results saved to {self.RAW_RESULTS_FILENAME} ({'OK' if ok else 'MISSING'}, {size} bytes)"
        )

    def save_clean_results(self, df: pd.DataFrame):
        """Saves the cleaned DataFrame to a CSV file."""
        self.RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        df.to_csv(self.CLEAN_RESULTS_FILENAME, index=False, quoting=csv.QUOTE_ALL)
        ok = self.CLEAN_RESULTS_FILENAME.exists()
        size = self.CLEAN_RESULTS_FILENAME.stat().st_size if ok else 0
        print(
            f"âœ… Clean results saved to {self.CLEAN_RESULTS_FILENAME} ({'OK' if ok else 'MISSING'}, {size} bytes)"
        )

    def clean_results(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Filters out rows with errors, normalizes columns, and attempts data recovery.
        """
        clean_df = df.copy()
        print(f"Starting cleaning with {len(clean_df)} rows")

        # 1. Question ID fallback logic (Regex extraction from custom_id)
        if "question_id" in clean_df.columns:
            null_qid_mask = clean_df["question_id"].isna()
            if null_qid_mask.sum() > 0:
                print(f"DIAGNOSTIC: {null_qid_mask.sum()} rows have null question_id. Attempting recovery.")
                
                def extract_qid_fallback(custom_id):
                    if pd.isna(custom_id) or custom_id == "": 
                        return None
                    import re
                    # Patterns to find ID in strings like "request-model-trait-123-ts"
                    patterns = [r"question[_-](\d+)", r"qid[_-](\d+)", r"id[_-](\d+)", r"-(\d+)-"]
                    for pattern in patterns: 
                        match = re.search(pattern, str(custom_id), re.IGNORECASE)
                        if match: return match.group(1)
                    # Last resort: find numbers
                    numbers = re.findall(r"\d+", str(custom_id))
                    return max(numbers, key=len) if numbers else None

                clean_df.loc[null_qid_mask, "question_id"] = clean_df.loc[null_qid_mask, "custom_id"].apply(extract_qid_fallback)

            # Standardize type
            clean_df["question_id"] = clean_df["question_id"].astype(str)

        # 2. Validity Coercion & Recovery
        if "validity" in clean_df.columns:
            clean_df["validity"] = clean_df["validity"].apply(self._coerce_bool)
            
            # Try to recover from raw text if validity is missing but we have text
            missing_validity = clean_df["validity"].isna() & clean_df["_raw_text"].notna()
            if missing_validity.sum() > 0:
                print(f"RECOVERY: Attempting to recover {missing_validity.sum()} validity values from raw text")
                def recover_validity(text):
                    t = str(text).lower()
                    if "true" in t and "false" not in t: return True
                    if "false" in t and "true" not in t: return False
                    return None
                
                recovered = clean_df.loc[missing_validity, "_raw_text"].apply(recover_validity)
                clean_df.loc[missing_validity, "validity"] = recovered

        # 3. Trait Recovery from custom_id if missing in JSON
        if "trait" in clean_df.columns:
             missing_trait = clean_df["trait"].isna() & clean_df["custom_id"].notna()
             if missing_trait.sum() > 0:
                 print(f"RECOVERY: Attempting to recover {missing_trait.sum()} traits from custom_id")
                 # Map your filename slugs to nice names
                 traits_map = {
                     "functional_relevance": "Functional Relevance",
                     "visual_clarity": "Visual Clarity",
                     "technical_quality": "Technical Quality", 
                     "standard_presentation": "Standard Presentation",
                     "text-image_coherence": "Text-Image Coherence",
                     "fair_representation": "Fair Representation"
                 }
                 def recover_trait(cid):
                     c_lower = str(cid).lower().replace("-", "_")
                     for key, val in traits_map.items():
                         if key in c_lower: return val
                     return None
                 
                 clean_df.loc[missing_trait, "trait"] = clean_df.loc[missing_trait, "custom_id"].apply(recover_trait)

        # 4. Model ID Standardization
        if "model_id" in clean_df.columns:
             # Fill from reported if missing
             clean_df["model_id"] = clean_df["model_id"].fillna(clean_df.get("model_reported"))
             
             # Standardize common names
             mask_mini = clean_df["model_id"].str.startswith("gpt-4o-mini", na=False)
             clean_df.loc[mask_mini, "model_id"] = "GPT4oMini"
             
             mask_4o = clean_df["model_id"].str.startswith("gpt-4o", na=False) & ~mask_mini
             clean_df.loc[mask_4o, "model_id"] = "GPT4o"

        # 5. Final Filtering
        required = ["question_id", "trait", "validity"]
        for col in required:
            if col in clean_df.columns:
                missing = clean_df[col].isna().sum()
                if missing > 0:
                    print(f"WARNING: {missing} rows missing '{col}'")

        # Only keep rows that actually have a validity result
        mask = clean_df["validity"].notna()
        clean_df = clean_df[mask]

        print(f"Final clean count: {len(clean_df)}")
        return clean_df

--------------------------------------------------
src/run_all_models.py
--------------------------------------------------
#!/usr/bin/env python3
import asyncio
import logging
from pathlib import Path
import pandas as pd
from datetime import datetime
import json
import argparse
from typing import Optional, List, Dict, Any

from src.config import settings, PROJECT_ROOT  # âœ… Added PROJECT_ROOT
from src.orchestrator import Orchestrator
from src.img_traits_def import ImgTraitDefinition
from src.science_qa import ScienceQA
from src.llm_service.providers.basic_batch import BaseBatchService
from src.llm_service.service import (
    OpenAIBatchService,
    NebiusBatchService,
)

# ---------- Setup logging at the MODULE LEVEL ----------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(name)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("run_all_models")


# ---------- PROVIDER DETECTION (from main.py) ----------
def get_provider_from_model_id(model_id: str) -> str:
    """Determines the provider based on the model ID prefix"""
    if model_id.upper().startswith("GPT"):
        return "openai"
    elif model_id.upper().startswith("L_"):
        return "nebius"
    raise ValueError(f"Could not determine provider for model {model_id}")


# ---------- BATCH HELPERS (from main.py) ----------
async def test_models_batch(
    model_ids: List[str],
    data: pd.DataFrame,
    orchestrator: Orchestrator,
    mode: str,  # "single", "combined", or "both"
) -> List[Dict[str, Any]]:
    """
    Collect all batch requests across models and questions based on the mode.
    """
    all_requests = []
    total_questions = len(data)

    for model_id in model_ids:
        try:
            provider = get_provider_from_model_id(model_id=model_id)
        except ValueError as e:
            logger.error(f"âŒ {e}. Skipping model {model_id}.")
            continue

        logger.info(
            f"\nðŸ“‹ Collecting requests for {model_id} (Provider: {provider}, Mode: {mode})..."
        )

        for i, (_, row) in enumerate(data.iterrows(), 1):
            question = ScienceQA.from_df_row(row)

            try:
                if mode in ("single", "both"):
                    # Assumes prepare_batch_requests adds internal keys:
                    # _provider, _model_id, _strategy="single"
                    single_reqs, _ = await orchestrator.prepare_batch_requests(
                        question, provider, model_id
                    )
                    all_requests.extend(single_reqs)

                if mode in ("combined", "both"):
                    # Assumes prepare_combined_batch_requests adds internal keys:
                    # _provider, _model_id, _strategy="combined"
                    combined_reqs, _ = (
                        await orchestrator.prepare_combined_batch_requests(
                            question, provider, model_id
                        )
                    )
                    all_requests.extend(combined_reqs)

            except Exception as e:
                qid = row.get("question_id", "Unknown")
                logger.error(
                    f"    Failed to prepare request for QID {qid}, Model {model_id}: {e}"
                )

            if (i + 1) % 10 == 0 or (i + 1) == total_questions:
                logger.info(f"    Processed {i + 1}/{total_questions} questions")

    return all_requests


async def monitor_batch_progress(
    batch_id: str,
    batch_service: BaseBatchService,
    check_interval: int,
    # âœ… Added metadata for download path
    model_id: str,
    provider: str,
    strategy: str,
):
    """Monitor batch progress and download results on completion."""
    _log = logging.getLogger(__name__)
    _log.info(f"ðŸ‘€ Monitoring batch {batch_id}...")

    while True:
        try:
            status = batch_service.get_batch_status(batch_id)
            batch_status = status.get("status", "unknown")

            if batch_status == "completed":
                _log.info(f"ðŸŽ‰ Batch {batch_id} completed!")

                # --- âœ… NEW DOWNLOAD LOGIC ---
                output_file_id = status.get("output_file_id")
                if not output_file_id:
                    _log.error(
                        f"âŒ Batch {batch_id} completed but has no output_file_id. Cannot download."
                    )
                else:
                    # Construct the path: data/batch_results/<provider>/<model_id>/<strategy>/
                    safe_model = "".join(
                        c if c.isalnum() else "_" for c in model_id
                    )
                    output_dir = (
                        PROJECT_ROOT
                        / "data"
                        / "batch_results"
                        / provider
                        / safe_model
                        / strategy
                    )
                    output_dir.mkdir(parents=True, exist_ok=True)

                    # Use a unique filename
                    output_filename = f"results_{batch_id}_{output_file_id}.jsonl"
                    output_path = output_dir / output_filename

                    _log.info(
                        f"ðŸ“¥ Downloading results for {batch_id} to {output_path}..."
                    )
                    try:
                        success = batch_service.download_batch_results(
                            batch_id, str(output_path)
                        )
                        if success:
                            _log.info(f"âœ… Download complete: {output_path.name}")
                        else:
                            _log.warning(f"âš ï¸ Download command failed for {batch_id}.")
                    except Exception as e:
                        _log.error(f"âŒ Download failed for {batch_id}: {e}")
                # --- END NEW DOWNLOAD LOGIC ---

                error_file_id = status.get("error_file_id")
                if error_file_id:
                    _log.warning("âš ï¸ Batch has errors, checking error file...")
                    try:
                        error_content = batch_service.client.files.content(
                            error_file_id
                        )
                        text_preview = getattr(error_content, "text", None)
                        if text_preview:
                            _log.error(f"First error: {text_preview[:1000]}...")
                    except Exception as e:
                        _log.error(f"Could not retrieve error file: {e}")
                return True

            elif batch_status == "failed":
                _log.error(f"âŒ Batch {batch_id} failed!")
                return False

            elif batch_status == "in_progress":
                counts = status.get("request_counts", {})
                _log.info(f"â³ Progress: {counts}")

            await asyncio.sleep(check_interval)

        except Exception as e:
            _log.error(f"Error checking batch: {e}")
            await asyncio.sleep(check_interval)


# ---------- MAIN RUNNER ----------
async def run_all_models(args):
    """
    Prepare and optionally submit batch JSONL files for ALL models
    defined in config, respecting the --trait-mode.
    """
    logger.info(
        f"ðŸš€ Starting batch preparation for ALL models (Mode: {args.trait_mode})..."
    )

    # --- Load ALL models from config ---
    all_models_dict = settings.get("all_models", {})
    if not all_models_dict:
        logger.error("âŒ No models found in settings['all_models']!")
        raise RuntimeError("No models found in settings['all_models']!")

    model_ids_to_test = list(all_models_dict.keys())
    logger.info(f"ðŸ§  Found {len(model_ids_to_test)} total models to process:")
    for m_alias in model_ids_to_test:
        try:
            provider = get_provider_from_model_id(m_alias)
            actual_name = all_models_dict[m_alias]
            logger.info(
                f"    - {m_alias} (Provider: {provider}, API Name: {actual_name})"
            )
        except Exception as e:
            logger.warning(f"    - Could not determine provider for {m_alias}: {e}")

    # --- Load dataset ---
    csv_path = Path(settings["paths"]["input_data_csv"])
    if not csv_path.exists():
        logger.error(f"âŒ Input data CSV not found at: {csv_path.resolve()}")
        raise FileNotFoundError(f"Input data CSV not found at: {csv_path.resolve()}")
    try:
        df = pd.read_csv(csv_path)
        if args.num_questions > 0 and args.num_questions < len(df):
            df = df.head(args.num_questions)
            logger.info(
                f"ðŸ“š Loaded {len(df)} questions (limited by --num-questions={args.num_questions}) from: {csv_path.resolve()}"
            )
        else:
            logger.info(
                f"ðŸ“š Loaded all {len(df)} questions from: {csv_path.resolve()}"
            )
    except Exception as e:
        logger.error(f"âŒ Failed to load or process CSV '{csv_path}': {e}")
        return

    # --- Initialize orchestrator and traits ---
    try:
        trait_def_path = Path(settings["paths"]["trait_definitions_json"])
        trait_def = ImgTraitDefinition(trait_def_path)
        trait_list = list(trait_def.traits.keys())
        if not trait_list:
            raise ValueError("Trait definitions loaded but resulted in an empty list.")
        logger.info(f"ðŸ§¬ Loaded {len(trait_list)} traits.")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize trait definitions: {e}")
        return

    # Initialize Orchestrator (following main.py, no checkpoint file)
    orchestrator = Orchestrator(
        trait_names=trait_list,
    )
    logger.info("âœ… Orchestrator initialized.")

    # --- Start request generation ---
    start_time_all = datetime.now()
    logger.info(
        f"ðŸ•“ Beginning request generation at {start_time_all.strftime('%Y-%m-%d %H:%M:%S')}..."
    )

    all_requests = await test_models_batch(
        model_ids_to_test, df, orchestrator, mode=args.trait_mode
    )

    elapsed_gen = (datetime.now() - start_time_all).total_seconds()
    logger.info(
        f"ðŸ“¦ Total collected {len(all_requests)} requests in {elapsed_gen:.2f}s."
    )

    if not all_requests:
        logger.warning("No requests were generated. Exiting.")
        return

    # --- Group requests by (provider, strategy, model) ---
    groups: dict[tuple[str, str, str], list[dict[str, Any]]] = {}
    for r in all_requests:
        provider = r.get("_provider")
        strategy = r.get("_strategy")  # e.g., "single" or "combined"
        model_id = r.get("_model_id")  # e.g., "GPT5Nano"

        if not all([provider, strategy, model_id]):
            logger.warning(
                f"Skipping request with missing metadata: {r.get('custom_id')}"
            )
            continue
        key = (provider, strategy, model_id)
        groups.setdefault(key, []).append(r)

    logger.info(f"ðŸ“Š Found {len(groups)} unique (provider, strategy, model) groups.")

    # --- Write files and optionally submit ---

    # Helper to drop internal metadata keys
    def _clean(reqs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        return [{k: v for k, v in r.items() if not k.startswith("_")} for r in reqs]

    base_path = Path(settings["paths"]["batch_request_file"])
    base_path.parent.mkdir(parents=True, exist_ok=True)
    files_created = []

    # Create a list of tasks to run concurrently (e.g., monitoring)
    monitor_tasks = []

    for (provider, strategy, model_id), reqs in groups.items():
        cleaned_reqs = _clean(reqs)
        safe_model = "".join(c if c.isalnum() else "_" for c in model_id)
        suffix_provider = "openai" if provider == "openai" else "nebius"

        # Generate a timestamp for this specific file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        output_file = base_path.with_name(
            f"{base_path.stem}_{suffix_provider}_{safe_model}_{strategy}_{timestamp}{base_path.suffix}"
        )

        batch_service = None
        try:
            if provider == "openai":
                batch_service = orchestrator.openai_batch_service
            elif provider == "nebius":
                batch_service = orchestrator.nebius_batch_service
            else:
                logger.error(f"Unknown provider '{provider}' for group. Skipping.")
                continue

            batch_service.write_jsonl_file(cleaned_reqs, str(output_file))
            logger.info(
                f"ðŸ’¾ Saved {len(cleaned_reqs)} {provider.upper()} {strategy} requests for {model_id} to: {output_file.name}"
            )
            files_created.append(output_file)

        except Exception as e:
            logger.error(f"âŒ Failed to write file {output_file.name}: {e}")
            continue

        # --- Submission Logic (from main.py) ---
        if not args.submit_batch:
            continue

        if not (args.trait_mode == strategy or args.trait_mode == "both"):
            logger.info(
                f"    (Skipping submission for {strategy} mode as --trait-mode={args.trait_mode})"
            )
            continue

        logger.info(f"ðŸ“¤ Submitting {output_file.name} for {model_id}...")
        try:
            batch_id = batch_service.submit_batch(str(output_file), args.batch_name)
            if batch_id and args.monitor:
                logger.info(f"    Submitted batch ID: {batch_id}. Monitoring...")
                # âœ… Schedule monitoring as a concurrent task
                task = asyncio.create_task(
                    monitor_batch_progress(
                        batch_id,
                        batch_service,
                        args.check_interval,
                        model_id=model_id,
                        provider=provider,
                        strategy=strategy,
                    )
                )
                monitor_tasks.append(task)
            elif batch_id:
                logger.info(f"    Submitted batch ID: {batch_id}. (Not monitoring)")
            else:
                logger.error(f"    Batch submission failed for {output_file.name}.")

        except Exception as e:
            logger.error(f"    âŒ Submission failed for {output_file.name}: {e}")

    # --- Wait for all monitoring tasks to complete ---
    if monitor_tasks:
        logger.info(f"Waiting for {len(monitor_tasks)} monitoring task(s) to complete...")
        await asyncio.gather(*monitor_tasks)
        logger.info("All monitoring tasks finished.")

    # --- Summary after all models ---
    end_time_all = datetime.now()
    total_duration_minutes = (end_time_all - start_time_all).total_seconds() / 60
    logger.info("=" * 50)
    logger.info(f"ðŸŽ‰ All models processed in {total_duration_minutes:.2f} minutes.")
    logger.info(f"ðŸ“¦ Generated {len(files_created)} batch file(s):")
    if files_created:
        for f_path in files_created:
            logger.info(f"   -> {f_path.name}")
    else:
        logger.info("   (No files were generated)")
    logger.info("\nâœ¨ Done!")


# ---------- ENTRY POINT ----------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate and submit batch request files for ALL configured models."
    )

    # --- Args from run_all_models.py ---
    parser.add_argument(
        "-n",
        "--num-questions",
        type=int,
        default=1,  # Reduced default for safety
        help="Number of questions to process (0 for all). Default: 10",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level (default: INFO).",
    )

    # --- Args from main.py ---
    parser.add_argument(
        "-tm",
        "--trait-mode",
        choices=["single", "combined", "both"],
        default="both",
        help="How to build batch requests: per-trait, combined, or both. Default: both.",
    )
    parser.add_argument(
        "--batch-name", type=str, help="Optional custom name for batch identification"
    )
    parser.add_argument(
        "--submit-batch",
        action="store_true",
        help="Actually submit the generated batch files to the API.",
    )
    parser.add_argument(
        "--monitor",
        action="store_true",
        help="Automatically monitor batch after submission (requires --submit-batch).",
    )
    parser.add_argument(
        "--check-interval",
        type=int,
        default=300,
        help="Batch monitoring interval in seconds (default: 300).",
    )

    args = parser.parse_args()

    # --- Set logging level based on argument ---
    log_level_numeric = getattr(logging, args.log_level.upper(), logging.INFO)
    logger.setLevel(log_level_numeric)
    logging.getLogger().setLevel(log_level_numeric)  # Set root logger level
    logger.info(f"Logging level set to {args.log_level.upper()}")

    if args.monitor and not args.submit_batch:
        logger.warning("--monitor flag ignored as --submit-batch was not specified.")

    try:
        asyncio.run(run_all_models(args))
    except (RuntimeError, FileNotFoundError, ValueError, Exception) as e:
        logger.error(f"ðŸ’¥ An unhandled error occurred: {e}", exc_info=True)
        import sys

        sys.exit(1)

--------------------------------------------------
src/science_qa.py
--------------------------------------------------
import re
import ast
import asyncio
import string
from pathlib import Path
from typing import List, Optional, Union

import pandas as pd
from PIL import Image
from pydantic import BaseModel

from src.config import settings, PROJECT_ROOT


class ScienceQA(BaseModel):
    id: str
    question: str
    choices: List[str]
    answer: int
    hint: str
    image: str
    task: str
    grade: str
    subject: str
    topic: str
    category: str
    skill: str
    lecture: str
    solution: str
    split: str
    _raw_row: Optional[pd.Series] = None

    @classmethod
    def from_df_row(cls, row: pd.Series) -> "ScienceQA":
        try:
            parsed_choices = (
                ast.literal_eval(row["choices"])
                if isinstance(row["choices"], str)
                else row["choices"]
            )
        except Exception:
            parsed_choices = []

        instance = cls(
            id=str(row.get("question_id", "")),
            question=str(row.get("question", "")),
            choices=parsed_choices or [],
            answer=int(row.get("answer", -1)),
            hint=str(row.get("hint", "")),
            image=str(row.get("image", "")),
            task=str(row.get("task", "")),
            grade=str(row.get("grade", "")),
            subject=str(row.get("subject", "")),
            topic=str(row.get("topic", "")),
            category=str(row.get("category", "")),
            skill=str(row.get("skill", "")),
            lecture=str(row.get("lecture", "")),
            solution=str(row.get("solution", "")),
            split=str(row.get("set", "")),
        )

        # Optional: attach the original row if needed later
        instance._raw_row = row
        return instance

    async def load_images(self) -> List[Image.Image]:
        """
        Asynchronously loads and returns .png images associated with this question.
        """
        images: List[Image.Image] = []
        image_base_dir: Path = PROJECT_ROOT / settings["paths"]["image_base_dir"]

        folder = self.image.strip() or self.id
        placeholders = settings.get("settings", {}).get(
            "image_folder_placeholders", ["", "none", "image.png"]
        )

        if folder.lower() in placeholders:
            folder = self.id

        image_dir = image_base_dir / folder
        if not image_dir.is_dir():
            return []

        loop = asyncio.get_running_loop()

        async def load(path: Path):
            try:
                return await loop.run_in_executor(
                    None, lambda: Image.open(path).convert("RGB")
                )
            except Exception as e:
                print(f"âš ï¸ Could not load image {path}: {e}")
                return None

        tasks = [load(p) for p in sorted(image_dir.glob("*.png"))]
        results = await asyncio.gather(*tasks)
        return [img for img in results if img]

    async def upload_images_to_openai(self, openai_client) -> List[str]:
        """
        Asynchronously uploads .png images to OpenAI and returns file IDs.
        """
        file_ids: List[str] = []
        image_base_dir: Path = PROJECT_ROOT / settings["paths"]["image_base_dir"]

        folder = self.image.strip() or self.id
        placeholders = settings.get("settings", {}).get(
            "image_folder_placeholders", ["", "none", "image.png"]
        )

        if folder.lower() in placeholders:
            folder = self.id

        image_dir = image_base_dir / folder
        if not image_dir.is_dir():
            return []

        loop = asyncio.get_running_loop()

        async def upload_openai(path: Path):
            try:
                return await loop.run_in_executor(
                    None,
                    lambda: openai_client.files.create(
                        file=open(str(path), "rb"), purpose="vision"
                    ).id,
                )
            except Exception as e:
                print(f"âš ï¸ Could not upload image {path}: {e}")
                return None

        tasks = [upload_openai(p) for p in sorted(image_dir.glob("*.png"))]
        results = await asyncio.gather(*tasks)
        return [file_id for file_id in results if file_id]


# === Utility: Question formatting ===
def build_question(problem: ScienceQA, format_str: str) -> str:
    options = settings.get("settings", {}).get("choice_options", [])

    def choice_text():
        return " ".join(
            [
                f"({options[i]}) {c}"
                for i, c in enumerate(problem.choices)
                if i < len(options)
            ]
        )

    def answer_text():
        idx = problem.answer
        return options[idx] if 0 <= idx < len(options) else ""

    def clean(text: str) -> str:
        return str(text or "").replace("\n", " ").strip()

    def make_question():
        input_map = {
            "Q": f"Question: {clean(problem.question)}\n",
            "C": f"Context: {clean(problem.hint)}\n",
            "M": f"Options: {choice_text()}\n",
            "L": f"BECAUSE: {clean(problem.lecture)}\n",
            "E": f"BECAUSE: {clean(problem.solution)}\n",
        }
        input_fmt, output_fmt = format_str.split("-")
        input_str = "".join([input_map.get(c, "") for c in input_fmt])

        output_map = {
            "A": f"The answer is {answer_text()}.",
            "L": clean(problem.lecture),
            "E": clean(problem.solution),
        }
        output_str = "Answer: " + " ".join(
            [output_map[c] for c in output_fmt if c in output_map and output_map[c]]
        )

        return (input_str + output_str).strip()

    return make_question()


# === Utility: Characteristics formatting ===
def build_characteristics(problem: ScienceQA, format_str: str) -> str:
    """Builds a formatted string of characteristics based on the format string."""
    parts = {
        "G": f"Student Grade: {problem.grade.lower().replace('grade', '').strip()}\n",
        "S": f"Subject: {problem.subject}\n",
        "T": f"Topic: {problem.topic}\n",
        "C": f"Category: {problem.category}\n",
        "Sk": f"Skill: {problem.skill}\n",
        "Ta": f"Task Type: {problem.task}\n",
    }

    keys_in_format = re.findall(f"({'|'.join(parts.keys())})", format_str)

    if not keys_in_format:
        print(f"âš ï¸ Unknown or empty format '{format_str}' for characteristics.")
        return ""

    return "".join(parts[key] for key in keys_in_format)


# === Utility: ACTIVITY block for OriginalTraitAgent ===
def build_activity_components(problem: ScienceQA) -> str:
    """
    STRICT, labeled block that matches OriginalTraitAgent's system prompt.
    Ensures stable Aâ€“Z labels even if settings choice_options is missing/short.
    Keeps 'solution' minimal (letter only) to reduce bias.
    """

    def clean(x: str) -> str:
        return str(x or "").replace("\n", " ").strip()

    # Letters fallback
    configured = settings.get("settings", {}).get("choice_options", [])
    letters = list(configured) if configured else list(string.ascii_uppercase)

    # Choices list
    choices = problem.choices or []
    choice_lines = []
    for i, c in enumerate(choices):
        label = letters[i] if i < len(letters) else f"({i})"
        choice_lines.append(f"- ({label}) {clean(str(c))}")
    choices_block = "\n".join(choice_lines) if choice_lines else "(none)"

    # Solution label only
    ans_idx = problem.answer if isinstance(problem.answer, int) else -1
    ans_label = letters[ans_idx] if 0 <= ans_idx < len(letters) else "(not provided)"

    # Optional extras (kept separate so they donâ€™t pollute evaluation)
    lecture = clean(problem.lecture)
    solution_expl = clean(problem.solution)
    extras = []
    if lecture:
        extras.append(f"- lecture: {lecture}")
    if solution_expl:
        extras.append(f"- solution_explanation: {solution_expl}")
    extra_block = "\n".join(extras)

    return (
        f"""ACTIVITY:
- skill: {clean(problem.skill)}
- question: {clean(problem.question)}
- passage: {clean(problem.hint)}
- choices:
{choices_block}
- solution: {ans_label}
{extra_block if extra_block else ""}"""
    ).strip()

--------------------------------------------------
src/stats.py
--------------------------------------------------
#!/usr/bin/env python3
"""
Statistics & Plots for Batch Evaluation Results

Revisions:
- Supports merging multiple CSVs (Single vs Combined) for side-by-side comparison.
- Auto-tags 'Strategy' based on file path keywords ('single', 'combined').
- Generates aggregate summary and comparison boxplots.
"""
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
)

# Handle imports if run as script vs module
try:
    from src.config import settings
except ImportError:
    # Fallback if run directly from src/
    sys.path.append(str(Path(__file__).parent.parent))
    from src.config import settings


class Statistics:
    """Handles statistical analysis, plots, and timing for the results."""

    def __init__(
        self,
        results_df: pd.DataFrame,
        results_dir: Path,
        ground_truth_col: Optional[str] = None,
    ):
        self.df = results_df.copy()
        self.RESULTS_DIR = results_dir
        self.RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        
        # Filenames
        self.SUMMARY_FILENAME = results_dir / "evaluation_summary.csv"
        self.PLOT_FILENAME = results_dir / "accuracy_comparison_boxplot.png"
        self.TIMEINFO_FILENAME = results_dir / "time_range.txt"

        # --- Validation ---
        if "validity" not in self.df.columns:
            raise ValueError("Input DataFrame must contain a 'validity' column.")
        self.df["validity"] = self.df["validity"].astype(bool)

        # --- Model ID Standardization ---
        # We extract the base model name, then append Strategy if available
        if "custom_id" in self.df.columns:
            print("Standardizing model_id from custom_id...")
            # Extract base alias (e.g. "L_Gemma327B")
            self.df["base_model"] = self.df["custom_id"].str.extract(
                r"^request-([^-]+)-", expand=False
            )
            
            # Standardize friendly names
            replacements = {
                "L_Gemma327B": "Gemma-3-27B",
                "L_Qwen25VL72B": "Qwen2.5-VL-72B",
                "GPT4oMini": "GPT-4o-Mini",
                "GPT4o": "GPT-4o"
            }
            self.df["base_model"] = self.df["base_model"].replace(replacements)

            # Create Final Model ID: "Model Name (Strategy)"
            if "strategy" in self.df.columns:
                self.df["model_id"] = self.df["base_model"] + " (" + self.df["strategy"] + ")"
            else:
                self.df["model_id"] = self.df["base_model"]
                
            print("Unique Models identified:", self.df["model_id"].unique())
        else:
            print("Warning: 'custom_id' missing, using existing 'model_id'.")

        # --- Ground Truth Logic ---
        self.ground_truth_col = ground_truth_col or next(
            (c for c in ["ground_truth", "label", "expected_validity", "gold_validity"] 
             if c in self.df.columns), None
        )

        if self.ground_truth_col is None:
            self.ground_truth_col = "_assumed_ground_truth_true"
            self.df[self.ground_truth_col] = True
        else:
            self.df[self.ground_truth_col] = self.df[self.ground_truth_col].astype(bool)

        self.traits = sorted(list(set(self.df["trait"].dropna().unique())))
        self._ensure_datetime_column()

    # ---------------- Helpers ----------------
    def _ensure_datetime_column(self) -> None:
        if "timestamp_token" in self.df.columns:
            self.df["timestamp_token_dt"] = pd.to_datetime(
                self.df["timestamp_token"], unit="ms", errors="coerce"
            )

    def _compute_metrics(self, y_pred: pd.Series, y_true: pd.Series) -> Dict[str, float]:
        y_p = y_pred.astype(bool).values
        y_t = y_true.astype(bool).values
        if len(y_p) == 0:
            return {k: np.nan for k in ["accuracy", "precision", "recall", "f1", "mcc"]}

        return {
            "accuracy": accuracy_score(y_t, y_p),
            "precision": precision_score(y_t, y_p, zero_division=0),
            "recall": recall_score(y_t, y_p, zero_division=0),
            "f1": f1_score(y_t, y_p, zero_division=0),
            "mcc": matthews_corrcoef(y_t, y_p) if len(np.unique(y_t)) > 1 else np.nan
        }

    # ---------------- Outputs ----------------
    def generate_summary(self):
        if self.df.empty: return

        summary_rows = []
        # Group by our new composite Model ID (includes strategy)
        for model_id in sorted(self.df["model_id"].dropna().unique()):
            model_data = self.df[self.df["model_id"] == model_id]
            
            # 1. Overall metrics
            overall = self._compute_metrics(model_data["validity"], model_data[self.ground_truth_col])
            
            row = {
                "model_id": model_id,
                "total_count": len(model_data),
                "accuracy": round(overall["accuracy"], 3),
                "f1": round(overall["f1"], 3)
            }

            # 2. Per-trait metrics
            for trait in self.traits:
                t_data = model_data[model_data["trait"] == trait]
                tm = self._compute_metrics(t_data["validity"], t_data[self.ground_truth_col])
                
                row[f"{trait}_acc"] = round(tm["accuracy"], 3)
                row[f"{trait}_count"] = len(t_data)

            summary_rows.append(row)

        summary = pd.DataFrame(summary_rows)
        summary.to_csv(self.SUMMARY_FILENAME, index=False)
        print(f"âœ… Summary saved: {self.SUMMARY_FILENAME}")

        # Timing
        if "timestamp_token_dt" in self.df.columns:
            dts = self.df["timestamp_token_dt"].dropna()
            if not dts.empty:
                with open(self.TIMEINFO_FILENAME, "w") as f:
                    f.write(f"Start: {dts.min()}\nEnd: {dts.max()}\nDuration: {dts.max() - dts.min()}\n")

    def generate_plots(self):
        if self.df.empty: return
        
        # Prep data: Calculate accuracy per Question
        df_plot = self.df.copy()
        df_plot["correct"] = df_plot["validity"] == df_plot[self.ground_truth_col]
        
        # Group by Model (which includes Strategy) and Question
        per_q = df_plot.groupby(["model_id", "question_id"])["correct"].mean().reset_index()
        per_q.rename(columns={"correct": "accuracy"}, inplace=True)

        try:
            import seaborn as sns
            from matplotlib import pyplot as plt

            plt.figure(figsize=(14, 8))
            
            # Create Boxplot
            # Sort models so 'Combined' and 'Single' for same model appear near each other
            order = sorted(per_q["model_id"].unique())
            
            ax = sns.boxplot(
                data=per_q, x="model_id", y="accuracy",
                order=order, palette="viridis", showmeans=True
            )
            
            plt.xticks(rotation=45, ha="right")
            plt.title("Accuracy Comparison: Single vs Combined Traits")
            plt.ylabel("Accuracy per Question")
            plt.tight_layout()
            plt.savefig(self.PLOT_FILENAME)
            plt.close()
            print(f"âœ… Plot saved: {self.PLOT_FILENAME}")
        except ImportError:
            print("âš ï¸ Seaborn/Matplotlib not found, skipping plots.")


# ---------------- Main Loader ----------------
def load_data_recursively(paths: List[str]) -> pd.DataFrame:
    """Loads CSVs from paths/dirs and tags them with Strategy based on folder name."""
    all_dfs = []
    
    # Resolve all files
    files = []
    for p_str in paths:
        p = Path(p_str).expanduser().resolve()
        if p.is_dir():
            # Find all clean result CSVs recursively
            files.extend(list(p.rglob("clean_evaluation_results.csv")))
        elif p.is_file():
            files.append(p)

    print(f"ðŸ” Found {len(files)} result files to process.")

    for f in files:
        try:
            df = pd.read_csv(f)
            
            # --- Strategy Auto-Tagging ---
            # Look at parent folders for keywords
            path_str = str(f).lower()
            if "combined" in path_str:
                df["strategy"] = "Combined"
            elif "single" in path_str:
                df["strategy"] = "Single"
            else:
                df["strategy"] = "Unknown"
                
            all_dfs.append(df)
            print(f"  -> Loaded {len(df)} rows from {f.parent.name} (Strategy: {df['strategy'].iloc[0]})")
        except Exception as e:
            print(f"  âŒ Failed to load {f}: {e}")

    if not all_dfs:
        return pd.DataFrame()
        
    return pd.concat(all_dfs, ignore_index=True)


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--inputs", 
        nargs="+", 
        required=True, 
        help="List of files or folders to scan for 'clean_evaluation_results.csv'"
    )
    ap.add_argument("--outdir", default="./results_comparison", help="Output directory")
    args = ap.parse_args()

    # 1. Load and Merge
    merged_df = load_data_recursively(args.inputs)
    
    if merged_df.empty:
        print("âŒ No data found.")
        sys.exit(1)

    # 2. Run Stats
    out_path = Path(args.outdir)
    stats = Statistics(merged_df, out_path)
    stats.generate_summary()
    stats.generate_plots()

--------------------------------------------------
src/trait_agent.py
--------------------------------------------------
from typing import Optional, Dict, Any, List
from PIL import Image
import logging
import time

from src.llm_service.utils import image_to_data_url
from src.config import settings
from src.img_traits_def import ImgTraitDefinition
from src.prompt_template import system_prompt_template
from src.science_qa import ScienceQA, build_question, build_characteristics
from src_orig.orig_traits_def import OriginalTraitDefinition
from src.llm_service.config import ConfigManager
from src.llm_service.schemas import ValidationSchema


logger = logging.getLogger(__name__)


class TraitAgent:
    """
    A base agent specialized in evaluating a single trait of a question-image pair.
    Designed to be subclassed for different trait definition sources.
    """

    def __init__(
        self,
        trait_name: str,
    ):
        # Stricter input validation
        self.config = ConfigManager()
        if not trait_name or not trait_name.strip():
            raise ValueError("Trait name cannot be empty or contain only whitespace.")

        self.trait_name = trait_name.strip()
        self.trait_definitions = ImgTraitDefinition()
        self.system_prompt = self._build_system_prompt(system_prompt_template)

        logger.info(f"TraitAgent for '{self.trait_name}' initialized.")

    def _build_system_prompt(self, template: str) -> str:
        """Helper to fetch components and format the system prompt."""
        definition = self.trait_definitions.retrieve_definition(self.trait_name)
        note = self.trait_definitions.retrieve_note(self.trait_name)
        evaluation_questions = self.trait_definitions.retrieve_evaluation_questions(
            self.trait_name
        )
        return template.format(
            trait_name=self.trait_name,
            definition=definition,
            note=note,
            evaluation_questions=evaluation_questions,
        )

    def _create_user_prompt(self, question_data: ScienceQA) -> str:
        """
        Combines question, context, options, and characteristics into one user prompt.
        """
        question_str = build_question(question_data, format_str="QCMLE-A")
        characteristics_str = build_characteristics(
            question_data, format_str="GSTCSkTa"
        )
        return f"{characteristics_str}\n\n{question_str}"

    def inputs_for(
        self,
        question_data: ScienceQA,
        provider: str,
        pil_images: Optional[List[Image.Image]] = None,
        image_file_ids: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Builds input messages.
        For OpenAI: returns list for Responses API "input" field
        For Nebius: returns list that will be added to messages array
        """
        user_text = self._create_user_prompt(question_data)

        user_content: List[Dict[str, Any]] = [{"type": "input_text", "text": user_text}]

        if provider == "openai":
            if image_file_ids:
                for fid in image_file_ids:
                    user_content.append({"type": "input_image", "file_id": fid})

            return [{"role": "user", "content": user_content}]

        elif provider == "nebius":
            # For Nebius, use Chat Completions format
            content: List[Dict[str, Any]] = [{"type": "text", "text": user_text}]

            if pil_images:
                for img in pil_images:
                    data_url = image_to_data_url(img)
                    content.append(
                        {"type": "image_url", "image_url": {"url": data_url}}
                    )

            return [{"role": "user", "content": content}]

        return []

    def prepare_single_request(
        self,
        question_data: ScienceQA,
        provider: str,
        model_id: str, 
        pil_images: Optional[List[Image.Image]] = None,
        image_file_ids: Optional[List[str]] = None,
    ):
        try:
            qid = question_data.id
            if provider == "openai":
                model = settings["models"]["openai"][model_id]
                request_url = "/v1/responses" 
            elif provider == "nebius":
                model = settings["models"]["nebius"][model_id]
                request_url = "/v1/chat/completions"
            else:
                raise ValueError(f"Unknown provider: {provider}")

            trait = "_".join(self.trait_name.split(" ")).lower()
            request_id = f"request-{model_id}-{trait}-{qid}-{int(time.time() * 1000)}"

            openai_params = self.config.params.get("openai", {})
            max_out = int(openai_params.get("max_tokens", 512))

            inputs = self.inputs_for(
                question_data=question_data,
                provider=provider,
                pil_images=pil_images,
                image_file_ids=image_file_ids,
            )

            schema = ValidationSchema.model_json_schema()
            if "properties" in schema:
                schema["required"] = list(schema["properties"].keys())

            # Different payload based on provider
            if provider == "openai":
                # OpenAI uses Responses API format (as requested)
                payload = {
                    "model": model,
                    "instructions": self.system_prompt,
                    "max_output_tokens": max_out,
                    "input": inputs,
                    "text": {
                        "format": {
                            "type": "json_schema",
                            "name": "validated_trait",
                            "schema": schema,
                            "strict": True,
                        },
                    },
                }

                if model_id.startswith("GPT5"):
                    payload["reasoning"] = {"effort": "low"}
                    payload["text"]["verbosity"] = "low"
                    
                # GPT5 does not take temperature or top_p parameters in the payload
                if not model_id.startswith("GPT5"):
                    payload["temperature"] = 0
                    payload["top_p"] = 1

            else:  # nebius
                messages = [{"role": "system", "content": self.system_prompt}]

                for msg in inputs:
                    if msg.get("role") == "user":
                        messages.append(msg)

                payload = {
                    "model": model,
                    "messages": messages,
                    "response_format": {
                        "type": "json_schema",
                        "json_schema": {
                            "name": "validated_trait",
                            "schema": schema,
                            "strict": True,
                        },
                    },
                    "max_tokens": max_out,
                }


                if not model_id.startswith("GPT5"):
                    payload["temperature"] = 0
                    payload["top_p"] = 1

            request_text = {
                "custom_id": request_id,
                "method": "POST",
                "url": request_url,
                "body": payload,
            }

            return request_text

        except Exception as e:
            logger.error(
                f"Failed to prepare request for trait '{self.trait_name}': {e}"
            )
            return None

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/LLMEnsembleEvaluation.py
--------------------------------------------------
from enum import Enum
import pandas as pd
import json
from collections import Counter
from typing import List, Dict, Tuple


class MultiJudgesType(Enum):
    MajorityVoting = 0
    AllTrue = 1
    AtLeastOne = 2

    @classmethod
    def all(self):
        return list(map(lambda c: c, self))


class LLMEnsembleEvaluation:
    def __init__(self, records_df: pd.DataFrame):
        """
        :param records_df: DataFrame with columns ['question_id', 'prompt_id', 'model_id', 'traits_output', 'accuracy']
        """
        self.records_df = records_df.copy()
        self.records_df["traits_output"] = self.records_df["traits_output"].apply(
            self._parse_traits_output
        )

    @staticmethod
    def _parse_traits_output(raw_output: str) -> List[Dict]:
        try:
            return json.loads(raw_output)
        except Exception:
            return []

    def _group_key(self, row) -> Tuple:
        return (row["question_id"], row["prompt_id"])

    def _vote_trait_majority(self, trait_outputs: List[Dict]) -> List[Dict]:
        """
        Given a list of trait outputs from multiple models, apply majority voting on each trait.
        """
        trait_votes = {}
        for output in trait_outputs:
            for item in output:
                trait = item["trait"]
                vote = item["valid"]
                trait_votes.setdefault(trait, []).append(vote)

        voted_traits = []
        for trait, votes in trait_votes.items():
            vote_count = Counter(votes)
            majority = (
                vote_count[True] > vote_count[False]
            )  # vote_count.most_common(1)[0][0]
            voted_traits.append(
                {
                    "trait": trait,
                    "valid": majority,
                    "reasoning": (
                        ""
                        if majority
                        else f"Majority voted invalid ({vote_count[False]}/{len(votes)})"
                    ),
                }
            )
        return voted_traits

    def _all_true(self, trait_outputs: List[Dict]) -> List[Dict]:
        """
        Given a list of trait outputs from multiple models, apply majority voting on each trait.
        """
        trait_votes = {}
        for output in trait_outputs:
            for item in output:
                trait = item["trait"]
                vote = item["valid"]
                trait_votes.setdefault(trait, []).append(vote)

        voted_traits = []
        for trait, votes in trait_votes.items():
            vote_count = Counter(votes)
            majority = vote_count[True] == len(votes)
            voted_traits.append(
                {
                    "trait": trait,
                    "valid": majority,
                    "reasoning": (
                        ""
                        if majority
                        else f"AllTrue invalid ({vote_count[False]}/{len(votes)})"
                    ),
                }
            )
        return voted_traits

    def _at_least_one(self, trait_outputs: List[Dict]) -> List[Dict]:
        """
        Given a list of trait outputs from multiple models, apply majority voting on each trait.
        """
        trait_votes = {}
        for output in trait_outputs:
            for item in output:
                trait = item["trait"]
                vote = item["valid"]
                trait_votes.setdefault(trait, []).append(vote)

        voted_traits = []
        for trait, votes in trait_votes.items():
            vote_count = Counter(votes)
            majority = vote_count[True] > 0
            voted_traits.append(
                {
                    "trait": trait,
                    "valid": majority,
                    "reasoning": (
                        ""
                        if majority
                        else f"AllTrue invalid ({vote_count[False]}/{len(votes)})"
                    ),
                }
            )
        return voted_traits

    def run_multiple_judges_strategies(
        self, judge=MultiJudgesType.MajorityVoting
    ) -> pd.DataFrame:
        """
        Performs majority voting for each (question_id, prompt_id) pair.
        Returns a new DataFrame with ensemble accuracy per question.
        """
        grouped = self.records_df.groupby(["question_id", "prompt_id"])
        results = []
        model_combinations = [
            # Same Family: Same version, different sizes
            # ("L_Llama38Instruct", "L_Llama370Instruct", ""),
            # ("L_Llama318Instruct", "L_Llama3170Instruct_Q4", ""),
            # ("L_DeepSeekR1Llama8", "L_DeepSeekR1Llama70_Q4", ""),
            # ("L_DeepSeekR1Qwen7", "DeepSeekR1Qwen32", ""),
            # ("Mixtral8x7B01Instruct", "L_Mixtral8x22B01Instruct_Q4", ""),
            # ("GPT4oMini", "GPT4o", ""),
            # ("GPT4oMini", "GPT4Turbo", ""),
            #
            # # Same Family: Different versions, same size
            # ("L_Llama38Instruct", "L_Llama318Instruct", ""),  # Llama 8B (v3, v3.1)
            # ("L_Llama370Instruct", "L_Llama3170Instruct_Q4", ""),  # Llama 70B (v3, v3.1)
            # ("L_Llama370Instruct", "Llama3370Instruct", ""),  # Llama 70B (v3, v3.3)
            # ("L_Llama3170Instruct_Q4", "Llama3370Instruct", ""),  # Llama 70B (v3.1, v3.3)
            # ("L_Llama370Instruct", "L_Llama3170Instruct_Q4", "Llama3370Instruct"),  # Llama 70B all versions
            # ("Mistral7B03Instruct", "L_Mistral7B02Instruct", ""),  # Mistral 7B (v0.2, v0.3)
            # ("Mistral7B03Instruct", "Mixtral8x7B01Instruct", ""),  # Mistral 7B (v0.1, v0.3)
            # ("L_Mistral7B02Instruct", "Mixtral8x7B01Instruct", ""),  # Mistral 7B (v0.2, v0.1)
            # ("Mistral7B03Instruct", "L_Mistral7B02Instruct", "Mixtral8x7B01Instruct"),  # Mistral 7B (v0.2, v0.3)
            # ("GPT35Turbo1106", "GPT4Turbo", ""),
            # ("GPT35Turbo1106", "GPT4o", ""),
            # Different Families: Same size, same/different versions
            ("L_DeepSeekR1Qwen7", "Mistral7B03Instruct", ""),  # DeepSeek and Mistral
            ("L_DeepSeekR1Qwen7", "L_Mistral7B02Instruct", ""),  # DeepSeek and Mistral
            ("L_DeepSeekR1Qwen7", "Mixtral8x7B01Instruct", ""),  # DeepSeek and Mistral
            ("L_Llama38Instruct", "L_DeepSeekR1Llama8", ""),  # Llama and DeepSeek
            ("L_Llama318Instruct", "L_DeepSeekR1Llama8", ""),  # Llama and DeepSeek
            ("L_Llama370Instruct", "L_DeepSeekR1Llama70_Q4", ""),  # Llama and DeepSeek
            (
                "L_Llama3170Instruct_Q4",
                "L_DeepSeekR1Llama70_Q4",
                "",
            ),  # Llama and DeepSeek
            ("Llama3370Instruct", "L_DeepSeekR1Llama70_Q4", ""),  # Llama and DeepSeek
            ("GPT4o", "L_DeepSeekR1Qwen7", ""),  # GPT and DeepSeek
            ("GPT4o", "L_Llama3170Instruct_Q4", ""),  # GPT and Llama
            ("GPT4o", "Mixtral8x7B01Instruct", ""),  # GPT and Mistral
            # Different Families: Best and Second performing models
            # Best 3 models individually
            # ("GPT4o","L_Llama3170Instruct_Q4", "L_DeepSeekR1Qwen7"),
            # ("GPT4o", "L_Mixtral8x22B01Instruct_Q4", "L_DeepSeekR1Qwen7"),
            # ("GPT4o", "L_Mixtral8x22B01Instruct_Q4", "L_Llama3170Instruct_Q4"),
            # ("L_DeepSeekR1Qwen7", "L_Mixtral8x22B01Instruct_Q4", "L_Llama3170Instruct_Q4"),
            #
            # # 1 Second, and 2 Best models
            # ("GPT4Turbo", "L_Llama3170Instruct_Q4", "L_DeepSeekR1Qwen7"),  # second GPT
            # ("GPT4o", "L_Llama318Instruct", "L_DeepSeekR1Qwen7"),   # second Llama
            # ("GPT4o", "L_Llama3170Instruct_Q4", "DeepSeekR1Qwen32"), #second DeepSeek
        ]

        for M1, M2, M3 in model_combinations:
            ensemble_name = judge.name + "__" + M1 + "__" + M2
            if M3 != "":
                ensemble_name = ensemble_name + "__" + M3
            for (qid, pid), group in grouped:
                model_trait_outputs = group["traits_output"][
                    (group["model_id"] == M1)
                    | (group["model_id"] == M2)
                    | (group["model_id"] == M3)
                ].tolist()
                if not model_trait_outputs:
                    continue

                voted_traits = None
                if judge == MultiJudgesType.MajorityVoting:
                    voted_traits = self._vote_trait_majority(model_trait_outputs)
                elif judge == MultiJudgesType.AllTrue:
                    voted_traits = self._all_true(model_trait_outputs)
                elif judge == MultiJudgesType.AtLeastOne:
                    voted_traits = self._at_least_one(model_trait_outputs)
                accuracy = sum(t["valid"] for t in voted_traits) / len(voted_traits)

                results.append(
                    {
                        "question_id": qid,
                        "prompt_id": pid,
                        "model_id": ensemble_name,
                        "traits_output": json.dumps(voted_traits),
                        "accuracy": accuracy,
                    }
                )

        return pd.DataFrame(results)

    # Not needed as we already have a functionality to plot and compare accordingly
    # def compare_ensemble_to_individuals(input_csv:str, ensemble_df: pd.DataFrame, output_csv:str, label: str = "ensemble") -> pd.DataFrame:
    #     """
    #     Compares accuracy of ensemble model vs individual models.
    #     Returns a DataFrame with average accuracy per model and ensemble.
    #     """
    #     df_clean = pd.read_csv(input_csv, keep_default_na=False)
    #     df_ensemble = ensemble_df.copy()
    #     df_ensemble["dataset"] = label

    #     # Normalize schema if needed
    #     df_clean["dataset"] = "individual"
    #     combined = pd.concat([df_clean[["model_id", "dataset", "accuracy"]],
    #                         df_ensemble[["model_id", "dataset", "accuracy"]]], ignore_index=True)

    #     summary = combined.groupby(["dataset", "model_id"]).agg(
    #         count=("accuracy", "count"),
    #         mean_accuracy=("accuracy", "mean"),
    #         std_accuracy=("accuracy", "std")
    #     ).reset_index()

    #     summary.to_csv(output_csv, index=False)
    #     print(f"Saved ensemble vs individual comparison to: {output_csv}")
    #     return summary

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/LLMEnsembleEvaluator.py
--------------------------------------------------
import argparse
import os
import pandas as pd
from pathlib import Path
from .LLMEnsembleEvaluation import LLMEnsembleEvaluation, MultiJudgesType

RESULTS_DIR = os.path.join(os.getcwd(), "results/")

GROUND_TRUTH_POSSIBILITIES = ["gtincorrect"]  # ["gtcorrect", "gtincorrect"]
CLEAN_RESULTS_FILENAME = RESULTS_DIR + "clean_questions_traits_evaluation.csv"
ENSEMBLE_RESULTS_FILENAME = RESULTS_DIR + "ensemble_evaluation.csv"
ENSEMBLE_COMPARISON_FILENAME = RESULTS_DIR + "ensemble_comparison.csv"


def load_evaluation_data(input_csv: str) -> pd.DataFrame:
    return pd.read_csv(input_csv)


def save_ensemble_results(results_df: pd.DataFrame, output_csv: str):
    results_df.to_csv(output_csv, index=False, quoting=1)
    print(f"Saved ensemble majority voting results to: {output_csv}")


# Not needed as we already have a functionality to plot and compare accordingly
# def run_ensemble_comparison(input_csv: str, ensemble_df: pd.DataFrame, output_csv: str):
#     LLMEnsembleEvaluation.compare_ensemble_to_individuals(input_csv, ensemble_df, output_csv)


def main():
    for gt in GROUND_TRUTH_POSSIBILITIES:
        input_csv = CLEAN_RESULTS_FILENAME.replace(".csv", "_" + gt + ".csv")
        output_csv = ENSEMBLE_RESULTS_FILENAME.replace(".csv", "_" + gt + ".csv")
        df = load_evaluation_data(input_csv)
        evaluator = LLMEnsembleEvaluation(df)
        traits_evaluation_results = pd.DataFrame(
            columns=[
                "question_id",
                "prompt_id",
                "model_id",
                "traits_output",
                "accuracy",
            ]
        )
        for judge in [MultiJudgesType.MajorityVoting, MultiJudgesType.AllTrue]:

            # Will only be needed in case we have to save evaluation numbers per strategy in a separate csv
            # path = Path(RESULTS_DIR + "/"+judge.name)
            # path.mkdir(parents=True, exist_ok=True)

            ensemble_results = evaluator.run_multiple_judges_strategies(judge)
            traits_evaluation_results = traits_evaluation_results._append(
                ensemble_results, ignore_index=True
            )

        traits_evaluation_results = traits_evaluation_results._append(df)
        save_ensemble_results(traits_evaluation_results, output_csv)

        # Not needed as we already have a functionality to plot and compare accordingly
        # output_comparison_csv = RESULTS_DIR + "/"+ judge.name+ "/ensemble_comparison_" + judge.name + ".csv"
        # run_ensemble_comparison(input_csv, ensemble_results, output_comparison_csv)


# To run it via command-line: python LLMEnsembleEvaluator.py
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="LLMEnsembleEvaluator",
        description="Evaluate ensemble performance of multiple LLMs acting as judges over question traits.",
    )

    main()

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/LLMValidatorsTesting.py
--------------------------------------------------
import csv
import random
import time
from typing import Union, List, Optional
import numpy as np
import pandas as pd

from config import settings
from .Prompt import PromptID, Prompt
from .LLM_Service import LLM_Service
from .MultimodalLLM_Service import MultimodalLLM_Service
from .ScienceQA import get_image_files
from .TraitList import supported_traits

from .Metrics import Metrics
from .Statistics import Statistics

from sklearn.metrics import accuracy_score


class LLMValidatorsTesting:
    """
    A class to test and evaluate LLM validators based on various prompts and models.
    """

    metrics: Metrics
    statistics: Statistics
    llm_service: Union[LLM_Service, MultimodalLLM_Service]
    ground_truth_questions: pd.DataFrame
    selected_questions: List[int]
    selected_traits: List[str]
    prompts: pd.DataFrame
    traits_evaluation_results: pd.DataFrame

    def __init__(
        self,
        input_filename: Optional[str] = None,
        results_dir: Optional[str] = None,
        MAX: int = 0,
        random_choice: bool = False,
        multimodal: bool = False,
    ):
        """
        Initializes the LLMValidatorsTesting instance.
        """
        self.metrics = Metrics()
        self.selected_traits = list(supported_traits["traits"].values)

        input_path = input_filename or settings.get("paths", {}).get("input_data_csv")
        results_path = results_dir or settings.get("paths", {}).get("results_dir")

        self.statistics = Statistics(
            input_path, results_path, self.metrics, self.selected_traits
        )

        if multimodal:
            print("Initializing in MULTIMODAL mode.")
            self.llm_service = MultimodalLLM_Service()
        else:
            print("Initializing in standard text-only mode.")
            self.llm_service = LLM_Service()

        self.prompts = pd.DataFrame(columns=["question_id", "prompt"])
        self.traits_evaluation_results = pd.DataFrame(
            columns=[
                "question_id",
                "prompt_id",
                "model_id",
                "traits_output",
                "accuracy",
                "time_taken_ms",
            ]
        )
        self.load_testing_data(input_path, MAX, random_choice)

    def generate_prompts(self, qid: int, pid: PromptID):
        """
        Generates a prompt and appends it to the prompts DataFrame using pd.concat.
        """
        qid_indexes = self.ground_truth_questions["question_id"] == qid
        question_df = self.ground_truth_questions[qid_indexes]

        new_prompt_row = pd.DataFrame(
            [{"question_id": qid, "prompt": Prompt(pid, question_df)}]
        )
        self.prompts = pd.concat([self.prompts, new_prompt_row], ignore_index=True)

    def execute(self, qid: int, pid: PromptID, mid: str):
        """
        Executes a prompt and records the evaluation, including timing.
        """
        prompt_series = self.prompts.loc[self.prompts["question_id"] == qid, "prompt"]
        for prompt_obj in prompt_series:
            if prompt_obj.id is not pid:
                continue

            print(f"Running QID: {qid}, PID: {prompt_obj.id.name}, MID: {mid}.")

            start_time = time.perf_counter()

            question_row = self.ground_truth_questions.loc[
                self.ground_truth_questions["question_id"] == qid
            ]
            pil_images = (
                get_image_files(question_row)
                if isinstance(self.llm_service, MultimodalLLM_Service)
                else None
            )

            exec_args = {
                "model_id": mid,
                "prompt": str(prompt_obj.prompt),
                "format_instructions": str(prompt_obj.format_instructions),
            }
            if pil_images:
                exec_args["pil_images"] = pil_images

            response = self.llm_service.execute_prompt(**exec_args)

            end_time = time.perf_counter()
            duration_ms = (end_time - start_time) * 1000

            answer_result = self._process_response(qid, pid, mid, response, duration_ms)

            new_result_row = pd.DataFrame([answer_result])

            if not new_result_row.empty:
                self.traits_evaluation_results = pd.concat(
                    [self.traits_evaluation_results, new_result_row], ignore_index=True
                )
            break

    def _process_response(
        self, qid: int, pid: PromptID, mid: str, response: any, duration_ms: float
    ) -> dict:
        """Helper method to process the model's response and return a result dictionary."""
        rate_limit_msg = settings.get("constants", {}).get("rate_limit_error")

        if response is None or response == rate_limit_msg:
            return {
                "question_id": qid,
                "prompt_id": pid.name,
                "model_id": mid,
                "traits_output": rate_limit_msg if response == rate_limit_msg else "{}",
                "accuracy": "0.0",
                "time_taken_ms": round(duration_ms, 2),
            }

        expected = np.ones(len(self.selected_traits), dtype=bool)
        obtained = np.zeros(len(self.selected_traits), dtype=bool)

        if hasattr(response, "traits_output") and response.traits_output:
            for trait_res in response.traits_output:
                if trait_res.trait in self.selected_traits and trait_res.valid:
                    try:
                        index = self.selected_traits.index(trait_res.trait)
                        obtained[index] = True
                    except ValueError:
                        continue

            accuracy = accuracy_score(expected, obtained)
            traits_json = (
                response.model_dump_json()
                if hasattr(response, "model_dump_json")
                else "{}"
            )
        else:
            accuracy, traits_json = 0.0, "{}"

        return {
            "question_id": qid,
            "prompt_id": pid.name,
            "model_id": mid,
            "traits_output": traits_json,
            "accuracy": f"{accuracy}",
            "time_taken_ms": round(duration_ms, 2),
        }

    def load_testing_data(
        self, inputname: str, MAX: int = 0, random_choice: bool = False
    ):
        """Loads and selects question data from the input CSV."""
        self.ground_truth_questions = pd.read_csv(inputname, keep_default_na=False)
        all_qids = sorted(self.ground_truth_questions["question_id"].unique())

        if 0 < MAX < len(all_qids):
            self.selected_questions = (
                random.sample(all_qids, MAX) if random_choice else all_qids[:MAX]
            )
        else:
            self.selected_questions = all_qids

    def is_valid_question(self, qid: int) -> bool:
        """Checks if a question has sufficient context (lecture or solution) to be valid."""
        question_data = self.ground_truth_questions[
            self.ground_truth_questions["question_id"] == qid
        ]
        if question_data.empty:
            return False

        lecture = str(question_data["lecture"].values[0] or "")
        solution = str(question_data["solution"].values[0] or "")

        has_lecture = lecture and lecture.lower() != "nan"
        has_solution = solution and solution.lower() != "nan"

        return has_lecture or has_solution

    def report(self, qid: int, pid: PromptID, mid: str, first_time: bool = False):
        """Writes the evaluation result for a single run to the main CSV file."""
        out_filename = self.statistics.RESULTS_FILENAME
        file_mode = "w" if first_time else "a"

        try:
            with open(
                out_filename, file_mode, newline="", encoding="utf-8"
            ) as data_file:
                csv_writer = csv.writer(data_file, quoting=csv.QUOTE_NONNUMERIC)
                if first_time:
                    csv_writer.writerow(self.traits_evaluation_results.columns)

                row_to_write = self.traits_evaluation_results.loc[
                    (self.traits_evaluation_results["question_id"] == qid)
                    & (self.traits_evaluation_results["prompt_id"] == pid.name)
                    & (self.traits_evaluation_results["model_id"] == mid)
                ]

                if not row_to_write.empty:
                    csv_writer.writerow(row_to_write.values[0])
        except IOError as e:
            print(f"Error writing to report file {out_filename}: {e}")

    def run_per_qid(
        self,
        prompt_ids: Optional[List[PromptID]] = None,
        model_ids: Optional[List[str]] = None,
        new_file: bool = True,
    ):
        """
        Runs the complete testing and evaluation pipeline for all specified models and prompts.
        """
        prompt_ids = prompt_ids or PromptID.all()
        model_ids = model_ids or []

        for mid_idx, mid in enumerate(model_ids):
            print(f"\n--- Processing Model: {mid} ---")
            for qid_idx, qid in enumerate(self.selected_questions):
                if not self.is_valid_question(qid):
                    continue
                for pid_idx, pid in enumerate(prompt_ids):
                    is_first_report = (
                        new_file and mid_idx == 0 and qid_idx == 0 and pid_idx == 0
                    )

                    self.generate_prompts(qid, pid)
                    self.execute(qid, pid, mid)
                    self.report(qid, pid, mid, first_time=is_first_report)

        print("\n--- Finalizing Statistics and Reports ---")
        self.statistics.clean_generated_questions()
        self.statistics.generate_summary()
        self.statistics.compute_statistics()
        self.statistics.generate_plots()
        self.statistics.classify_generated_questions()

        print("\n" + "=" * 30)
        print("Processing Complete!")
        print(f"Cold Models Encountered: {self.llm_service.cold_models}")
        print(f"Unsupported/Error Models: {self.llm_service.error_models}")
        print("=" * 30 + "\n")

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/LLM_Service.py
--------------------------------------------------
import os
import requests
from typing import Any, Dict, List, Optional, Union
from openai import OpenAI
from langchain.output_parsers import PydanticOutputParser
from pydantic import ValidationError
from .TraitList import TraitList
from huggingface_hub import InferenceClient
from ollama import chat
from config import settings


class LLM_Service:
    """
    A service class to interact with various Large Language Model (LLM) APIs,
    including OpenAI, Hugging Face, and local Ollama instances.
    """

    def __init__(self) -> None:
        """
        Initializes the LLM_Service, loading all configuration from the central settings.
        """
        print("ðŸ”§ Initializing LLM_Service with configuration...")

        # --- Instance attributes with type hints ---
        self.supported_models: Dict[str, str] = {}
        self.defaults: Dict[str, str] = settings.get("defaults", {})
        self.TIMEOUT: int = settings.get("settings", {}).get("default_timeout", 600)

        self.hf_api_key: Optional[str] = os.environ.get(
            settings.get("api", {}).get("huggingface_api_key_env")
        )
        self.openai_api_key: Optional[str] = os.environ.get(
            settings.get("api", {}).get("openai_api_key_env")
        )

        self.gpt_client: Optional[OpenAI] = None
        if self.openai_api_key:
            self.gpt_client = OpenAI(api_key=self.openai_api_key, timeout=self.TIMEOUT)
            print("âœ… OpenAI client initialized.")
        else:
            print("âš ï¸ OpenAI client not initialized: API key not found.")

        # --- Runtime state tracking ---
        self.cold_models: List[str] = []
        self.error_models: List[str] = []
        self.timeout_models: List[str] = []

        # --- Load all models from different groups into one dictionary ---
        model_groups: Dict[str, Dict[str, str]] = settings.get("models", {})
        for group_name, models_in_group in model_groups.items():
            self.supported_models.update(models_in_group)

    def print_supported_llms(self) -> None:
        """Prints a list of all supported model friendly names and their API identifiers."""
        print("List of supported LLMs:")
        for llm, url in self.supported_models.items():
            print(f"{llm} : {url}")

    def get_all_models(self) -> List[str]:
        """Returns a list of all supported model friendly names."""
        return list(self.supported_models.keys())

    def get_model_url(self, model_id: str) -> str:
        """
        Retrieves the API model identifier for a given friendly name (case-insensitive).
        Returns an empty string if not found.
        """
        for key, value in self.supported_models.items():
            if key.upper() == model_id.upper():
                return value
        return ""

    def get_model_ids_startswith(self, prefix: str) -> List[str]:
        """Returns a list of model friendly names that start with a given prefix."""
        model_ids: List[str] = []
        for key in self.supported_models.keys():
            if key.upper().startswith(prefix.upper()):
                model_ids.append(key)
        return model_ids

    def execute_prompt(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[Any]:
        """
        Dispatches a prompt to the appropriate LLM service based on the model ID's prefix.
        """
        response: Optional[Any] = None
        if model_id == "GPT35TurboInstruct":
            response = self.gpt_old_execute_prompt(
                model_id, prompt, format_instructions
            )
        elif model_id.startswith("GPT"):
            response = self.gpt_execute_prompt(model_id, prompt, format_instructions)
        elif model_id.startswith("Llama32"):
            response = self.hf_execute_prompt_Llama32(
                model_id, prompt, format_instructions
            )
        elif model_id.startswith("L_"):
            response = self.ollama_execute_prompt(model_id, prompt, format_instructions)
        else:  # Assume model is from Hugging Face
            response = self.hf_execute_prompt(model_id, prompt, format_instructions)
        return response

    def gpt_execute_prompt(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[TraitList]:
        """Executes a prompt against the OpenAI Chat Completions API."""
        if not self.gpt_client:
            print("gpt_execute_prompt: Aborting, OpenAI client not initialized.")
            return None

        default_model_id = self.defaults.get("openai_default", "GPT4oMini")
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model_id)

        parser = PydanticOutputParser(pydantic_object=TraitList)
        if not format_instructions:
            format_instructions = parser.get_format_instructions()
        try:
            messages = [{"role": "user", "content": prompt + format_instructions}]
            completion = self.gpt_client.chat.completions.create(
                model=model_url, messages=messages
            )
            gpt_response = completion.choices[0].message
            if gpt_response.refusal:
                print(f"gpt_execute_prompt: Refusal: {gpt_response.refusal}")
                return None

            return parser.invoke(gpt_response.content)
        except ValidationError as err:
            print(f"gpt_execute_prompt: ValidationError: {err}")
            return None
        except Exception as exc:
            print(f"gpt_execute_prompt: General exception: {exc}")
            return None

    def gpt_old_execute_prompt(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[TraitList]:
        """Executes a prompt against the legacy OpenAI Completions API."""
        if not self.gpt_client:
            print("gpt_old_execute_prompt: Aborting, OpenAI client not initialized.")
            return None

        default_model_id = self.defaults.get(
            "openai_legacy_default", "GPT35TurboInstruct"
        )
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model_id)

        parser = PydanticOutputParser(pydantic_object=TraitList)
        if not format_instructions:
            format_instructions = parser.get_format_instructions()
        try:
            messages = prompt + format_instructions
            completion = self.gpt_client.completions.create(
                model=model_url, prompt=messages
            )
            gpt_response = completion.choices[0].text

            if "error" in gpt_response:
                print(f"gpt_execute_prompt: API Error: {gpt_response}")
                return None

            return parser.invoke(gpt_response)
        except ValidationError as err:
            print(f"gpt_execute_prompt: ValidationError: {err}")
            return None
        except Exception as exc:
            print(f"gpt_execute_prompt: General exception: {exc}")
            return None

    def hf_execute_prompt(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[Union[TraitList, str]]:
        """Executes a prompt against the Hugging Face Inference API."""
        if not self.hf_api_key:
            print("hf_execute_prompt: Aborting, HuggingFace API key not found.")
            return None

        default_model_id = self.defaults.get("huggingface_default", "Llama3170Instruct")
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model_id)

        base_url = (
            settings.get("services", {}).get("huggingface", {}).get("base_url", "")
        )
        api_url = f"{base_url}{model_url}"

        headers = {"Authorization": f"Bearer {self.hf_api_key}"}
        parameters = {}
        if not model_id.startswith(("Flan", "MT5")):
            parameters["return_full_text"] = False

        parser = PydanticOutputParser(pydantic_object=TraitList)
        if not format_instructions:
            format_instructions = parser.get_format_instructions()

        payload = {"inputs": prompt + format_instructions, "parameters": parameters}
        try:
            response = requests.post(
                api_url, headers=headers, json=payload, timeout=self.TIMEOUT
            )

            if not response.ok:
                if response.status_code == 503:  # Model is cold
                    self.cold_models.append(model_id)
                elif response.status_code == 504:  # Model timeout
                    self.timeout_models.append(model_id)
                elif response.status_code == 429:  # Rate limit reached
                    print(f"hf_execute_prompt: Rate limit reached: {response.text}")
                    return "error=429"
                else:
                    self.error_models.append(model_id)
                print(f"hf_execute_prompt: Request failed: {response.text}")
                return None

            generated_text = response.json()[0]["generated_text"]
            return parser.invoke(generated_text)
        except ValidationError as err:
            print(f"hf_execute_prompt: ValidationError: {err}")
            return None
        except Exception as exc:
            print(f"hf_execute_prompt: General exception: {exc}")
            return None

    def ollama_execute_prompt(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[TraitList]:
        """Executes a prompt against a local Ollama service."""
        default_model_id = self.defaults.get("ollama_default", "L_Phi4")
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model_id)

        if not format_instructions:
            parser = PydanticOutputParser(pydantic_object=TraitList)
            format_instructions = parser.get_format_instructions()

        messages = [{"role": "user", "content": prompt + format_instructions}]
        try:
            response = chat(
                messages=messages,
                model=model_url,
                format="json",  # Use Ollama's built-in JSON mode
            )
            # Ollama with format='json' should return a valid JSON string
            parsed_response = TraitList.model_validate_json(
                response["message"]["content"]
            )
            return parsed_response
        except ValidationError as err:
            print(f"ollama_execute_prompt: ValidationError: {err}")
            return None
        except Exception as exc:
            print(f"ollama_execute_prompt: General exception: {exc}")
            self.error_models.append(model_id)
            return None

    def hf_execute_prompt_Llama32(
        self, model_id: str, prompt: str, format_instructions: str = ""
    ) -> Optional[TraitList]:
        """Executes a prompt against the HF API using the dedicated InferenceClient."""
        if not self.hf_api_key:
            print("hf_execute_prompt_Llama32: Aborting, HuggingFace API key not found.")
            return None

        default_model_id = self.defaults.get(
            "huggingface_llama32_default", "Llama32Vision11B"
        )
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model_id)

        parser = PydanticOutputParser(pydantic_object=TraitList)
        if not format_instructions:
            format_instructions = parser.get_format_instructions()
        try:
            client = InferenceClient(
                model_url, token=self.hf_api_key, timeout=self.TIMEOUT
            )
            messages = [{"role": "user", "content": prompt + format_instructions}]
            completion = client.chat.completions.create(
                model=model_url, messages=messages
            )
            generated_text = completion.choices[0].message.content

            if "error" in generated_text:
                print(f"hf_llama32: API error: {generated_text}")
                return None

            return parser.invoke(generated_text)
        except ValidationError as err:
            print(f"hf_llama32: ValidationError: {err}")
            return None
        except Exception as exc:
            print(f"hf_llama32: General exception: {exc}")
            return None

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/LLMsValidatorsTestingTest.py
--------------------------------------------------
import unittest
from .LLMValidatorsTesting import LLMValidatorsTesting
from .Prompt import Prompt, PromptID
from pathlib import Path

print("Starting LLMValidatorsTestingTest...")  # Debugging line to indicate test start


class MyTestCase(unittest.TestCase):
    """
    Test suite for the LLMValidatorsTesting class.

    This class contains various test methods to evaluate the functionality
    of LLMValidatorsTesting under different configurations, including
    different models, prompts, datasets, and multimodal capabilities.
    Each method typically sets up a specific scenario by defining
    the dataset, number of questions (MAX), selection strategy (random_choice),
    output directory, prompt types, and target LLM models.
    """

    def test_multimodal(self):
        """
        Tests the LLMValidatorsTesting pipeline with a multimodal model.

        This test processes a single randomly chosen question (MAX=1, random_choice=True)
        from a ScienceQA dataset that includes image references (`ScienceQA_test_mc_images.csv`).
        It uses the General_V1 prompt and targets the "GPT4oMini" multimodal model.
        The `multimodal=True` flag is passed to `LLMValidatorsTesting` to initialize
        the appropriate multimodal service. Paths are resolved using `pathlib.Path`.
        """
        print("Starting multimodal test...")
        MAX = 5
        random_choice = True
        SciQdataset = str(Path(r"../data/raw/ScienceQA_test_mc_images.csv").resolve())
        print("SciQdataset:", SciQdataset)
        results_dir = str(Path(r"../data/results/test_results" + str(MAX)).resolve())
        LLMrunner = LLMValidatorsTesting(
            SciQdataset, results_dir, MAX, random_choice, multimodal=True
        )
        prompt_IDs = [PromptID.General_V1_Img]
        models = ["GPT4oMini", "GPT4o", "GPT4Turbo"]
        # "L_Qwen25VL3B", "L_Qwen25L7B", "L_Qwen25VL32B", "L_Qwen25VL72B",
        #   "L_Gemma34B", "L_Gemma312B", "L_Gemma327B",
        #     "L_Llama32Vision11B", "L_Llama32Vision90B",
        #     "L_Llama4Maverick", "L_Llama4Scout",
        #     "L_MistralSmall3124B"
        LLMrunner.run_per_qid(prompt_IDs, models)


if __name__ == "__main__":
    unittest.main()

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/Main.py
--------------------------------------------------
import argparse
import os
import uuid
from config import settings
from .LLMValidatorsTesting import LLMValidatorsTesting
from .Prompt import PromptID

print("LLMValidatorsTesting - Main.py")


def list_of_strings(arg):
    return arg.split(",")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="LLMValidatorsTesting",
        description="Testing framework to assess the performance of LLM-based Validators.",
        epilog="",
    )
    parser.add_argument(
        "-i",
        "--input",
        dest="input_filename",
        default=None,
        help="Input .csv file. Defaults to the path in config.toml.",
        metavar="INPUT",
    )

    parser.add_argument(
        "-d",
        "--dir",
        "-o",
        "--out",
        dest="results_dir",
        default=None,
        help="Output directory. Defaults to 'results/<uuid>' using path from config.toml.",
        metavar="DIR",
    )

    parser.add_argument(
        "-N",
        "-n",
        "--max",
        dest="MAX",
        type=int,
        default=0,
        help="Maximum number of questions analysed from the dataset.",
        metavar="MAX",
    )

    parser.add_argument(
        "-r",
        "--random",
        dest="random_choice",
        action="store_true",
        help="Random order of analysed questions.",
    )

    parser.add_argument(
        "--multimodal",
        action="store_true",
        help="Run in multimodal mode for models that accept images.",
    )

    parser.add_argument(
        "-m",
        "--models",
        type=list_of_strings,
        dest="models_list",
        default="",
        help="List the LLMs to run.",
        metavar="MODELS",
    )

    parser.add_argument(
        "-sw",
        "--starts-with",
        type=str,
        dest="models_prefix",
        default=None,
        help="Selects all LLMs starting with the <prefix>.",
        metavar="PREFIX",
    )

    parser.add_argument(
        "-p",
        "--prompts",
        type=list_of_strings,
        dest="prompts_list",
        default="",
        help="List the prompts to use.",
        metavar="PROMPTS",
    )

    parser.add_argument(
        "-ll",
        "--llms",
        "--llm-list",
        dest="list_llms",
        action="store_true",
        help="List the supported LLMs.",
    )

    parser.add_argument(
        "-pl",
        "--prompt-list",
        dest="list_prompts",
        action="store_true",
        help="List the available prompts.",
    )

    args = parser.parse_args()

    input_file = args.input_filename or settings.get("paths", {}).get("input_data_csv")

    results_path = args.results_dir
    if results_path is None:
        base_results_dir = settings.get("paths", {}).get("results_dir", "results/")
        results_path = os.path.join(base_results_dir, str(uuid.uuid4()))

    print(args)
    if not input_file:
        parser.error(
            "Input filename is mandatory (provide via --input or in config.toml)."
        )
    elif not os.path.exists(input_file):
        parser.error(f"Input file '{input_file}' does not exist.")
    else:
        # Create results directory if it doesn't exist
        if not os.path.exists(results_path):
            os.makedirs(results_path)

        LLMrunner = LLMValidatorsTesting(
            input_filename=input_file,
            results_dir=results_path,
            MAX=args.MAX,
            random_choice=args.random_choice,
            multimodal=args.multimodal,
        )

        if args.list_llms:
            LLMrunner.llm_service.print_supported_llms()
        if args.list_prompts:
            PromptID.print_supported_prompts()

        models = []
        if args.models_prefix is not None:
            models = LLMrunner.llm_service.get_model_ids_startswith(args.models_prefix)
            if not models:
                parser.error("Invalid models prefix.")
        elif args.models_list and args.models_list != [""]:
            all_supported_models = LLMrunner.llm_service.get_all_models()
            models = [m for m in args.models_list if m in all_supported_models]
        else:
            models = LLMrunner.llm_service.get_all_models()

        if not models:
            parser.error("No valid models were selected to run.")

        prompt_IDs = []
        if args.prompts_list and args.prompts_list != [""]:
            all_prompt_names = [p.name for p in PromptID.all()]
            for p_name in args.prompts_list:
                if p_name in all_prompt_names:
                    prompt_IDs.append(PromptID[p_name])
        else:
            prompt_IDs = PromptID.all()

        if not prompt_IDs:
            parser.error("No valid prompts were selected to run.")

        LLMrunner.run_per_qid(prompt_ids=prompt_IDs, model_ids=models)

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/Metrics.py
--------------------------------------------------
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    precision_score,
    recall_score,
    matthews_corrcoef,
)


class Metrics:
    def get_available_metrics(self):
        return [
            "accuracy",
            "precision",
            "recall",
            "f1",
            "mcc",
        ]

    def compute_scores(self, prediction, ground_truth):
        results = {"accuracy": [], "precision": [], "recall": [], "f1": [], "mcc": []}
        results["accuracy"].append(accuracy_score(ground_truth, prediction))
        results["precision"].append(precision_score(ground_truth, prediction))
        results["recall"].append(recall_score(ground_truth, prediction))
        results["f1"].append(f1_score(ground_truth, prediction))
        results["mcc"].append(matthews_corrcoef(ground_truth, prediction))

        return results

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/MultimodalLLM_Service.py
--------------------------------------------------
import os
import requests
import base64
import io
from typing import List, Dict, Any, Optional

from ollama import chat
from pydantic import ValidationError
from PIL import Image

from config import settings
from .TraitList import TraitList


class MultimodalLLM_Service:
    """
    A service class to interact with multimodal LLMs.
    It handles prompt execution, image encoding, and model management.
    """

    def __init__(self) -> None:
        """
        Initializes the MultimodalLLM_Service with settings from the config file.
        """
        print("ðŸ”§ Initializing MultimodalLLM_Service with configuration...")

        # --- Load settings and models from the config file ---
        self.supported_models: Dict[str, str] = {}
        model_groups: Dict[str, Dict[str, str]] = settings.get("models", {})
        for group_name, models_in_group in model_groups.items():
            self.supported_models.update(models_in_group)

        self.defaults: Dict[str, str] = settings.get("defaults", {})
        self.params: Dict[str, Any] = settings.get("parameters", {})
        self.TIMEOUT: int = settings.get("settings", {}).get("default_timeout", 600)

        # --- Load API key using the name specified in the config ---
        self.openai_api_key: Optional[str] = os.environ.get(
            settings.get("api", {}).get("openai_api_key_env", "")
        )
        if self.openai_api_key:
            print("âœ… OpenAI API key loaded successfully.")
        else:
            print("âš ï¸ WARNING: OpenAI API key not set or key name misconfigured.")

        # --- Instance variables for runtime state ---
        self.cold_models: List[str] = []
        self.error_models: List[str] = []
        self.timeout_models: List[str] = []

    def print_supported_models(self) -> None:
        """Prints a list of all supported model friendly names and their API identifiers."""
        print("List of supported MLLMs:")
        for mllm, url in self.supported_models.items():
            print(f"{mllm}: {url}")

    def get_all_models(self) -> List[str]:
        """Returns a list of all supported model friendly names."""
        return list(self.supported_models.keys())

    def get_model_url(self, model_id: str) -> str:
        """
        Retrieves the API model identifier for a given friendly name (case-insensitive).
        """
        for key, value in self.supported_models.items():
            if key.upper() == model_id.upper():
                return value
        return ""

    def get_model_ids_startswith(self, prefix: str) -> List[str]:
        """Returns a list of model friendly names that start with a given prefix."""
        return [key for key in self.supported_models if key.startswith(prefix)]

    @staticmethod
    def encode_image(pil_image: Image.Image) -> Optional[str]:
        """Encodes a PIL image to a base64 string."""
        try:
            buffered = io.BytesIO()
            pil_image.save(buffered, format="PNG")
            return base64.b64encode(buffered.getvalue()).decode("utf-8")
        except Exception as e:
            print(f"Error encoding PIL image: {e}")
            return None

    def ollama_execute_prompt(
        self,
        model_id: str,
        prompt: str,
        pil_images: Optional[List[Image.Image]],
        format_instructions: str = "",
    ) -> Optional[TraitList]:
        """Executes a multimodal prompt against a local Ollama service."""
        default_model = self.defaults.get("ollama_multimodal_default", "L_Qwen25VL3B")
        model_url = self.get_model_url(model_id) or self.get_model_url(default_model)

        final_prompt = prompt + format_instructions

        image_payload = []
        if pil_images:
            image_payload = [
                b64_img for img in pil_images if (b64_img := self.encode_image(img))
            ]

        messages = [{"role": "user", "content": final_prompt, "images": image_payload}]

        try:
            response = chat(messages=messages, model=model_url, format="json")
            raw_content = response["message"]["content"]
            return TraitList.model_validate_json(raw_content)
        except Exception as exc:
            print(f"Ollama execution error on model {model_id}: {exc}")
            self.error_models.append(model_id)
            return None

    def gpt_execute_prompt(
        self,
        model_id: str,
        prompt: str,
        pil_images: Optional[List[Image.Image]],
        format_instructions: str = "",
        image_detail: str = "",
    ) -> Optional[TraitList]:
        """Executes a multimodal prompt against the OpenAI API."""
        if not self.openai_api_key:
            print("Cannot execute GPT prompt: API key is missing.")
            return None

        # --- Load defaults and parameters from config ---
        default_model = self.defaults.get("openai_multimodal_default", "GPT4oMini")
        api_model_id = self.get_model_url(model_id) or self.get_model_url(default_model)

        openai_params = self.params.get("openai", {})
        max_tokens = openai_params.get("max_tokens", 2048)
        detail_level = image_detail or openai_params.get("image_detail", "auto")

        final_prompt = prompt + format_instructions
        content_parts: List[Dict[str, Any]] = [{"type": "text", "text": final_prompt}]

        if pil_images:
            for img in pil_images:
                if base64_image := self.encode_image(img):
                    content_parts.append(
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}",
                                "detail": detail_level,
                            },
                        }
                    )

        payload = {
            "model": api_model_id,
            "messages": [{"role": "user", "content": content_parts}],
            "response_format": {"type": "json_object"},
            "max_tokens": max_tokens,
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.openai_api_key}",
        }
        api_url = settings.get("services", {}).get("openai", {}).get("base_url")

        try:
            response = requests.post(
                api_url, headers=headers, json=payload, timeout=self.TIMEOUT
            )
            response.raise_for_status()
            response_data = response.json()
            gpt_response_content = response_data["choices"][0]["message"]["content"]

            return TraitList.model_validate_json(gpt_response_content)
        except (requests.RequestException, ValidationError) as exc:
            print(f"GPT execution error on model {model_id}: {exc}")
            self.error_models.append(model_id)
            return None

    def execute_prompt(
        self,
        model_id: str,
        prompt: str,
        pil_images: Optional[List[Image.Image]] = None,
        format_instructions: str = "",
        **kwargs: Any,
    ) -> Optional[TraitList]:
        """Dispatches a prompt to the appropriate multimodal service."""
        if model_id.startswith(("L_", "PCL_")):
            return self.ollama_execute_prompt(
                model_id, prompt, pil_images, format_instructions
            )
        elif model_id.startswith("GPT"):
            return self.gpt_execute_prompt(
                model_id,
                prompt,
                pil_images,
                format_instructions,
                image_detail=kwargs.get("image_detail", ""),
            )
        else:
            print(
                f"Model Skipped: {model_id} (Unknown or unsupported prefix for multimodal service)"
            )
            self.error_models.append(model_id)
            return None

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/Prompt.py
--------------------------------------------------
from enum import Enum
from typing import Dict, List, Any, Optional

from pandas import DataFrame

from config import settings
from .TraitList import traits_list, supported_traits
from .ScienceQA import build_question, build_characteristics


class PromptID(Enum):
    """Enumeration of available prompt template identifiers."""

    General_V1 = 0
    General_V2 = 1
    General_V1_Img = 2
    Optimized_V1 = 3

    @classmethod
    def all(cls) -> List["PromptID"]:
        """Returns a list of all prompt IDs."""
        return list(cls)

    @classmethod
    def print_supported_prompts(cls) -> None:
        """Prints the names of all supported prompts."""
        print("List of supported Prompts:")
        for p in cls.all():
            print(f"- {p.name}")


class Prompt:
    """
    A class that constructs a final, formatted prompt string from a template
    and ground truth data, based on settings from the config file.
    """

    id: PromptID
    template: str
    prompt: str
    question_format: str
    characteristics_format: str
    format_instructions: str
    gt_question: Optional[DataFrame]
    selected_traits: List[str]

    def __init__(
        self,
        id: PromptID = PromptID.General_V1,
        question: Optional[DataFrame] = None,
        traits: Optional[List[str]] = None,
    ):
        self.id = id
        self.gt_question = question
        # Fallback to all supported traits if none are provided
        self.selected_traits = (
            traits if traits is not None else list(supported_traits["traits"].values)
        )
        # Initialize all attributes to be safe
        self.template = ""
        self.prompt = ""
        self.question_format = ""
        self.characteristics_format = ""
        self.format_instructions = ""

        self.instantiate_prompt_template()

    def instantiate_prompt_template(self) -> None:
        """
        This method dynamically builds the prompt by looking up
        the templates and formats in the central configuration file.
        """
        prompt_name = self.id.name
        prompt_config = settings.get("prompts", {}).get(prompt_name, {})

        if not prompt_config:
            print(
                f"âš ï¸ Warning: Prompt '{prompt_name}' not found in config.toml. Using empty prompt."
            )
            return

        defaults = settings.get("defaults", {})
        default_q_format = defaults.get("default_question_format", "QCM-A")
        default_c_format = defaults.get("default_characteristics_format", "GSTCSk")

        self.template = prompt_config.get("template", "")
        self.format_instructions = prompt_config.get("format_instructions", "")
        self.question_format = prompt_config.get("question_format", default_q_format)
        self.characteristics_format = prompt_config.get(
            "characteristics_format", default_c_format
        )

        list_of_conditions = {
            t: d for t, d in traits_list.items() if t in self.selected_traits
        }

        question_text = build_question(self.gt_question, self.question_format)
        characteristics_text = build_characteristics(
            self.gt_question, self.characteristics_format
        )

        format_data = {
            "prompt": characteristics_text,
            "characteristics": characteristics_text,
            "questionnaire": question_text,
            "question": question_text,
            "conditions": str(list_of_conditions),
            "format_instructions": self.format_instructions,
        }

        self.prompt = self.template.format_map(format_data)

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/ScienceQA.py
--------------------------------------------------
import pandas as pd
from PIL import Image
from pathlib import Path
import ast
from typing import List, Union
from config import settings, PROJECT_ROOT


class ScienceQA:
    """
    Represents a single problem from the ScienceQA dataset.
    """

    id: str
    question: str
    choices: List[str]
    answer: int
    hint: str
    image: str
    task: str
    grade: str
    subject: str
    topic: str
    category: str
    skill: str
    lecture: str
    solution: str
    split: str

    def __init__(self, problem_data: pd.DataFrame):
        """
        Initializes a ScienceQA problem instance from a DataFrame row.
        """
        if not problem_data.empty:
            row = problem_data.iloc[0]
            self.id = str(row.get("question_id", ""))
            self.question = str(row.get("question", ""))
            # Safely evaluate the 'choices' string into a list
            try:
                choices_val = row.get("choices", "[]")
                self.choices = (
                    ast.literal_eval(choices_val)
                    if isinstance(choices_val, str)
                    else choices_val or []
                )
            except (ValueError, SyntaxError):
                self.choices = []
            self.answer = int(row.get("answer", -1))
            self.hint = str(row.get("hint", ""))
            self.image = str(row.get("image", ""))
            self.task = str(row.get("task", ""))
            self.grade = str(row.get("grade", ""))
            self.subject = str(row.get("subject", ""))
            self.topic = str(row.get("topic", ""))
            self.category = str(row.get("category", ""))
            self.skill = str(row.get("skill", ""))
            self.lecture = str(row.get("lecture", ""))
            self.solution = str(row.get("solution", ""))
            self.split = str(row.get("set", ""))
        else:
            # Initialize with empty values if no data is provided
            self.id, self.question, self.hint, self.image, self.task = (
                "",
                "",
                "",
                "",
                "",
            )
            self.grade, self.subject, self.topic, self.category = "", "", "", ""
            self.skill, self.lecture, self.solution, self.split = "", "", "", ""
            self.choices, self.answer = [], -1


### --- Generate QUESTION / RESPONSE part of the Validators ---


def get_question_text(problem: pd.DataFrame) -> str:
    """Extracts the question text from the problem data."""
    return str(problem["question"].iloc[0])


def get_context_text(problem: pd.DataFrame, use_caption: bool) -> str:
    """Generates a context string from the problem's hint and optionally its image caption."""
    txt_context = str(problem["hint"].iloc[0] or "")
    img_context = ""
    if use_caption and "caption" in problem.columns:
        img_context = str(problem["caption"].iloc[0] or "")

    context = " ".join([txt_context, img_context]).strip()

    na_string = settings.get("constants", {}).get("not_applicable_string", "N/A")
    return context or na_string


def get_image_files(problem: pd.DataFrame) -> List[Image.Image]:
    """
    Loads and returns all .png images for a problem from the directory specified in the config.
    """
    images: List[Image.Image] = []
    image_base_dir: Path = PROJECT_ROOT / settings["paths"]["image_base_dir"]

    question_id = str(problem["question_id"].iloc[0]).strip()
    image_folder_name = str(problem["image"].iloc[0]).strip()

    placeholders = settings.get("settings", {}).get(
        "image_folder_placeholders", ["", "none", "image.png"]
    )
    if image_folder_name.lower() in placeholders:
        image_folder_name = question_id

    image_dir = image_base_dir / image_folder_name
    if not image_dir.is_dir():
        return []

    for img_path in sorted(image_dir.glob("*.png")):
        try:
            with Image.open(img_path) as img:
                images.append(img.convert("RGB"))
        except Exception as e:
            print(f"âš ï¸ Could not load image {img_path}: {e}")

    return images


def get_choice_text(problem: pd.DataFrame, options: List[str]) -> str:
    """Formats the multiple-choice options into a single string."""
    choices_str: Union[str, list] = problem["choices"].iloc[0]
    choices_list: List[str] = []
    try:
        choices_list = (
            ast.literal_eval(choices_str)
            if isinstance(choices_str, str)
            else choices_str
        )
    except (ValueError, SyntaxError):
        print(f"Warning: Could not parse choices: '{choices_str}'.")
        choices_list = []

    return " ".join(
        [f"({options[i]}) {c}" for i, c in enumerate(choices_list) if i < len(options)]
    )


def get_answer(problem: pd.DataFrame, options: List[str]) -> str:
    """Retrieves the correct answer option letter for the problem."""
    answer_index: int = problem["answer"].iloc[0]
    return options[answer_index] if 0 <= answer_index < len(options) else ""


def get_lecture_text(problem: pd.DataFrame) -> str:
    """Extracts and formats the lecture text from the problem data."""
    return str(problem["lecture"].iloc[0] or "").replace("\n", " ")


def get_solution_text(problem: pd.DataFrame) -> str:
    """Extracts and formats the solution text from the problem data."""
    return str(problem["solution"].iloc[0] or "").replace("\n", " ")


def create_one_question(
    format_str: str,
    question: str,
    context: str,
    choice: str,
    answer: str,
    lecture: str,
    solution: str,
) -> str:
    """Constructs a formatted question string based on the specified format and components."""
    input_format, output_format = format_str.split("-")

    input_map = {
        "Q": f"Question: {question}\n",
        "C": f"Context: {context}\n",
        "M": f"Options: {choice}\n",
        "L": f"BECAUSE: {lecture}\n",
        "E": f"BECAUSE: {solution}\n",
    }
    input_str = "".join(input_map.get(char, "") for char in input_format)

    output_map = {"A": f"The answer is {answer}.", "L": lecture, "E": solution}
    output_parts = [output_map.get(char) for char in output_format]
    output_str = "Answer: " + " ".join(filter(None, output_parts))

    text = (input_str + output_str).replace("  ", " ").strip()
    return text.replace("BECAUSE:", "").strip() if text.endswith("BECAUSE:") else text


def build_question(problem: pd.DataFrame, format_str: str) -> str:
    """
    Builds a complete question string from problem data using choices from the config.
    """
    options = settings.get("settings", {}).get("choice_options", [])

    return create_one_question(
        format_str,
        get_question_text(problem),
        get_context_text(problem, use_caption=False),
        get_choice_text(problem, options),
        get_answer(problem, options),
        get_lecture_text(problem),
        get_solution_text(problem),
    )


### --- Generate CHARACTERISTICS / PROMPT part of the Validators ---


def get_skill(problem: pd.DataFrame) -> str:
    return str(problem["skill"].iloc[0])


def get_topic(problem: pd.DataFrame) -> str:
    return str(problem["topic"].iloc[0])


def get_subject(problem: pd.DataFrame) -> str:
    return str(problem["subject"].iloc[0])


def get_task(problem: pd.DataFrame) -> str:
    return str(problem["task"].iloc[0])


def get_grade(problem: pd.DataFrame) -> str:
    return str(problem["grade"].iloc[0])


def get_category(problem: pd.DataFrame) -> str:
    return str(problem["category"].iloc[0])


def create_one_characteristic(
    input_format: str,
    grade: str,
    subject: str,
    topic: str,
    category: str,
    skill: str,
) -> str:
    """Constructs a formatted string of educational characteristics based on the input format."""
    parts = {
        "G": f"Student Grade: {grade}\n",
        "S": f"Subject: {subject}\n",
        "T": f"Topic: {topic}\n",
        "C": f"Category: {category}\n",
        "Sk": f"Skill: {skill}\n",
    }

    if input_format == "GSTCSk":
        return "".join(parts.values())
    elif input_format == "GSTC":
        return parts["G"] + parts["S"] + parts["T"] + parts["C"]
    elif input_format == "GST":
        return parts["G"] + parts["S"] + parts["T"]
    elif input_format == "GS":
        return parts["G"] + parts["S"]
    elif input_format == "G":
        return parts["G"]

    print(f"Warning: Unknown characteristic format '{input_format}'.")
    return ""


def build_characteristics(problem: pd.DataFrame, format_str: str) -> str:
    """Builds a formatted string of educational characteristics from problem data."""
    grade_val = get_grade(problem)
    grade_cleaned = grade_val.lower().replace("grade", "").strip()

    return create_one_characteristic(
        format_str,
        grade_cleaned,
        get_subject(problem),
        get_topic(problem),
        get_category(problem),
        get_skill(problem),
    )

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/Statistics.py
--------------------------------------------------
import csv
import json
from pathlib import Path
from bisect import bisect_left
from typing import List, Tuple

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import scipy.stats as ss
from sklearn.metrics import accuracy_score

from config import settings
from .Metrics import Metrics


class Statistics:
    """
    Handles statistical analysis, data cleaning, and plot generation for the results.
    """

    metrics: Metrics
    selected_traits: List[str]
    RESULTS_DIR: Path
    RESULTS_FILENAME: Path
    CLEAN_RESULTS_FILENAME: Path
    CLASSIFICATION_FILENAME: Path
    RESULTS_STATISTICS: Path
    RESULTS_SUMMARY: Path

    def __init__(
        self,
        input_filename: str,
        results_dir: str,
        metrics: Metrics,
        selected_traits: List[str],
    ):
        self.metrics = metrics
        self.selected_traits = selected_traits

        results_path = Path(results_dir)
        results_path.mkdir(parents=True, exist_ok=True)
        self.RESULTS_DIR = results_path

        filenames_config = settings.get("filenames", {})
        self.RESULTS_FILENAME = results_path / filenames_config.get(
            "results_evaluation", "questions_traits_evaluation.csv"
        )
        self.CLEAN_RESULTS_FILENAME = results_path / filenames_config.get(
            "clean_results_evaluation", "clean_questions_traits_evaluation.csv"
        )
        self.CLASSIFICATION_FILENAME = results_path / filenames_config.get(
            "classification", "classification.csv"
        )
        self.RESULTS_STATISTICS = results_path / filenames_config.get(
            "statistics", "statistics.csv"
        )
        self.RESULTS_SUMMARY = results_path / filenames_config.get(
            "summary", "summary.csv"
        )

    def VD_A(self, treatment: List[float], control: List[float]) -> Tuple[float, str]:
        """
        Computes Vargha and Delaney A index.
        """
        m, n = len(treatment), len(control)
        if m != n:
            raise ValueError("Data d and f must have the same length")

        r = ss.rankdata(treatment + control)
        r1 = sum(r[:m])
        A = (2 * r1 - m * (m + 1)) / (2 * n * m)

        levels = [0.147, 0.33, 0.474]
        magnitudes = ["negligible", "small", "medium", "large"]
        magnitude = magnitudes[bisect_left(levels, abs((A - 0.5) * 2))]

        return A, magnitude

    def clean_generated_questions(self) -> None:
        """Filters out invalid or error-filled rows from the raw results file."""
        df = pd.read_csv(self.RESULTS_FILENAME, keep_default_na=False)

        with open(
            self.CLEAN_RESULTS_FILENAME, "w", newline="", encoding="utf-8"
        ) as data_file:
            csv_writer = csv.writer(data_file, quoting=csv.QUOTE_NONNUMERIC)
            csv_writer.writerow(df.columns)

            for _, row in df.iterrows():
                traits_output = row.get("traits_output", "")
                if not traits_output or traits_output == "{}":
                    continue
                try:
                    traits_list = json.loads(traits_output)
                    if isinstance(traits_list, list) and all(
                        t.get("trait") == "ERROR" for t in traits_list
                    ):
                        continue
                except (json.JSONDecodeError, AttributeError):
                    continue
                csv_writer.writerow(row)

    def compute_statistics(self) -> None:
        """Computes and saves statistical comparisons between different prompts."""
        df_all = pd.read_csv(self.CLEAN_RESULTS_FILENAME, keep_default_na=False)
        with open(
            self.RESULTS_STATISTICS, "w", newline="", encoding="utf-8"
        ) as statistics_file:
            statistics_writer = csv.writer(statistics_file)
            statistics_writer.writerow(
                [
                    "model_id",
                    "metric",
                    "treatment",
                    "control",
                    "median_treatment",
                    "mean_treatment",
                    "median_control",
                    "mean_control",
                    "wilcoxon",
                    "kendall",
                    "A12",
                ]
            )

            for mid in df_all["model_id"].unique():
                df = df_all[df_all["model_id"] == mid]
                if df["traits_output"].astype(str).str.len().sum() == 0:
                    continue

                for metric in ["accuracy"]:
                    for treatment in df["prompt_id"].unique():
                        for control in df["prompt_id"].unique():
                            if treatment == control:
                                continue

                            treatment_values, control_values = [], []
                            for qid in df["question_id"].unique():
                                t_val = df[
                                    (df["question_id"] == qid)
                                    & (df["prompt_id"] == treatment)
                                ][metric].values.astype(float)
                                c_val = df[
                                    (df["question_id"] == qid)
                                    & (df["prompt_id"] == control)
                                ][metric].values.astype(float)
                                if t_val.size == 1 and c_val.size == 1:
                                    treatment_values.append(t_val[0])
                                    control_values.append(c_val[0])

                            wil_p, ken_p, A12 = 0.0, 0.0, 0.0
                            if treatment_values and not np.allclose(
                                treatment_values, control_values
                            ):
                                _, wil_p = ss.wilcoxon(treatment_values, control_values)
                                _, ken_p = ss.kendalltau(
                                    treatment_values, control_values
                                )
                                A12, _ = self.VD_A(treatment_values, control_values)

                            statistics_writer.writerow(
                                [
                                    mid,
                                    metric,
                                    treatment,
                                    control,
                                    f"{np.median(treatment_values):.2f}",
                                    f"{np.mean(treatment_values):.2f}",
                                    f"{np.median(control_values):.2f}",
                                    f"{np.mean(control_values):.2f}",
                                    f"{wil_p:.2f}",
                                    f"{ken_p:.2f}",
                                    f"{A12:.2f}",
                                ]
                            )

    def generate_summary(self) -> None:
        """Generates a summary CSV file with aggregated metrics per model and prompt."""
        df = pd.read_csv(self.CLEAN_RESULTS_FILENAME, keep_default_na=False)
        df_gen = pd.read_csv(self.RESULTS_FILENAME, keep_default_na=False)
        total_questions = df_gen["question_id"].nunique()
        total_questions = 1 if total_questions == 0 else total_questions

        with open(
            self.RESULTS_SUMMARY, "w", newline="", encoding="utf-8"
        ) as summary_file:
            summary_writer = csv.writer(summary_file)
            header = (
                ["prompt_id", "model_id", "accuracy"]
                + self.selected_traits
                + ["num_questions", "success_ratio"]
            )
            summary_writer.writerow(header)

            for pid in df["prompt_id"].unique():
                for mid in df["model_id"].unique():
                    subset = df[(df["prompt_id"] == pid) & (df["model_id"] == mid)]
                    if subset.empty:
                        continue

                    num_gen_questions = len(subset)
                    accuracy = subset["accuracy"].astype(float).mean()
                    stats_row = [pid, mid, f"{accuracy:.2f}"]

                    for trait in self.selected_traits:
                        trait_obtained = np.zeros(num_gen_questions, dtype=bool)
                        for i, traits_json in enumerate(subset["traits_output"]):
                            try:
                                for t in json.loads(traits_json):
                                    if t.get("trait") == trait and t.get("valid"):
                                        trait_obtained[i] = True
                                        break
                            except (json.JSONDecodeError, TypeError):
                                continue

                        trait_accuracy = accuracy_score(
                            np.ones(num_gen_questions, dtype=bool), trait_obtained
                        )
                        stats_row.append(f"{trait_accuracy:.2f}")

                    stats_row.append(f"{num_gen_questions}")
                    stats_row.append(f"{num_gen_questions / total_questions:.2f}")
                    summary_writer.writerow(stats_row)

    def generate_plots(self) -> None:
        """Generates boxplots for metrics, configured via config.toml."""
        df = pd.read_csv(self.CLEAN_RESULTS_FILENAME, keep_default_na=False)

        plotting_config = settings.get("plotting", {})
        figsize = (
            plotting_config.get("figsize_width", 12),
            plotting_config.get("figsize_height", 7),
        )
        palette = plotting_config.get("palette", "Set2")
        y_bottom = plotting_config.get("y_limit_bottom", -0.5)
        y_top = plotting_config.get("y_limit_top", 1.5)
        rotation = plotting_config.get("x_label_rotation", 90)
        legend_loc = plotting_config.get("legend_location", "lower center")

        filenames_config = settings.get("filenames", {})
        plot_template = filenames_config.get("plot_template", "{metric}.png")

        for metric in ["accuracy"]:
            plt.figure(figsize=figsize)
            ax = sns.boxplot(
                x="model_id",
                y=metric,
                hue="prompt_id",
                data=df.astype({metric: float}),
                showmeans=True,
                palette=palette,
            )
            ax.legend(
                title="Prompt",
                loc=legend_loc,
                bbox_to_anchor=(0.5, 1.05),
                ncol=3,
                frameon=False,
            )
            plt.xticks(rotation=rotation)
            plt.xlabel("Large Language Model")
            plt.ylabel(metric.capitalize())
            plt.ylim(y_bottom, y_top)
            plt.tight_layout()

            plot_path = self.RESULTS_DIR / plot_template.format(metric=metric)
            plt.savefig(plot_path)
            plt.close()
            print(f"âœ… Plot saved to {plot_path}")

    def classify_generated_questions(self) -> None:
        """Classifies questions based on how many LLMs failed a given trait."""
        df = pd.read_csv(self.CLEAN_RESULTS_FILENAME, keep_default_na=False)
        class_labels = settings.get("classification", {}).get(
            "labels", ["passing", "doubtful", "failing"]
        )

        with open(
            self.CLASSIFICATION_FILENAME, "w", newline="", encoding="utf-8"
        ) as data_file:
            csv_writer = csv.writer(data_file)
            csv_writer.writerow(
                ["question_id", "prompt_id", "trait", "trait_class", "failing_llms"]
            )

            for name, group in df.groupby(["question_id", "prompt_id"]):
                qid, pid = name
                for trait in self.selected_traits:
                    failing_llms = []
                    for _, row in group.iterrows():
                        try:
                            for t in json.loads(row["traits_output"]):
                                if t.get("trait") == trait and not t.get("valid"):
                                    failing_llms.append(row["model_id"])
                                    break
                        except (json.JSONDecodeError, TypeError):
                            continue

                    num_fails = len(failing_llms)
                    class_label = class_labels[min(num_fails, len(class_labels) - 1)]
                    csv_writer.writerow([qid, pid, trait, class_label, failing_llms])

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/TraitList.py
--------------------------------------------------
import json
from typing import List, Dict, Any

import pandas as pd
from pydantic import BaseModel, Field, model_validator
from typing_extensions import Self

from config import settings

# Load traits directly from the config file at the module level
traits_list: Dict[str, str] = settings.get("traits", {})

# Create a DataFrame of supported traits for easy access in other modules
supported_traits: pd.DataFrame = pd.DataFrame(
    traits_list.items(), columns=["traits", "condition"]
)


class Trait(BaseModel):
    """Represents the validation result for a single trait."""

    trait: str = Field(..., description="The name of the trait being evaluated.")
    valid: bool = Field(
        ..., description="True if the condition is met, False otherwise."
    )
    reasoning: str = Field(
        ..., description="An explanation for why 'valid' is False, or empty if True."
    )

    @model_validator(mode="after")
    def check_dependencies(self) -> Self:
        """
        Validates the trait against business logic and the supported traits
        list loaded from the configuration file.
        """
        is_invalid = False
        error_message = ""

        # The validation logic correctly checks against the config-loaded traits_list
        if not self.trait:
            is_invalid = True
            error_message = "Invalid trait: name is empty."
        elif self.trait not in traits_list:
            is_invalid = True
            error_message = f"Invalid trait: name '{self.trait}' is not supported."
        elif self.valid and self.reasoning:
            is_invalid = True
            error_message = "Invalid reasoning: must be empty when 'valid' is True."
        elif not self.valid and not self.reasoning:
            is_invalid = True
            error_message = (
                "Invalid reasoning: must not be empty when 'valid' is False."
            )

        if is_invalid:
            # Standardize the error format for downstream processing
            self.trait = "ERROR"
            self.valid = False
            self.reasoning = error_message

        return self


class TraitList(BaseModel):
    """A container for a list of Trait objects, representing the full LLM output."""

    traits_output: List[Trait] = Field(description="A list of Trait objects.")

    def toJSONstr(self) -> str:
        """Serializes the list of traits to a JSON string."""
        return json.dumps([t.model_dump() for t in self.traits_output])

    def toJSON(self) -> List[Dict[str, Any]]:
        """Deserializes the JSON string back into a list of dictionaries."""
        return json.loads(self.toJSONstr())

--------------------------------------------------
src_orig/MMLLMValidatorsTesting/__init__.py
--------------------------------------------------

--------------------------------------------------
src_orig/config/__init__.py
--------------------------------------------------
from pathlib import Path
import sys

# Use the standard library tomllib if available (Python 3.11+)
# and fall back to 'tomli' for older versions.
if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib


# This assumes __init__.py is located in .../PROJECT_ROOT/config/
PROJECT_ROOT = Path(__file__).parent.parent.parent

# CORRECTED PATH: Look for config.toml in the project root, not inside the config folder.
CONFIG_FILE_PATH = PROJECT_ROOT / "src" / "config" / "config.toml"


def load_config() -> dict:
    """Loads the configuration from the config.toml file."""
    try:
        with open(CONFIG_FILE_PATH, "rb") as f:
            return tomllib.load(f)
    except FileNotFoundError:
        print(f"ERROR: Configuration file not found at '{CONFIG_FILE_PATH}'")
        return {}
    except tomllib.TOMLDecodeError as e:
        print(f"ERROR: Could not parse '{CONFIG_FILE_PATH}': {e}")
        return {}


# Load all settings from the file.
settings = load_config()

# Pre-process the settings to create a combined dictionary of all models.
# This simplifies the code in the service classes.
all_models = {}
if "models" in settings:
    for model_group in settings.get("models", {}).values():
        if isinstance(model_group, dict):
            all_models.update(model_group)

settings["all_models"] = all_models

--------------------------------------------------
src_orig/orig_traits_def.py
--------------------------------------------------
import json
from typing import Optional
from pathlib import Path
from src.config import PROJECT_ROOT


class OriginalTraitDefinition:
    """
    Loads and provides trait definitions and example prompts for multimodal evaluation.
    """

    def __init__(self, path: Optional[Path] = None):
        prompt_file = path or PROJECT_ROOT / "data" / "orig_prompt_fillers.json"
        try:
            with open(prompt_file, "r", encoding="utf-8") as file:
                self.traits = json.load(file)
        except FileNotFoundError:
            print(f"âŒ Could not find prompt file at {prompt_file}")
            self.traits = {}

    def retrieve_definition(self, trait_name: str) -> str:
        trait = self.traits.get(trait_name.title())
        if trait:
            return trait.get("definition", "Definition not found.")
        return "Trait not found."

    def retrieve_note(self, trait_name: str) -> str:
        note = self.traits.get(trait_name.title(), {}).get("note", "")
        if note:
            return note
        return "Note not found."

    def retrieve_evaluation_questions(self, trait_name: str) -> str:
        eval_questions_list = self.traits.get(trait_name.title(), {}).get(
            "evaluation_questions", []
        )
        if eval_questions_list:
            return "\n".join([f"- {q}" for q in eval_questions_list])
        return "Evaluation Questions not found."

    def get_examples_for_prompt(self, trait_name: str) -> str:
        examples = self.traits.get(trait_name.title(), {}).get("examples", [])
        if not examples:
            return "No examples available."

        formatted = []
        for i, ex in enumerate(examples, 1):
            input_data = ex["input"]
            output_data = ex["output"]

            output = {
                "trait": output_data["trait"],
                "validity": output_data["validity"],
            }
            if "reasoning" in output_data:
                output["reasoning"] = output_data["reasoning"]

            formatted.append(
                f"Example {i}:\n"
                f"Input:\n"
                f"  Trait: {input_data['trait']}\n"
                f'  Activity Description: "{input_data["activity_description"]}"\n'
                f"Correct Output:\n{json.dumps(output, indent=2)}\n"
            )
        return "\n".join(formatted)
