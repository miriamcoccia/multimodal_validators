# Main Configuration File for MMLLMValidatorsTesting Project

# =============================================================================
#  PATHS
# =============================================================================
[paths]
# Base directory for the ScienceQA image dataset
image_base_dir = "data/raw/ScienceQA_images/test"
input_data_csv = "/home/ldap/coccia@private.list.lu/oat_2024/MultimodalLLMs/MMLLMValidatorsTesting/data/raw/ScienceQA_test_mc_images.csv"
results_dir = "/home/ldap/coccia@private.list.lu/oat_2024/MultimodalLLMs/MMLLMValidatorsTesting/data/results/"


# =============================================================================
#  SETTINGS
# =============================================================================
[settings]
# Default timeout for API requests in seconds
default_timeout = 600

# Options for multiple-choice questions
choice_options = ["A", "B", "C", "D", "E", "F", "G"]
image_folder_placeholders = ["", "none", "image.png"]


# =============================================================================
#  DEFAULTS
# =============================================================================
[defaults]
# Default models to use as fallbacks in the LLM_Service
openai_default = "GPT4oMini"
openai_legacy_default = "GPT35TurboInstruct"
huggingface_default = "Llama3170Instruct"
ollama_default = "L_Phi4"
huggingface_llama32_default = "Llama32Vision11B"
ollama_multimodal_default = "L_Qwen25VL3B"
openai_multimodal_default = "GPT4oMini"
default_question_format = "QCM-A"
default_characteristics_format = "GSTCSk"

# =============================================================================
#  API & SERVICES
# =============================================================================
[api]
# Environment variable names for API keys. The application will read the keys from these variables.
openai_api_key_env = "OPENAI_API_KEY"
huggingface_api_key_env = "API_KEY_HUGGINGFACE"

[services]
# Base URLs for different model-serving APIs
[services.openai]
base_url = "https://api.openai.com/v1/chat/completions"

[services.huggingface]
base_url = "https://api-inference.huggingface.co/models/"

[services.ollama]
base_url = "http://localhost:11434/api/generate"


# =============================================================================
#  MODEL DEFINITIONS
# =============================================================================
# Each subsection defines a group of models. The key is the friendly name used in the code,
# and the value is the actual model identifier for the API.

[models.local_ollama]

L_Qwen25VL3B = "qwen2.5vl:3b"
L_Qwen25VL7B = "qwen2.5vl:7b"
L_Qwen25VL32B = "qwen2.5vl:32b"
L_Gemma34B = "gemma3:4b"
L_Gemma312B = "gemma3:12b"
L_Gemma327B = "gemma3:27b"
L_Llama32Vision11B = "llama3.2-vision:11b"
L_Llama4Scout = "llama4:scout"
L_MistralSmall3124B = "mistral-small3.1:24b"

[models.pcl_ollama]
PCL_Qwen25VL3B = "pcl_qwen25vl:3b"
PCL_Qwen25VL7B = "pcl_qwen25vl:7b"
PCL_Qwen25VL32B = "pcl_qwen25vl:32b"
PCL_Gemma34B = "pcl_gemma3:4b"
PCL_Gemma312B = "pcl_gemma3:12b"
PCL_Gemma327B = "pcl_gemma3:27b"
PCL_Llama32Vision11B = "pcl_llama3.2-vision:11b"
PCL_Llama4Scout = "pcl_llama4:scout"
PCL_MistralSmall3124B = "pcl_mistral-small3.1:24b"

[models.openai]
GPT4o = "gpt-4o"
GPT4oMini = "gpt-4o-mini"
GPT4Turbo = "gpt-4-turbo"
GPT4 = "gpt-4"
GPT35Turbo = "gpt-3.5-turbo"
GPT35TurboInstruct = "gpt-3.5-turbo-instruct"
GPT35Turbo1106 = "gpt-3.5-turbo-1106"
GPT40613 = "gpt-4-0613"


# =============================================================================
#  PROMPT TEMPLATES
# =============================================================================
# Using TOML's multi-line strings for readability.

[prompts.General_V1]
question_format = "QCM-ALE"
characteristics_format = "GSTCSk"
template = '''Given the CHARACTERISTICS below and the QUESTION given by an AI assistant as a response to the CHARACTERISTICS. 
Does the QUESTION comply with the following LIST_OF_CONDITIONS (which are provided in JSON format, with keys "trait" and "condition")?

CHARACTERISTICS: ```{prompt}```

QUESTION: ```{questionnaire}```

LIST_OF_CONDITIONS: ```{conditions}```
'''
format_instructions = '''Reply with a text in valid JSON format, that is: the content is embedded within an open and a closing bracket.
Do not include in your answer the term "json". Do not include in your answer any carry return, nor any special character other than brackets and curly brackets.

Your answer must include, for each item in the LIST_OF_CONDITIONS:
1. A key "trait" with the trait of the corresponding LIST_OF_CONDITIONS item.
2. A key "valid" only with "True" if the corresponding condition of the LIST_OF_CONDITIONS item is fulfilled by the QUESTION; or "False" otherwise.
3. A key "reasoning", empty if "valid" is "True"; or, only if "valid" is "False", with an explanation of no more than 40 words of why the value in key "valid" is "False", pointing out which concrete questions or answers raise this problem.
'''

[prompts.General_V1_Img]
question_format = "QCM-A"
characteristics_format = "GSTCSk"
template = '''You will evaluate educational questions against quality criteria. You will receive three inputs:
- CHARACTERISTICS: The learning context and requirements
- QUESTION: The educational question to evaluate
- IMAGE: Supporting visual content (if applicable)

Your task is to evaluate whether the QUESTION meets each condition in the evaluation criteria below.

EVALUATION CRITERIA: ```{conditions}```

EVALUATION PROCESS:
1. Review the CHARACTERISTICS to understand the learning context
2. Examine the QUESTION against each criterion above
3. Consider the IMAGE if provided for additional context
4. Determine if each criterion is met (true/false)
5. Provide brief reasoning only for failed criteria

CHARACTERISTICS: ```{prompt}```

QUESTION: ```{questionnaire}```

RESPONSE FORMAT: ```{format_instructions}```

If the trait is valid, the reasoning must be an empty string.
'''
format_instructions = '''Your response must be valid JSON starting with `{` and ending with `}`. Do not include conversational text, markdown formatting, or code fences.
Structure your response as a single JSON object with the key "traits_output" containing an array of assessments. For each criterion, include:
- "trait": The criterion name
- "valid": Boolean (true if criterion is met, false if not)
- "reasoning": Empty string "" if valid is true, otherwise brief explanation (max 40 words)
'''

[prompts.Optimized_V1]
question_format = "QCM-A"
characteristics_format = "GSTCSk"
template = '''Critically evaluate the following QUESTION based on the CHARACTERISTICS and the provided IMAGE, adhering strictly to the evaluation criteria and JSON format defined in your system instructions.

CHARACTERISTICS: ```{characteristics}```

QUESTION: ```{question}

If the trait is valid, the reasoning must be an empty string.```'''
format_instructions = ""

# =============================================================================
#  VALIDATION TRAITS
# =============================================================================
# The list of traits and their descriptions for evaluation.

[traits]
Scope = "The activity is suitable to assess a target learning outcome at a given level."
Background = "Students have the required background to understand and solve the activity."
Clarity = "The activity describes the context, task and intended outcome unambiguously. The solution is clear and all the steps, decisions and alternatives are outlined."
Conciseness = "The problem statement is succinct, avoiding repetition and verbosity."
Reliability = "It is possible to consistently evaluate the correctness of a candidate solution."
Discrimination = "Distractors are effective at drawing incorrect answers."
Correctness = "The problem statement does not contain errors, missing or inconsistent data. The solution fulfills all the requirements and contains no errors or omissions. All valid solutions have been characterized & invalid alternatives are indeed invalid."
Difficulty = "The activity is neither obvious nor impossible: it is challenging yet feasible."
Workload = "The activity takes an average student a reasonable amount of time to be solved."
Format = "The description of the activity adheres to the requested format or template."
Accessibility = "The activity can be understood and solved by students with special needs."
Authenticity = "The activity captures a realistic scenario that is relevant to students and the field."
Inclusivity = "The activity does not contain inappropriate or biased content."
Validity = "The activity tests a relevant skill considering all its relevant perspectives."

# =============================================================================
#  CONSTANTS
# =============================================================================
[constants]
rate_limit_error = "error=429"
not_applicable_string = "N/A"

# =============================================================================
#  MODEL PARAMETERS
# =============================================================================
[parameters.openai]
max_tokens = 2048
image_detail = "auto" # Can be "low", "high", or "auto"

# =============================================================================
#  FILENAMES
# =============================================================================
[filenames]
results_evaluation = "questions_traits_evaluation.csv"
clean_results_evaluation = "clean_questions_traits_evaluation.csv"
classification = "classification.csv"
statistics = "statistics.csv"
summary = "summary.csv"
plot_template = "{metric}.png"

# =============================================================================
#  PLOTTING
# =============================================================================
[plotting]
figsize_width = 12
figsize_height = 7
palette = "Set2"
y_limit_bottom = -0.5
y_limit_top = 1.5
x_label_rotation = 90
legend_location = "lower center"


# =============================================================================
#  CLASSIFICATION
# =============================================================================
[classification]
# Labels for 0, 1, or >1 failing LLMs
labels = ["passing", "doubtful", "failing"]


# cd /home/ldap/coccia@private.list.lu/oat_2024/MultimodalLLMs/MMLLMValidatorsTesting/scripts